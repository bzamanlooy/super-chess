{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02944396",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885205de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from pdb import set_trace\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9652bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from degree_freedom_queen import *\n",
    "from degree_freedom_king1 import *\n",
    "from degree_freedom_king2 import *\n",
    "from generate_game import *\n",
    "from Chess_env import *\n",
    "\n",
    "\n",
    "size_board = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bceca7c",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "You can find the environment in the file Chess_env, which contains the class Chess_env. To define an object, you need to provide the board size considered as input. In our example, size_board=4. \n",
    "Chess_env is composed by the following methods:\n",
    "\n",
    "1. Initialise_game. The method initialises an episode by placing the three pieces considered (Agent's king and queen, enemy's king) in the chess board. The outputs of the method are described below in order.\n",
    "\n",
    "     S $\\;$ A matrix representing the board locations filled with 4 numbers: 0, no piece in that position; 1, location of the \n",
    "     agent's king; 2 location of the queen; 3 location of the enemy king.\n",
    "     \n",
    "     X $\\;$ The features, that is the input to the neural network. See the assignment for more information regarding the            definition of the features adopted. To personalise this, go into the Features method of the class Chess_env() and change        accordingly.\n",
    "     \n",
    "     allowed_a $\\;$ The allowed actions that the agent can make. The agent is moving a king, with a total number of 8                possible actions, and a queen, with a total number of $(board_{size}-1)\\times 8$ actions. The total number of possible actions correspond      to the sum of the two, but not all actions are allowed in a given position (movements to locations outside the borders or      against chess rules). Thus, the variable allowed_a is a vector that is one (zero) for an action that the agent can (can't)      make. Be careful, apply the policy considered on the actions that are allowed only.\n",
    "     \n",
    "\n",
    "2. OneStep. The method performs a one step update of the system. Given as input the action selected by the agent, it updates the chess board by performing that action and the response of the enemy king (which is a random allowed action in the settings considered). The first three outputs are the same as for the Initialise_game method, but the variables are computed for the position reached after the update of the system. The fourth and fifth outputs are:\n",
    "\n",
    "     R $\\;$ The reward. To change this, look at the OneStep method of the class where the rewards are set.\n",
    "     \n",
    "     Done $\\;$ A variable that is 1 if the episode has ended (checkmate or draw).\n",
    "     \n",
    "     \n",
    "3. Features. Given the chessboard position, the method computes the features.\n",
    "\n",
    "This information and a quick analysis of the class should be all you need to get going. The other functions that the class exploits are uncommented and constitute an example on how not to write a python code. You can take a look at them if you want, but it is not necessary.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9593a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITIALISE THE ENVIRONMENT\n",
    "\n",
    "env=Chess_Env(size_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc05bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 3 0]\n",
      " [0 0 0 0]\n",
      " [0 0 1 2]\n",
      " [0 0 0 0]]\n",
      "check?  0\n",
      "dofk2  0\n",
      "\n",
      "[[0 0 3 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 2]\n",
      " [0 0 0 0]]\n",
      "0.0  1\n",
      "check?  0\n",
      "dofk2  0\n"
     ]
    }
   ],
   "source": [
    "## PRINT 5 STEPS OF AN EPISODE CONSIDERING A RANDOM AGENT\n",
    "\n",
    "S,X,allowed_a=env.Initialise_game()                       # INTIALISE GAME\n",
    "\n",
    "print(S)                                                  # PRINT CHESS BOARD (SEE THE DESCRIPTION ABOVE)\n",
    "\n",
    "print('check? ',env.check)                                # PRINT VARIABLE THAT TELLS IF ENEMY KING IS IN CHECK (1) OR NOT (0)\n",
    "print('dofk2 ',np.sum(env.dfk2_constrain).astype(int))    # PRINT THE NUMBER OF LOCATIONS THAT THE ENEMY KING CAN MOVE TO\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    a,_=np.where(allowed_a==1)                  # FIND WHAT THE ALLOWED ACTIONS ARE\n",
    "    a_agent=np.random.permutation(a)[0]         # MAKE A RANDOM ACTION\n",
    "\n",
    "#     set_trace()\n",
    "    S,X,allowed_a,R,Done=env.OneStep(a_agent)   # UPDATE THE ENVIRONMENT\n",
    "    \n",
    "    \n",
    "    ## PRINT CHESS BOARD AND VARIABLES\n",
    "    print('')\n",
    "    print(S)\n",
    "    print(R,'', Done)\n",
    "    print('check? ',env.check)\n",
    "    print('dofk2 ',np.sum(env.dfk2_constrain).astype(int))\n",
    "    \n",
    "    \n",
    "    # TERMINATE THE EPISODE IF Done=True (DRAW OR CHECKMATE)\n",
    "    if Done:\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc16cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Agent, Average reward: 0.217 Number of steps:  6.828\n"
     ]
    }
   ],
   "source": [
    "# PERFORM N_episodes=1000 EPISODES MAKING RANDOM ACTIONS AND COMPUTE THE AVERAGE REWARD AND NUMBER OF MOVES \n",
    "\n",
    "S,X,allowed_a=env.Initialise_game()\n",
    "N_episodes=1000\n",
    "\n",
    "# VARIABLES WHERE TO SAVE THE FINAL REWARD IN AN EPISODE AND THE NUMBER OF MOVES \n",
    "R_save_random = np.zeros([N_episodes, 1])\n",
    "N_moves_save_random = np.zeros([N_episodes, 1])\n",
    "\n",
    "for n in range(N_episodes):\n",
    "    \n",
    "    S,X,allowed_a=env.Initialise_game()     # INITIALISE GAME\n",
    "    Done=0                                  # SET Done=0 AT THE BEGINNING\n",
    "    i=1                                     # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
    "    \n",
    "    # UNTIL THE EPISODE IS NOT OVER...(Done=0)\n",
    "    while Done==0:\n",
    "        \n",
    "        # SAME AS THE CELL BEFORE, BUT SAVING THE RESULTS WHEN THE EPISODE TERMINATES \n",
    "        \n",
    "        a,_=np.where(allowed_a==1)\n",
    "        a_agent=np.random.permutation(a)[0]\n",
    "\n",
    "        S,X,allowed_a,R,Done=env.OneStep(a_agent)\n",
    "        \n",
    "        \n",
    "        if Done:\n",
    "            \n",
    "            R_save_random[n]=np.copy(R)\n",
    "            N_moves_save_random[n]=np.copy(i)\n",
    "\n",
    "            break\n",
    "\n",
    "        i=i+1                               # UPDATE THE COUNTER\n",
    "\n",
    "\n",
    "\n",
    "# AS YOU SEE, THE PERFORMANCE OF A RANDOM AGENT ARE NOT GREAT, SINCE THE MAJORITY OF THE POSITIONS END WITH A DRAW \n",
    "# (THE ENEMY KING IS NOT IN CHECK AND CAN'T MOVE)\n",
    "\n",
    "print('Random_Agent, Average reward:',np.mean(R_save_random),'Number of steps: ',np.mean(N_moves_save_random))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebce4c7",
   "metadata": {},
   "source": [
    "## Defining a simple NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f3b8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    \n",
    "    def __init__(self, Xavier_init= True, layer_type = 'ReLU',number_of_layers = 1, number_of_neurons_per_layer = [200], output_size = 1, input_size = 58):\n",
    "        \n",
    "        self.Ws = []\n",
    "        self.bs = []\n",
    "        self.layer_type = layer_type\n",
    "        \n",
    "        if Xavier_init:\n",
    "            W1 = np.random.randn(number_of_neurons_per_layer[0], input_size) * np.sqrt(1 / (input_size))  # first layer \n",
    "            self.Ws.append(W1)\n",
    "            for i in range(1, number_of_layers):\n",
    "            \n",
    "                W = np.random.randn(number_of_neurons_per_layer[i], number_of_neurons_per_layer[i-1]) * np.sqrt(1 / (number_of_neurons_per_layer[i-1]))\n",
    "                self.Ws.append(W)\n",
    "                \n",
    "            W_out = np.random.randn(output_size, number_of_neurons_per_layer[-1]) * np.sqrt(1 / (number_of_neurons_per_layer[-1]))\n",
    "            self.Ws.append(W_out)\n",
    "            \n",
    "            \n",
    "        else: \n",
    "            print('Not Implemented :=(')\n",
    "            \n",
    "\n",
    "        for i in range(number_of_layers): \n",
    "            b = np.zeros((number_of_neurons_per_layer[i],))\n",
    "            self.bs.append(b)\n",
    "        b_out = np.zeros((output_size,))\n",
    "        self.bs.append(b_out)\n",
    "        \n",
    "        \n",
    "    def forward(self, x0): \n",
    "        \n",
    "        layer_out = x0\n",
    "        for i in range(len(self.Ws) - 1):\n",
    "            layer_out_pre_activation = np.dot(self.Ws[i], layer_out) + self.bs[i]   # calculate Wx + b\n",
    "            layer_out = 1/(1+np.exp(-layer_out_pre_activation))                     # apply sigmoid funciton \n",
    "            \n",
    "        return layer_out_pre_activation  # because the last layer should have no sigmoid! \n",
    "    \n",
    "    def _forward_batch(self, x0s): \n",
    "        outputs = [] \n",
    "        intermediate_outputs_list = [] \n",
    "        \n",
    "        for x0 in x0s:\n",
    "            output, intermediate_outputs = self._forward(x0)\n",
    "            \n",
    "            outputs.append(output)\n",
    "            intermediate_outputs_list.append(intermediate_outputs)\n",
    "        \n",
    "        return outputs, intermediate_outputs_list\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    def _forward(self, x0): \n",
    "        \n",
    "        intermediate_outputs = [] \n",
    "        layer_out = x0\n",
    "        intermediate_outputs.append(layer_out)\n",
    "        \n",
    "        if self.layer_type == 'ReLU': \n",
    "            for i in range(len(self.Ws) - 1):\n",
    "                layer_out_pre_activation = np.dot(self.Ws[i], layer_out) + self.bs[i]   # calculate Wx + b\n",
    "                layer_out = layer_out_pre_activation.copy()\n",
    "                layer_out[layer_out< 0] = 0    # for ReLU: make all positions <0 equal to 0\n",
    "                intermediate_outputs.append(layer_out)\n",
    "                \n",
    "            layer_out = np.dot(self.Ws[-1], layer_out) + self.bs[-1]\n",
    "            intermediate_outputs.append(layer_out)\n",
    "            \n",
    "            return layer_out, intermediate_outputs\n",
    "                \n",
    "                \n",
    "            \n",
    "        else: \n",
    "            print('entering sigmoid part of forward')\n",
    "\n",
    "            for i in range(len(self.Ws) - 1):\n",
    "                layer_out_pre_activation = np.dot(self.Ws[i], layer_out) + self.bs[i]   # calculate Wx + b\n",
    "                layer_out = 1/(1+np.exp(-layer_out_pre_activation))                     # apply sigmoid funciton\n",
    "                intermediate_outputs.append(layer_out)\n",
    "\n",
    "            layer_out = np.dot(self.Ws[-1], layer_out) + self.bs[-1]\n",
    "            intermediate_outputs.append(layer_out)\n",
    "            \n",
    "            return layer_out, intermediate_outputs\n",
    "    \n",
    "    \n",
    "    def backward(self, x0s, outputs, desired_outputs, intermediate_outputs_list): \n",
    "        \n",
    "        \n",
    "        dWs = [np.zeros(shape = self.Ws[i].shape) for i in range(len(self.Ws))]\n",
    "        dbiases = [np.zeros(shape = self.bs[i].shape) for i in range(len(self.bs))]  \n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range(len(x0s)): \n",
    "            \n",
    "            output = outputs[j]\n",
    "            desired_output = desired_outputs[j]\n",
    "            intermediate_outputs = intermediate_outputs_list[j]\n",
    "            x0 = x0s[j]\n",
    "            \n",
    "            \n",
    "            error_signal = desired_output - output\n",
    "\n",
    "            if self.layer_type == 'ReLU': \n",
    "                \n",
    "                for i in reversed(range(len(self.Ws))):\n",
    "                    if i == len(self.Ws) - 1:\n",
    "                        delta = error_signal                  # error signal -> L,  delta x input at layer-> Derivative at that layer    :=>    dW = derivative * lr \n",
    "                        \n",
    "                    else: \n",
    "                        intermediate_outputs_copy = intermediate_outputs[i+1].copy()\n",
    "                        intermediate_outputs_copy[intermediate_outputs_copy < 0] = 0 \n",
    "                        intermediate_outputs_copy[intermediate_outputs_copy > 0 ] = 1\n",
    "                        \n",
    "                        delta = intermediate_outputs_copy * error_signal\n",
    "                    \n",
    "#                     set_trace()\n",
    "                    \n",
    "                    dWs[i] = dWs[i] +  np.outer(delta, intermediate_outputs[i])\n",
    "                    dbiases[i] = dbiases[i] + delta\n",
    "                    \n",
    "                    error_signal = np.dot(self.Ws[i].T, delta)     \n",
    "            \n",
    "            else: \n",
    "                print('backward entering sigmoid part!')\n",
    "            \n",
    "                for i in reversed(range(len(self.Ws))):\n",
    "\n",
    "                    if i == len(self.Ws) - 1:\n",
    "    #                     print('last layer!')\n",
    "                        delta = error_signal\n",
    "                    else:\n",
    "                        delta = intermediate_outputs[i+1]*(1-intermediate_outputs[i+1]) * error_signal\n",
    "\n",
    "                    dWs[i] = dWs[i] +  np.outer(delta, intermediate_outputs[i])\n",
    "                    dbiases[i] = dbiases[i] + delta\n",
    "\n",
    "                    error_signal = np.dot(self.Ws[i].T, delta)\n",
    "                \n",
    "        return dWs, dbiases\n",
    "    \n",
    "    def update(self, dWs, dbiases, batch_size, lr): \n",
    "        \n",
    "        for i in range(len(self.Ws)): \n",
    "            self.Ws[i] = self.Ws[i] + (dWs[i] * lr) / batch_size \n",
    "            self.bs[i] = self.bs[i] + (dbiases[i] * lr) / batch_size\n",
    "            \n",
    "            \n",
    "        \n",
    "    def train(self, x_train, y_train, lr, epochs, batch_size, print_frequency = 10 ,shuffle_indexes = True):\n",
    "        \n",
    "        number_of_batches = int(math.floor(len(x_train)/batch_size))  \n",
    "        \n",
    "        print('---Before training---')\n",
    "        for j in range(len(self.Ws)):\n",
    "            print(f'Sum of Ws for layer {j}: {np.sum(np.abs(self.Ws[j]))} and sum of bs: {np.sum(np.abs(self.bs[j]))}')\n",
    "        \n",
    "        for i in range(epochs): \n",
    "            \n",
    "            if shuffle_indexes: \n",
    "                shuffled_idxs = np.random.permutation(X_train.shape[0])\n",
    "            else: \n",
    "                shuffled_idxs = np.array([i for i in range(len(X_train))])\n",
    "                \n",
    "                \n",
    "            for batch in range(0, number_of_batches): \n",
    "                \n",
    "                idxs_current_batch = shuffled_idxs[batch * batch_size : (batch + 1) * batch_size]\n",
    "\n",
    "                X_batch = X_train[idxs_current_batch]\n",
    "                y_batch = y_train[idxs_current_batch]\n",
    "\n",
    "\n",
    "                outputs, intermediate_outputs_list =  self._forward_batch(x0s= X_batch )\n",
    "                dWs, dbiases = self.backward(x0s = X_batch, outputs = outputs, desired_outputs = y_batch,intermediate_outputs_list = intermediate_outputs_list)\n",
    "                \n",
    "\n",
    "                self.update(dWs, dbiases, batch_size, lr)\n",
    "                \n",
    "            if (i % print_frequency == 0): \n",
    "                print(f'Epoch: {i}')\n",
    "                for j in range(len(self.Ws)): \n",
    "                    print(f'sum of abs dWs for layer {j}: {np.sum(np.abs(dWs[j]))} and sum of dbs: {np.sum(np.abs(dbiases[j]))}')\n",
    "                    print(f'sum of Ws for layer {j}: {np.sum(np.abs(self.Ws[j]))} and sum of bs: {np.sum(np.abs(self.bs[j]))}')\n",
    "\n",
    "\n",
    "                y_pred, _ = self._forward_batch(x0s = X_train)\n",
    "                print(f'MSE on the training set: { np.square((np.array(y_pred) - np.array(y_train))).sum() / np.array(y_pred).shape[0] }')\n",
    "                    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6804ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1388bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[i] for i in range(100)])\n",
    "\n",
    "y_train = 2 * X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ab5bdd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([74.39682141]),\n",
       " [array([15]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([74.39682141])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_nn._forward(X_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03fb7c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Before training---\n",
      "Sum of Ws for layer 0: 86.3854827902921 and sum of bs: 0.0\n",
      "Sum of Ws for layer 1: 320.7208905230938 and sum of bs: 0.0\n",
      "Sum of Ws for layer 2: 340.3759188579585 and sum of bs: 0.0\n",
      "Sum of Ws for layer 3: 7.150046190977206 and sum of bs: 0.0\n",
      "Epoch: 0\n",
      "sum of abs dWs for layer 0: 1529706.6705131067 and sum of dbs: 23060.904580599592\n",
      "sum of Ws for layer 0: 86.38482165490204 and sum of bs: 2.3060904580599593e-05\n",
      "sum of abs dWs for layer 1: 65531779.29338612 and sum of dbs: 24803.872347957717\n",
      "sum of Ws for layer 1: 320.7149032996516 and sum of bs: 2.480387234795771e-05\n",
      "sum of abs dWs for layer 2: 30615673.943302106 and sum of dbs: 50011.07918227741\n",
      "sum of Ws for layer 2: 340.3740284821264 and sum of bs: 5.001107918227742e-05\n",
      "sum of abs dWs for layer 3: 12348558.28491178 and sum of dbs: 13972.459256855345\n",
      "sum of Ws for layer 3: 7.143626382910938 and sum of bs: 1.3972459256855343e-05\n",
      "MSE on the training set: 25730.541554902866\n",
      "Epoch: 10\n",
      "sum of abs dWs for layer 0: 1199403.083850784 and sum of dbs: 18081.44535317881\n",
      "sum of Ws for layer 0: 86.3788558423571 and sum of bs: 0.0002233035041794439\n",
      "sum of abs dWs for layer 1: 49038894.27774218 and sum of dbs: 18564.108804558477\n",
      "sum of Ws for layer 1: 320.6715537540691 and sum of bs: 0.00023794470340028162\n",
      "sum of abs dWs for layer 2: 24175983.20627415 and sum of dbs: 42056.02542369091\n",
      "sum of Ws for layer 2: 340.3603833426313 and sum of bs: 0.0005102546249032363\n",
      "sum of abs dWs for layer 3: 10852167.979631484 and sum of dbs: 12947.369990236673\n",
      "sum of Ws for layer 3: 7.09605319890105 and sum of bs: 0.00014778273979619257\n",
      "MSE on the training set: 22165.134469897035\n",
      "Epoch: 20\n",
      "sum of abs dWs for layer 0: 900692.6557771923 and sum of dbs: 13578.268921779529\n",
      "sum of Ws for layer 0: 86.3742949515543 and sum of bs: 0.0003728166949017046\n",
      "sum of abs dWs for layer 1: 40122199.46106689 and sum of dbs: 15190.414464776015\n",
      "sum of Ws for layer 1: 320.6391955767119 and sum of bs: 0.00040055489300163966\n",
      "sum of abs dWs for layer 2: 22857496.241243236 and sum of dbs: 41733.50738429123\n",
      "sum of Ws for layer 2: 340.35210890279916 and sum of bs: 0.0009364178462853573\n",
      "sum of abs dWs for layer 3: 9827784.88474149 and sum of dbs: 12166.13850256254\n",
      "sum of Ws for layer 3: 7.059564485653558 and sum of bs: 0.00027281987164257576\n",
      "MSE on the training set: 19603.811513826295\n",
      "Epoch: 30\n",
      "sum of abs dWs for layer 0: 851285.6512216341 and sum of dbs: 12833.434517481179\n",
      "sum of Ws for layer 0: 86.37099001820086 and sum of bs: 0.0004939279387986388\n",
      "sum of abs dWs for layer 1: 37702329.40732016 and sum of dbs: 14275.447186007088\n",
      "sum of Ws for layer 1: 320.6132775045113 and sum of bs: 0.0005377375240568033\n",
      "sum of abs dWs for layer 2: 19745067.989794794 and sum of dbs: 37463.89525979744\n",
      "sum of Ws for layer 2: 340.3470545349088 and sum of bs: 0.0013377431122289533\n",
      "sum of abs dWs for layer 3: 9028599.795230124 and sum of dbs: 11500.312665950109\n",
      "sum of Ws for layer 3: 7.0296842068376595 and sum of bs: 0.0003907476963929567\n",
      "MSE on the training set: 17527.753279366178\n",
      "Epoch: 40\n",
      "sum of abs dWs for layer 0: 759097.1579392378 and sum of dbs: 11443.656008720476\n",
      "sum of Ws for layer 0: 86.36844328840367 and sum of bs: 0.0005959372406343947\n",
      "sum of abs dWs for layer 1: 30618696.441540763 and sum of dbs: 11594.085208050994\n",
      "sum of Ws for layer 1: 320.59828432151477 and sum of bs: 0.0006550423634063673\n",
      "sum of abs dWs for layer 2: 16388321.136052147 and sum of dbs: 32127.057585448707\n",
      "sum of Ws for layer 2: 340.3469682477867 and sum of bs: 0.001708282830875645\n",
      "sum of abs dWs for layer 3: 8407521.587538278 and sum of dbs: 10911.606654598656\n",
      "sum of Ws for layer 3: 7.005516998849579 and sum of bs: 0.0005024004188817669\n",
      "MSE on the training set: 15814.786336489004\n",
      "Epoch: 50\n",
      "sum of abs dWs for layer 0: 586231.3150368499 and sum of dbs: 8837.638336461085\n",
      "sum of Ws for layer 0: 86.36646919819994 and sum of bs: 0.000669911164656218\n",
      "sum of abs dWs for layer 1: 20788584.67686547 and sum of dbs: 7872.217160527498\n",
      "sum of Ws for layer 1: 320.5974379094053 and sum of bs: 0.0007356199092706303\n",
      "sum of abs dWs for layer 2: 17048330.333004795 and sum of dbs: 34012.91137188021\n",
      "sum of Ws for layer 2: 340.35261963066466 and sum of bs: 0.0020402266149651604\n",
      "sum of abs dWs for layer 3: 7931217.171727271 and sum of dbs: 10445.208394335352\n",
      "sum of Ws for layer 3: 7.000744599961227 and sum of bs: 0.0006089099378543432\n",
      "MSE on the training set: 14506.974712530602\n",
      "Epoch: 60\n",
      "sum of abs dWs for layer 0: 559076.1489946077 and sum of dbs: 8428.262737820623\n",
      "sum of Ws for layer 0: 86.36538766991336 and sum of bs: 0.0007090012176636459\n",
      "sum of abs dWs for layer 1: 20312495.91271208 and sum of dbs: 7692.149871199939\n",
      "sum of Ws for layer 1: 320.5986130687369 and sum of bs: 0.0007873942527230125\n",
      "sum of abs dWs for layer 2: 13773620.694574654 and sum of dbs: 27617.24294974229\n",
      "sum of Ws for layer 2: 340.361476875714 and sum of bs: 0.0023416174540571563\n",
      "sum of abs dWs for layer 3: 7544332.964526984 and sum of dbs: 10057.415340485242\n",
      "sum of Ws for layer 3: 6.9982582680460705 and sum of bs: 0.0007112048133139881\n",
      "MSE on the training set: 13454.497017351005\n",
      "Epoch: 70\n",
      "sum of abs dWs for layer 0: 444917.7661559641 and sum of dbs: 6707.283202781924\n",
      "sum of Ws for layer 0: 86.36502550321265 and sum of bs: 0.0007315372982952144\n",
      "sum of abs dWs for layer 1: 18264155.34814502 and sum of dbs: 6916.5347683597\n",
      "sum of Ws for layer 1: 320.6009045242523 and sum of bs: 0.0008168948962255973\n",
      "sum of abs dWs for layer 2: 14371741.444476593 and sum of dbs: 28788.276282321443\n",
      "sum of Ws for layer 2: 340.37066457140384 and sum of bs: 0.0026535074988938995\n",
      "sum of abs dWs for layer 3: 7219444.547049893 and sum of dbs: 9711.608926321636\n",
      "sum of Ws for layer 3: 6.997483755271275 and sum of bs: 0.0008098514620708517\n",
      "MSE on the training set: 12551.152732887973\n",
      "Epoch: 80\n",
      "sum of abs dWs for layer 0: 364436.86599237623 and sum of dbs: 5494.0046357764195\n",
      "sum of Ws for layer 0: 86.36505666813892 and sum of bs: 0.0007535385801574826\n",
      "sum of abs dWs for layer 1: 11970772.056838792 and sum of dbs: 4533.2657830380995\n",
      "sum of Ws for layer 1: 320.6076007968032 and sum of bs: 0.0008563806256797536\n",
      "sum of abs dWs for layer 2: 14853747.472002147 and sum of dbs: 29617.57519580666\n",
      "sum of Ws for layer 2: 340.38040258295416 and sum of bs: 0.002960683948620234\n",
      "sum of abs dWs for layer 3: 6948633.000372339 and sum of dbs: 9384.00208211351\n",
      "sum of Ws for layer 3: 6.998114011006614 and sum of bs: 0.0009051590386069071\n",
      "MSE on the training set: 11719.517503987838\n",
      "Epoch: 90\n",
      "sum of abs dWs for layer 0: 425516.9443315848 and sum of dbs: 6414.803116407998\n",
      "sum of Ws for layer 0: 86.36540229412748 and sum of bs: 0.0007737615555748157\n",
      "sum of abs dWs for layer 1: 16090286.738302428 and sum of dbs: 6093.247818406313\n",
      "sum of Ws for layer 1: 320.61653591160535 and sum of bs: 0.0008943598871198428\n",
      "sum of abs dWs for layer 2: 13610890.742583413 and sum of dbs: 26920.985165858052\n",
      "sum of Ws for layer 2: 340.3912181545786 and sum of bs: 0.0032564878304655416\n",
      "sum of abs dWs for layer 3: 6706956.469347567 and sum of dbs: 9062.32113282736\n",
      "sum of Ws for layer 3: 6.999961244596772 and sum of bs: 0.0009972275899735646\n",
      "MSE on the training set: 10926.782490698148\n",
      "Epoch: 100\n",
      "sum of abs dWs for layer 0: 376731.28241268 and sum of dbs: 5679.341182005501\n",
      "sum of Ws for layer 0: 86.36605267957749 and sum of bs: 0.000791288140154116\n",
      "sum of abs dWs for layer 1: 12265767.117785593 and sum of dbs: 4644.86465413451\n",
      "sum of Ws for layer 1: 320.62897664693617 and sum of bs: 0.0009312785488810155\n",
      "sum of abs dWs for layer 2: 14153138.147435471 and sum of dbs: 27728.19218461841\n",
      "sum of Ws for layer 2: 340.40276504821963 and sum of bs: 0.003546267375847851\n",
      "sum of abs dWs for layer 3: 6497540.214805345 and sum of dbs: 8745.083143350263\n",
      "sum of Ws for layer 3: 7.009670368290449 and sum of bs: 0.001086104863575729\n",
      "MSE on the training set: 10171.375776376082\n",
      "Epoch: 110\n",
      "sum of abs dWs for layer 0: 386147.99688137986 and sum of dbs: 5821.396056575735\n",
      "sum of Ws for layer 0: 86.36694840160129 and sum of bs: 0.0008041707127538109\n",
      "sum of abs dWs for layer 1: 12416325.508729214 and sum of dbs: 4701.902085614225\n",
      "sum of Ws for layer 1: 320.64373945574937 and sum of bs: 0.0009669409459070993\n",
      "sum of abs dWs for layer 2: 13855697.950521111 and sum of dbs: 26821.914997888103\n",
      "sum of Ws for layer 2: 340.4152262039063 and sum of bs: 0.0038261578776547333\n",
      "sum of abs dWs for layer 3: 6296641.356587413 and sum of dbs: 8426.0944587787\n",
      "sum of Ws for layer 3: 7.022263232687414 and sum of bs: 0.0011718024605659153\n",
      "MSE on the training set: 9442.78402702977\n",
      "Epoch: 120\n",
      "sum of abs dWs for layer 0: 507436.81091596937 and sum of dbs: 7649.759538086807\n",
      "sum of Ws for layer 0: 86.36810128526241 and sum of bs: 0.0008190361048499223\n",
      "sum of abs dWs for layer 1: 14148440.170343075 and sum of dbs: 5357.527424478829\n",
      "sum of Ws for layer 1: 320.661570334193 and sum of bs: 0.001003284447208966\n",
      "sum of abs dWs for layer 2: 12636450.254674822 and sum of dbs: 24138.276115376466\n",
      "sum of Ws for layer 2: 340.42841216725606 and sum of bs: 0.004096234343475443\n",
      "sum of abs dWs for layer 3: 6112955.69644872 and sum of dbs: 8107.674079189221\n",
      "sum of Ws for layer 3: 7.0355474574256895 and sum of bs: 0.0012543127417705571\n",
      "MSE on the training set: 8737.967552232383\n",
      "Epoch: 130\n",
      "sum of abs dWs for layer 0: 407106.4202787257 and sum of dbs: 6137.244782732362\n",
      "sum of Ws for layer 0: 86.36948140702613 and sum of bs: 0.0008299659667272744\n",
      "sum of abs dWs for layer 1: 12574031.536564982 and sum of dbs: 4761.189953319153\n",
      "sum of Ws for layer 1: 320.68065901936694 and sum of bs: 0.0010376006665103001\n",
      "sum of abs dWs for layer 2: 13276936.164346917 and sum of dbs: 24991.601964654892\n",
      "sum of Ws for layer 2: 340.4422019610705 and sum of bs: 0.004355937569344294\n",
      "sum of abs dWs for layer 3: 5932369.527438317 and sum of dbs: 7784.812486522414\n",
      "sum of Ws for layer 3: 7.049439514347001 and sum of bs: 0.0013336102737433952\n",
      "MSE on the training set: 8052.179976486379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140\n",
      "sum of abs dWs for layer 0: 363047.92084198253 and sum of dbs: 5473.045699533591\n",
      "sum of Ws for layer 0: 86.37101293743467 and sum of bs: 0.0008399500489132632\n",
      "sum of abs dWs for layer 1: 13330616.4154229 and sum of dbs: 5047.476073104294\n",
      "sum of Ws for layer 1: 320.7018411108446 and sum of bs: 0.001072771945108966\n",
      "sum of abs dWs for layer 2: 14088349.025117531 and sum of dbs: 26099.819894361917\n",
      "sum of Ws for layer 2: 340.45611233758734 and sum of bs: 0.004609170340971886\n",
      "sum of abs dWs for layer 3: 5747546.173608263 and sum of dbs: 7452.490395874758\n",
      "sum of Ws for layer 3: 7.063858774910409 and sum of bs: 0.0014096481486808741\n",
      "MSE on the training set: 7374.954585816248\n",
      "Epoch: 150\n",
      "sum of abs dWs for layer 0: 375145.2729355916 and sum of dbs: 5655.3525746246405\n",
      "sum of Ws for layer 0: 86.37259989007578 and sum of bs: 0.0008509198890337044\n",
      "sum of abs dWs for layer 1: 13433865.820705123 and sum of dbs: 5086.421118101438\n",
      "sum of Ws for layer 1: 320.7245531493252 and sum of bs: 0.0011108326957224184\n",
      "sum of abs dWs for layer 2: 13716493.409526749 and sum of dbs: 24996.46341501427\n",
      "sum of Ws for layer 2: 340.4702184905365 and sum of bs: 0.0048554415714506345\n",
      "sum of abs dWs for layer 3: 5565548.614195446 and sum of dbs: 7107.339637836417\n",
      "sum of Ws for layer 3: 7.078781800992868 and sum of bs: 0.0014822803498454997\n",
      "MSE on the training set: 6702.554086592128\n",
      "Epoch: 160\n",
      "sum of abs dWs for layer 0: 515301.9522436337 and sum of dbs: 7768.3036479218845\n",
      "sum of Ws for layer 0: 86.37436861477725 and sum of bs: 0.0008682721159696042\n",
      "sum of abs dWs for layer 1: 14944398.949387344 and sum of dbs: 5658.032517912511\n",
      "sum of Ws for layer 1: 320.7478067133065 and sum of bs: 0.0011460831524174012\n",
      "sum of abs dWs for layer 2: 12424012.74378097 and sum of dbs: 22261.787405421353\n",
      "sum of Ws for layer 2: 340.4854351364453 and sum of bs: 0.005091416761294321\n",
      "sum of abs dWs for layer 3: 5382076.420049306 and sum of dbs: 6759.966312626951\n",
      "sum of Ws for layer 3: 7.094109581171322 and sum of bs: 0.001551440633677207\n",
      "MSE on the training set: 6061.430414853103\n",
      "Epoch: 170\n",
      "sum of abs dWs for layer 0: 511351.8461763196 and sum of dbs: 7708.750121708063\n",
      "sum of Ws for layer 0: 86.37637220021519 and sum of bs: 0.000889095662511786\n",
      "sum of abs dWs for layer 1: 15221747.092907794 and sum of dbs: 5762.750805002288\n",
      "sum of Ws for layer 1: 320.77030248878566 and sum of bs: 0.0011741547996189708\n",
      "sum of abs dWs for layer 2: 13874029.31353114 and sum of dbs: 24434.372082978272\n",
      "sum of Ws for layer 2: 340.502054144842 and sum of bs: 0.005323251087359209\n",
      "sum of abs dWs for layer 3: 5192387.742756935 and sum of dbs: 6414.692350759525\n",
      "sum of Ws for layer 3: 7.1097054496345935 and sum of bs: 0.0016171419053793415\n",
      "MSE on the training set: 5454.823414489524\n",
      "Epoch: 180\n",
      "sum of abs dWs for layer 0: 503858.3271361942 and sum of dbs: 7595.7777095723795\n",
      "sum of Ws for layer 0: 86.378439913964 and sum of bs: 0.0009162888176091891\n",
      "sum of abs dWs for layer 1: 14313973.045988107 and sum of dbs: 5418.790315722187\n",
      "sum of Ws for layer 1: 320.79740403071804 and sum of bs: 0.0012025982702703391\n",
      "sum of abs dWs for layer 2: 11655880.676089566 and sum of dbs: 20171.310302746315\n",
      "sum of Ws for layer 2: 340.51964331022594 and sum of bs: 0.005537888198089087\n",
      "sum of abs dWs for layer 3: 4992637.3535380885 and sum of dbs: 6067.097842573148\n",
      "sum of Ws for layer 3: 7.125840756783804 and sum of bs: 0.0016793765741957615\n",
      "MSE on the training set: 4875.240880191341\n",
      "Epoch: 190\n",
      "sum of abs dWs for layer 0: 485949.8940697745 and sum of dbs: 7324.997540151922\n",
      "sum of Ws for layer 0: 86.38062742135916 and sum of bs: 0.0009447296582184384\n",
      "sum of abs dWs for layer 1: 17528592.06105713 and sum of dbs: 6633.691701895115\n",
      "sum of Ws for layer 1: 320.82475305917734 and sum of bs: 0.0012301992381024227\n",
      "sum of abs dWs for layer 2: 12098793.797535116 and sum of dbs: 20575.079564320396\n",
      "sum of Ws for layer 2: 340.5370173859773 and sum of bs: 0.005743040488562225\n",
      "sum of abs dWs for layer 3: 4784793.07416461 and sum of dbs: 5718.607066811771\n",
      "sum of Ws for layer 3: 7.143337930187208 and sum of bs: 0.0017381275596590937\n",
      "MSE on the training set: 4329.678351012618\n",
      "Epoch: 200\n",
      "sum of abs dWs for layer 0: 483410.5733113976 and sum of dbs: 7287.507020455287\n",
      "sum of Ws for layer 0: 86.38286078976874 and sum of bs: 0.0009755242212935624\n",
      "sum of abs dWs for layer 1: 13741035.529537845 and sum of dbs: 5201.307641096945\n",
      "sum of Ws for layer 1: 320.8536101152274 and sum of bs: 0.001256923036332213\n",
      "sum of abs dWs for layer 2: 10796902.666557943 and sum of dbs: 18054.649142507566\n",
      "sum of Ws for layer 2: 340.554484241758 and sum of bs: 0.005933878815336466\n",
      "sum of abs dWs for layer 3: 4577215.975434944 and sum of dbs: 5371.762050153685\n",
      "sum of Ws for layer 3: 7.162320369658162 and sum of bs: 0.001793400690290395\n",
      "MSE on the training set: 3816.445990552427\n",
      "Epoch: 210\n",
      "sum of abs dWs for layer 0: 466223.48009662516 and sum of dbs: 7028.398355847504\n",
      "sum of Ws for layer 0: 86.38514301632723 and sum of bs: 0.0010057681824101675\n",
      "sum of abs dWs for layer 1: 12639668.36103491 and sum of dbs: 4784.136004846912\n",
      "sum of Ws for layer 1: 320.8825563569948 and sum of bs: 0.0012826264476189918\n",
      "sum of abs dWs for layer 2: 11189809.326498678 and sum of dbs: 18404.753120510788\n",
      "sum of Ws for layer 2: 340.5725420644278 and sum of bs: 0.006113288530276363\n",
      "sum of abs dWs for layer 3: 4358151.735041103 and sum of dbs: 5026.844178647247\n",
      "sum of Ws for layer 3: 7.181619357176818 and sum of bs: 0.0018452104538546862\n",
      "MSE on the training set: 3339.388858227255\n",
      "Epoch: 220\n",
      "sum of abs dWs for layer 0: 455417.3383628716 and sum of dbs: 6865.48443328287\n",
      "sum of Ws for layer 0: 86.38742245340842 and sum of bs: 0.0010383492521233233\n",
      "sum of abs dWs for layer 1: 13079733.369709054 and sum of dbs: 4950.409271042172\n",
      "sum of Ws for layer 1: 320.9107877235574 and sum of bs: 0.0013071879335394365\n",
      "sum of abs dWs for layer 2: 9846627.794452673 and sum of dbs: 15935.13164562563\n",
      "sum of Ws for layer 2: 340.5913075890518 and sum of bs: 0.006280575027593311\n",
      "sum of abs dWs for layer 3: 4131034.717400895 and sum of dbs: 4685.804099813429\n",
      "sum of Ws for layer 3: 7.202112565391173 and sum of bs: 0.0018935927529240627\n",
      "MSE on the training set: 2899.5684085823427\n",
      "Epoch: 230\n",
      "sum of abs dWs for layer 0: 428624.3949104928 and sum of dbs: 6461.529155360998\n",
      "sum of Ws for layer 0: 86.38972357076136 and sum of bs: 0.0010721956379458187\n",
      "sum of abs dWs for layer 1: 12443529.144305728 and sum of dbs: 4709.262886770165\n",
      "sum of Ws for layer 1: 320.93833005006314 and sum of bs: 0.001330745842954997\n",
      "sum of abs dWs for layer 2: 9426404.447692405 and sum of dbs: 15022.619633164068\n",
      "sum of Ws for layer 2: 340.60929372622496 and sum of bs: 0.006437163530566312\n",
      "sum of abs dWs for layer 3: 3898639.8970738687 and sum of dbs: 4351.079620860693\n",
      "sum of Ws for layer 3: 7.221850490460907 and sum of bs: 0.0019386032479710837\n",
      "MSE on the training set: 2498.7525898948147\n",
      "Epoch: 240\n",
      "sum of abs dWs for layer 0: 398231.2560692813 and sum of dbs: 6003.373240895997\n",
      "sum of Ws for layer 0: 86.39198098335464 and sum of bs: 0.0011043630815194944\n",
      "sum of abs dWs for layer 1: 11308930.505425755 and sum of dbs: 4279.692173526049\n",
      "sum of Ws for layer 1: 320.9652136645992 and sum of bs: 0.0013510587155269884\n",
      "sum of abs dWs for layer 2: 9564651.522269983 and sum of dbs: 15021.91444650582\n",
      "sum of Ws for layer 2: 340.6279172871463 and sum of bs: 0.006583712723712188\n",
      "sum of abs dWs for layer 3: 3665069.0710315555 and sum of dbs: 4025.6498506151065\n",
      "sum of Ws for layer 3: 7.240753590357134 and sum of bs: 0.0019803183865654637\n",
      "MSE on the training set: 2138.2715390683593\n",
      "Epoch: 250\n",
      "sum of abs dWs for layer 0: 379097.160954668 and sum of dbs: 5714.432496510655\n",
      "sum of Ws for layer 0: 86.39419625214809 and sum of bs: 0.0011342503177617755\n",
      "sum of abs dWs for layer 1: 10495701.45672325 and sum of dbs: 3971.1780650955843\n",
      "sum of Ws for layer 1: 320.9912077582567 and sum of bs: 0.0013705874766722823\n",
      "sum of abs dWs for layer 2: 9053969.51354605 and sum of dbs: 14025.939296064662\n",
      "sum of Ws for layer 2: 340.6465418726769 and sum of bs: 0.0067219364058700186\n",
      "sum of abs dWs for layer 3: 3433033.6473495625 and sum of dbs: 3711.399873369604\n",
      "sum of Ws for layer 3: 7.258751788392309 and sum of bs: 0.0020188391992018587\n",
      "MSE on the training set: 1815.3023121100234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260\n",
      "sum of abs dWs for layer 0: 367186.93505670084 and sum of dbs: 5535.351063489692\n",
      "sum of Ws for layer 0: 86.39633120662367 and sum of bs: 0.0011655971829190447\n",
      "sum of abs dWs for layer 1: 10262719.391755868 and sum of dbs: 3883.326308138581\n",
      "sum of Ws for layer 1: 321.0165471042659 and sum of bs: 0.0013891059731192309\n",
      "sum of abs dWs for layer 2: 8231473.34357765 and sum of dbs: 12589.610821636923\n",
      "sum of Ws for layer 2: 340.6652537744918 and sum of bs: 0.006849581861749065\n",
      "sum of abs dWs for layer 3: 3198772.9854796794 and sum of dbs: 3408.1323737844064\n",
      "sum of Ws for layer 3: 7.2757878178111 and sum of bs: 0.0020542740487649947\n",
      "MSE on the training set: 1529.2049669670728\n",
      "Epoch: 270\n",
      "sum of abs dWs for layer 0: 345075.36320148694 and sum of dbs: 5201.036805027304\n",
      "sum of Ws for layer 0: 86.3984299450714 and sum of bs: 0.0011942315062719487\n",
      "sum of abs dWs for layer 1: 9797926.293404542 and sum of dbs: 3707.1883165663157\n",
      "sum of Ws for layer 1: 321.0407908213467 and sum of bs: 0.0014049954807100256\n",
      "sum of abs dWs for layer 2: 6264707.709814228 and sum of dbs: 9462.693967636136\n",
      "sum of Ws for layer 2: 340.6829186920427 and sum of bs: 0.006962795225944731\n",
      "sum of abs dWs for layer 3: 2969988.89113756 and sum of dbs: 3120.4995307619115\n",
      "sum of Ws for layer 3: 7.295363774828996 and sum of bs: 0.0020867559873270834\n",
      "MSE on the training set: 1283.2227067645165\n",
      "Epoch: 280\n",
      "sum of abs dWs for layer 0: 333726.77215371124 and sum of dbs: 5030.824886739067\n",
      "sum of Ws for layer 0: 86.40048380195849 and sum of bs: 0.0012267089931233756\n",
      "sum of abs dWs for layer 1: 8937431.595522113 and sum of dbs: 3381.4732277286193\n",
      "sum of Ws for layer 1: 321.06496837586735 and sum of bs: 0.0014194800489072085\n",
      "sum of abs dWs for layer 2: 6297156.73347386 and sum of dbs: 9411.419389534949\n",
      "sum of Ws for layer 2: 340.69874601757306 and sum of bs: 0.007057332003685931\n",
      "sum of abs dWs for layer 3: 2762101.928969684 and sum of dbs: 2860.6716432572616\n",
      "sum of Ws for layer 3: 7.313725904169917 and sum of bs: 0.0021165225037617134\n",
      "MSE on the training set: 1077.4273450886012\n",
      "Epoch: 290\n",
      "sum of abs dWs for layer 0: 308737.72015564603 and sum of dbs: 4654.201524751617\n",
      "sum of Ws for layer 0: 86.40243993007671 and sum of bs: 0.0012571809942676687\n",
      "sum of abs dWs for layer 1: 8563098.485997854 and sum of dbs: 3239.6857247108046\n",
      "sum of Ws for layer 1: 321.0875692855243 and sum of bs: 0.0014433782401364736\n",
      "sum of abs dWs for layer 2: 5086135.7637188835 and sum of dbs: 7525.276806784421\n",
      "sum of Ws for layer 2: 340.7136960964142 and sum of bs: 0.0071429064886045\n",
      "sum of abs dWs for layer 3: 2557166.747998263 and sum of dbs: 2612.977278193555\n",
      "sum of Ws for layer 3: 7.3308581061283835 and sum of bs: 0.0021437548675367324\n",
      "MSE on the training set: 898.0055725177136\n",
      "Epoch: 300\n",
      "sum of abs dWs for layer 0: 294905.13949257386 and sum of dbs: 4445.660062594718\n",
      "sum of Ws for layer 0: 86.40428713676293 and sum of bs: 0.0012870677388743818\n",
      "sum of abs dWs for layer 1: 8658218.336682094 and sum of dbs: 3275.5090393682694\n",
      "sum of Ws for layer 1: 321.10921909365874 and sum of bs: 0.001465945835795987\n",
      "sum of abs dWs for layer 2: 4706904.926656809 and sum of dbs: 6900.10050892173\n",
      "sum of Ws for layer 2: 340.7275529932065 and sum of bs: 0.007220620807144602\n",
      "sum of abs dWs for layer 3: 2357571.962332429 and sum of dbs: 2379.269646671618\n",
      "sum of Ws for layer 3: 7.346925531435927 and sum of bs: 0.0021685883720260093\n",
      "MSE on the training set: 744.3843036988345\n",
      "Epoch: 310\n",
      "sum of abs dWs for layer 0: 266749.3133215815 and sum of dbs: 4021.1997050591485\n",
      "sum of Ws for layer 0: 86.40601852190093 and sum of bs: 0.0013154631056773217\n",
      "sum of abs dWs for layer 1: 7356549.444184827 and sum of dbs: 2782.939113963917\n",
      "sum of Ws for layer 1: 321.12983902087797 and sum of bs: 0.0014873266473263151\n",
      "sum of abs dWs for layer 2: 4327222.8477094285 and sum of dbs: 6289.156264352828\n",
      "sum of Ws for layer 2: 340.7405921070275 and sum of bs: 0.0072921277512622085\n",
      "sum of abs dWs for layer 3: 2165492.560421415 and sum of dbs: 2160.3327699923807\n",
      "sum of Ws for layer 3: 7.361873787117986 and sum of bs: 0.002191167553716257\n",
      "MSE on the training set: 613.3254644135984\n",
      "Epoch: 320\n",
      "sum of abs dWs for layer 0: 247684.73229829696 and sum of dbs: 3733.766879028947\n",
      "sum of Ws for layer 0: 86.40763190557225 and sum of bs: 0.0013423596704299086\n",
      "sum of abs dWs for layer 1: 7034376.010067569 and sum of dbs: 2660.8731324438536\n",
      "sum of Ws for layer 1: 321.14903106149035 and sum of bs: 0.0015067216942396791\n",
      "sum of abs dWs for layer 2: 4782015.54370202 and sum of dbs: 6895.665846444517\n",
      "sum of Ws for layer 2: 340.752564292383 and sum of bs: 0.007357302813525027\n",
      "sum of abs dWs for layer 3: 1981823.7869116724 and sum of dbs: 1956.4563137592634\n",
      "sum of Ws for layer 3: 7.375627857177055 and sum of bs: 0.0022116371254806482\n",
      "MSE on the training set: 502.6787401178512\n",
      "Epoch: 330\n",
      "sum of abs dWs for layer 0: 238212.4780657551 and sum of dbs: 3589.649933534469\n",
      "sum of Ws for layer 0: 86.40912675169838 and sum of bs: 0.001366855470769533\n",
      "sum of abs dWs for layer 1: 6214671.6700339075 and sum of dbs: 2349.775161775002\n",
      "sum of Ws for layer 1: 321.16658198030177 and sum of bs: 0.0015245212176974092\n",
      "sum of abs dWs for layer 2: 4442294.703248115 and sum of dbs: 6355.545122174905\n",
      "sum of Ws for layer 2: 340.7638402750738 and sum of bs: 0.007415948553938417\n",
      "sum of abs dWs for layer 3: 1806442.7425855661 and sum of dbs: 1766.4052702781948\n",
      "sum of Ws for layer 3: 7.388233148584423 and sum of bs: 0.0022301446255428127\n",
      "MSE on the training set: 409.88276756544457\n",
      "Epoch: 340\n",
      "sum of abs dWs for layer 0: 217916.6536201697 and sum of dbs: 3283.785441900517\n",
      "sum of Ws for layer 0: 86.41049914094614 and sum of bs: 0.0013897543692834961\n",
      "sum of abs dWs for layer 1: 6077113.93143416 and sum of dbs: 2297.8846324976057\n",
      "sum of Ws for layer 1: 321.1827241058427 and sum of bs: 0.0015413014688446603\n",
      "sum of abs dWs for layer 2: 3535556.757410084 and sum of dbs: 5026.544717432543\n",
      "sum of Ws for layer 2: 340.7740379341619 and sum of bs: 0.0074687714899812395\n",
      "sum of abs dWs for layer 3: 1642201.4150577188 and sum of dbs: 1591.6386424202326\n",
      "sum of Ws for layer 3: 7.399743171783184 and sum of bs: 0.002246837411462723\n",
      "MSE on the training set: 332.52349792607856\n",
      "Epoch: 350\n",
      "sum of abs dWs for layer 0: 185006.08283977146 and sum of dbs: 2788.830226464917\n",
      "sum of Ws for layer 0: 86.41177195683883 and sum of bs: 0.0014101967020415333\n",
      "sum of abs dWs for layer 1: 4977480.482036888 and sum of dbs: 1882.3649330021512\n",
      "sum of Ws for layer 1: 321.19726861117454 and sum of bs: 0.0015562669653181166\n",
      "sum of abs dWs for layer 2: 3092952.023608195 and sum of dbs: 4372.478921736609\n",
      "sum of Ws for layer 2: 340.7830955870169 and sum of bs: 0.007515190152771162\n",
      "sum of abs dWs for layer 3: 1489172.2960539872 and sum of dbs: 1432.0055933164276\n",
      "sum of Ws for layer 3: 7.410217689289542 and sum of bs: 0.0022618606789322447\n",
      "MSE on the training set: 269.0606245173297\n",
      "Epoch: 360\n",
      "sum of abs dWs for layer 0: 168742.328196142 and sum of dbs: 2543.4671571089057\n",
      "sum of Ws for layer 0: 86.41293941939725 and sum of bs: 0.0014292814776457625\n",
      "sum of abs dWs for layer 1: 4777765.761004267 and sum of dbs: 1805.6376102783183\n",
      "sum of Ws for layer 1: 321.21037095037275 and sum of bs: 0.00157024957901002\n",
      "sum of abs dWs for layer 2: 2632467.298052691 and sum of dbs: 3699.9797170436846\n",
      "sum of Ws for layer 2: 340.79112146422756 and sum of bs: 0.007555136782454653\n",
      "sum of abs dWs for layer 3: 1348348.8282482224 and sum of dbs: 1287.0972134036845\n",
      "sum of Ws for layer 3: 7.419730717961474 and sum of bs: 0.0022753725086469977\n",
      "MSE on the training set: 217.27630728566533\n",
      "Epoch: 370\n",
      "sum of abs dWs for layer 0: 153506.2105721807 and sum of dbs: 2313.996553763512\n",
      "sum of Ws for layer 0: 86.41399923032077 and sum of bs: 0.0014472609968802976\n",
      "sum of abs dWs for layer 1: 4026708.8364986163 and sum of dbs: 1522.9180775364985\n",
      "sum of Ws for layer 1: 321.22231469246617 and sum of bs: 0.0015828408387772446\n",
      "sum of abs dWs for layer 2: 2378359.0478546936 and sum of dbs: 3327.929942848082\n",
      "sum of Ws for layer 2: 340.79857453854356 and sum of bs: 0.007591302850738905\n",
      "sum of abs dWs for layer 3: 1217479.1315867326 and sum of dbs: 1154.4665880653677\n",
      "sum of Ws for layer 3: 7.428347087050114 and sum of bs: 0.0022875039257114808\n",
      "MSE on the training set: 174.74485569129737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380\n",
      "sum of abs dWs for layer 0: 143934.3717111443 and sum of dbs: 2168.156363495225\n",
      "sum of Ws for layer 0: 86.41496309550149 and sum of bs: 0.0014637048851797937\n",
      "sum of abs dWs for layer 1: 3886156.1272028573 and sum of dbs: 1468.4797209958415\n",
      "sum of Ws for layer 1: 321.2331817672713 and sum of bs: 0.0015942908126983014\n",
      "sum of abs dWs for layer 2: 2158866.8314894047 and sum of dbs: 3005.0675456447393\n",
      "sum of Ws for layer 2: 340.80538440998225 and sum of bs: 0.007623384474688269\n",
      "sum of abs dWs for layer 3: 1096819.8933950413 and sum of dbs: 1033.8385569019279\n",
      "sum of Ws for layer 3: 7.436130296371164 and sum of bs: 0.0022983761086156043\n",
      "MSE on the training set: 140.18468727692593\n",
      "Epoch: 390\n",
      "sum of abs dWs for layer 0: 128328.47578411791 and sum of dbs: 1932.5321754310098\n",
      "sum of Ws for layer 0: 86.41584525667703 and sum of bs: 0.0014782401866083256\n",
      "sum of abs dWs for layer 1: 3488858.4952135836 and sum of dbs: 1316.7462018564\n",
      "sum of Ws for layer 1: 321.2428988175078 and sum of bs: 0.0016042504717017018\n",
      "sum of abs dWs for layer 2: 1931636.7773168571 and sum of dbs: 2673.88104644619\n",
      "sum of Ws for layer 2: 340.8115719456501 and sum of bs: 0.007652409041986413\n",
      "sum of abs dWs for layer 3: 985927.4950748082 and sum of dbs: 924.3661107513431\n",
      "sum of Ws for layer 3: 7.4431442163133354 and sum of bs: 0.0023081042428974227\n",
      "MSE on the training set: 111.96662452336278\n",
      "Epoch: 400\n",
      "sum of abs dWs for layer 0: 115060.04515635071 and sum of dbs: 1734.3937170547667\n",
      "sum of Ws for layer 0: 86.41663793872083 and sum of bs: 0.0014914311965710842\n",
      "sum of abs dWs for layer 1: 2994944.828266681 and sum of dbs: 1132.589009452422\n",
      "sum of Ws for layer 1: 321.25162052027576 and sum of bs: 0.0016135528382331043\n",
      "sum of abs dWs for layer 2: 1847603.5586430256 and sum of dbs: 2554.577988248061\n",
      "sum of Ws for layer 2: 340.81712478103736 and sum of bs: 0.007678627101987313\n",
      "sum of abs dWs for layer 3: 884611.527585153 and sum of dbs: 825.3524682517847\n",
      "sum of Ws for layer 3: 7.449450726941698 and sum of bs: 0.002316795537420096\n",
      "MSE on the training set: 89.2486494585051\n",
      "Epoch: 410\n",
      "sum of abs dWs for layer 0: 103347.39336223308 and sum of dbs: 1557.8184242694133\n",
      "sum of Ws for layer 0: 86.41735086348369 and sum of bs: 0.001503402337178375\n",
      "sum of abs dWs for layer 1: 2688448.1764315004 and sum of dbs: 1016.6507112875203\n",
      "sum of Ws for layer 1: 321.2595400496132 and sum of bs: 0.0016218710828788717\n",
      "sum of abs dWs for layer 2: 1656070.1895100863 and sum of dbs: 2282.44054886461\n",
      "sum of Ws for layer 2: 340.82209833788176 and sum of bs: 0.007701666301476644\n",
      "sum of abs dWs for layer 3: 792342.0902214658 and sum of dbs: 736.0674337802703\n",
      "sum of Ws for layer 3: 7.4551098045972894 and sum of bs: 0.0023245500522844865\n",
      "MSE on the training set: 70.93784844469748\n",
      "Epoch: 420\n",
      "sum of abs dWs for layer 0: 92631.59007166963 and sum of dbs: 1396.271626971003\n",
      "sum of Ws for layer 0: 86.41799511054212 and sum of bs: 0.001513979489386605\n",
      "sum of abs dWs for layer 1: 2408225.7269678027 and sum of dbs: 910.6546211624132\n",
      "sum of Ws for layer 1: 321.266552688008 and sum of bs: 0.001629318012623757\n",
      "sum of abs dWs for layer 2: 1481508.683642257 and sum of dbs: 2036.0129993871435\n",
      "sum of Ws for layer 2: 340.82660854901997 and sum of bs: 0.007722306708271333\n",
      "sum of abs dWs for layer 3: 708375.9460296709 and sum of dbs: 655.5129650774147\n",
      "sum of Ws for layer 3: 7.46017846548762 and sum of bs: 0.002331460041245355\n",
      "MSE on the training set: 56.26054791630482\n",
      "Epoch: 430\n",
      "sum of abs dWs for layer 0: 83248.72335165186 and sum of dbs: 1254.8187981459735\n",
      "sum of Ws for layer 0: 86.4185706763022 and sum of bs: 0.0015236135042640709\n",
      "sum of abs dWs for layer 1: 2249496.7510400866 and sum of dbs: 850.6056388407563\n",
      "sum of Ws for layer 1: 321.2729045865054 and sum of bs: 0.0016359904426933758\n",
      "sum of abs dWs for layer 2: 1243833.0564454636 and sum of dbs: 1705.0021634098018\n",
      "sum of Ws for layer 2: 340.83061059968384 and sum of bs: 0.0077406059472696406\n",
      "sum of abs dWs for layer 3: 632438.6074288327 and sum of dbs: 583.229318231018\n",
      "sum of Ws for layer 3: 7.464711012620238 and sum of bs: 0.0023376109134706172\n",
      "MSE on the training set: 44.53247652560631\n",
      "Epoch: 440\n",
      "sum of abs dWs for layer 0: 73269.7121871833 and sum of dbs: 1104.096458474814\n",
      "sum of Ws for layer 0: 86.41908717782073 and sum of bs: 0.0015321561441704117\n",
      "sum of abs dWs for layer 1: 1955047.007938943 and sum of dbs: 739.5046366843081\n",
      "sum of Ws for layer 1: 321.27853009362786 and sum of bs: 0.0016420341830212471\n",
      "sum of abs dWs for layer 2: 1186782.6562874224 and sum of dbs: 1620.47749493823\n",
      "sum of Ws for layer 2: 340.83419389384653 and sum of bs: 0.007757055660021876\n",
      "sum of abs dWs for layer 3: 563887.5123713886 and sum of dbs: 518.411489764459\n",
      "sum of Ws for layer 3: 7.468758086562524 and sum of bs: 0.0023430805691145957\n",
      "MSE on the training set: 35.19294193085733\n",
      "Epoch: 450\n",
      "sum of abs dWs for layer 0: 64828.49680152518 and sum of dbs: 977.1038092538782\n",
      "sum of Ws for layer 0: 86.41955125050085 and sum of bs: 0.0015397041624111304\n",
      "sum of abs dWs for layer 1: 1798976.8009938502 and sum of dbs: 680.0610742989138\n",
      "sum of Ws for layer 1: 321.2835566859442 and sum of bs: 0.0016472519332249378\n",
      "sum of abs dWs for layer 2: 988166.2327350418 and sum of dbs: 1348.48483509276\n",
      "sum of Ws for layer 2: 340.8374069664553 and sum of bs: 0.007771540024437431\n",
      "sum of abs dWs for layer 3: 502175.41859394073 and sum of dbs: 460.409843107834\n",
      "sum of Ws for layer 3: 7.472366827390161 and sum of bs: 0.0023479400602617827\n",
      "MSE on the training set: 27.749070282673188\n",
      "Epoch: 460\n",
      "sum of abs dWs for layer 0: 61099.98681289468 and sum of dbs: 919.126824787247\n",
      "sum of Ws for layer 0: 86.41996430483965 and sum of bs: 0.0015465068204334434\n",
      "sum of abs dWs for layer 1: 1576783.5726496906 and sum of dbs: 595.4442231224256\n",
      "sum of Ws for layer 1: 321.28802732934696 and sum of bs: 0.0016519247546160516\n",
      "sum of abs dWs for layer 2: 1000557.9426002114 and sum of dbs: 1358.3159900782791\n",
      "sum of Ws for layer 2: 340.84028519448606 and sum of bs: 0.007784618421899118\n",
      "sum of abs dWs for layer 3: 446681.06873355823 and sum of dbs: 408.51537644608953\n",
      "sum of Ws for layer 3: 7.47558084050375 and sum of bs: 0.0023522539765973654\n",
      "MSE on the training set: 21.85438286666736\n",
      "Epoch: 470\n",
      "sum of abs dWs for layer 0: 52389.4570837699 and sum of dbs: 789.5788059130525\n",
      "sum of Ws for layer 0: 86.42033157582344 and sum of bs: 0.0015525966159493922\n",
      "sum of abs dWs for layer 1: 1358972.4979159469 and sum of dbs: 513.7948784744406\n",
      "sum of Ws for layer 1: 321.2920443382592 and sum of bs: 0.00165613382400302\n",
      "sum of abs dWs for layer 2: 832262.9557857242 and sum of dbs: 1131.9310175771996\n",
      "sum of Ws for layer 2: 340.84283872276166 and sum of bs: 0.007795916114686543\n",
      "sum of abs dWs for layer 3: 397012.69292285957 and sum of dbs: 362.29292361411905\n",
      "sum of Ws for layer 3: 7.478440278079064 and sum of bs: 0.002356080882069195\n",
      "MSE on the training set: 17.18036274738607\n",
      "Epoch: 480\n",
      "sum of abs dWs for layer 0: 46624.69228192292 and sum of dbs: 702.672370221777\n",
      "sum of Ws for layer 0: 86.42065987586608 and sum of bs: 0.0015579666326827981\n",
      "sum of abs dWs for layer 1: 1211263.643211652 and sum of dbs: 457.9283294003798\n",
      "sum of Ws for layer 1: 321.29563927670984 and sum of bs: 0.001659887711196404\n",
      "sum of abs dWs for layer 2: 739333.8237914294 and sum of dbs: 1004.076423302139\n",
      "sum of Ws for layer 2: 340.84510610984546 and sum of bs: 0.007806215539343151\n",
      "sum of abs dWs for layer 3: 352570.4972810723 and sum of dbs: 321.10638285540074\n",
      "sum of Ws for layer 3: 7.480981729483003 and sum of bs: 0.0023594734638081784\n",
      "MSE on the training set: 13.493958538949975\n",
      "Epoch: 490\n",
      "sum of abs dWs for layer 0: 41421.50317703127 and sum of dbs: 624.2451057241442\n",
      "sum of Ws for layer 0: 86.42095130753431 and sum of bs: 0.001562763981018666\n",
      "sum of abs dWs for layer 1: 1075751.413083149 and sum of dbs: 406.6832786157454\n",
      "sum of Ws for layer 1: 321.29893799282445 and sum of bs: 0.0016632161708272095\n",
      "sum of abs dWs for layer 2: 656300.6010549803 and sum of dbs: 890.179063078186\n",
      "sum of Ws for layer 2: 340.84711654147304 and sum of bs: 0.007815151404684723\n",
      "sum of abs dWs for layer 3: 312887.7287408182 and sum of dbs: 284.4668080090632\n",
      "sum of Ws for layer 3: 7.4832386193948395 and sum of bs: 0.0023624792773337903\n",
      "MSE on the training set: 10.58560774496646\n",
      "Epoch: 500\n",
      "sum of abs dWs for layer 0: 36818.17897783752 and sum of dbs: 551.5119833175859\n",
      "sum of Ws for layer 0: 86.42121050922898 and sum of bs: 0.0015669984793946132\n",
      "sum of abs dWs for layer 1: 985868.1899257645 and sum of dbs: 369.5479558861572\n",
      "sum of Ws for layer 1: 321.30186644898237 and sum of bs: 0.0016661811468065074\n",
      "sum of abs dWs for layer 2: 603621.8582192317 and sum of dbs: 810.5805643631642\n",
      "sum of Ws for layer 2: 340.84891745067614 and sum of bs: 0.007823141721625277\n",
      "sum of abs dWs for layer 3: 277418.6978130788 and sum of dbs: 251.8256271491123\n",
      "sum of Ws for layer 3: 7.4852412638675805 and sum of bs: 0.002365141028424205\n",
      "MSE on the training set: 8.29986542269056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 510\n",
      "sum of abs dWs for layer 0: 32584.00563603035 and sum of dbs: 491.0168719320023\n",
      "sum of Ws for layer 0: 86.42144043794788 and sum of bs: 0.0015707809689113846\n",
      "sum of abs dWs for layer 1: 844315.0772322287 and sum of dbs: 319.1591616950945\n",
      "sum of Ws for layer 1: 321.3045507430844 and sum of bs: 0.001668805267059063\n",
      "sum of abs dWs for layer 2: 516029.0132095722 and sum of dbs: 698.3886960732718\n",
      "sum of Ws for layer 2: 340.8504992612479 and sum of bs: 0.007830182474264685\n",
      "sum of abs dWs for layer 3: 245900.99869092513 and sum of dbs: 222.9052783080905\n",
      "sum of Ws for layer 3: 7.487017164266524 and sum of bs: 0.002367497114642818\n",
      "MSE on the training set: 6.500000365469749\n",
      "Epoch: 520\n",
      "sum of abs dWs for layer 0: 28769.271553660674 and sum of dbs: 430.0426130243364\n",
      "sum of Ws for layer 0: 86.42164422917159 and sum of bs: 0.0015741222832828048\n",
      "sum of abs dWs for layer 1: 764868.149147746 and sum of dbs: 288.89459031234173\n",
      "sum of Ws for layer 1: 321.30692169417546 and sum of bs: 0.0016711503544920919\n",
      "sum of abs dWs for layer 2: 423345.4996038937 and sum of dbs: 566.4124077274232\n",
      "sum of Ws for layer 2: 340.85190773399796 and sum of bs: 0.007836406295443445\n",
      "sum of abs dWs for layer 3: 217733.55979272776 and sum of dbs: 197.1301674457959\n",
      "sum of Ws for layer 3: 7.488591014505631 and sum of bs: 0.0023695817508286167\n",
      "MSE on the training set: 5.0877346753223\n",
      "Epoch: 530\n",
      "sum of abs dWs for layer 0: 25009.538502049505 and sum of dbs: 376.63698789446033\n",
      "sum of Ws for layer 0: 86.42182690141696 and sum of bs: 0.0015770526104896413\n",
      "sum of abs dWs for layer 1: 665158.3731170761 and sum of dbs: 251.33408640153147\n",
      "sum of Ws for layer 1: 321.3090021148721 and sum of bs: 0.0016731541714049667\n",
      "sum of abs dWs for layer 2: 428841.3111120565 and sum of dbs: 577.841827938611\n",
      "sum of Ws for layer 2: 340.85315967546603 and sum of bs: 0.007841973446999275\n",
      "sum of abs dWs for layer 3: 192769.39785108203 and sum of dbs: 174.3328975931052\n",
      "sum of Ws for layer 3: 7.489985042973857 and sum of bs: 0.002371425529757301\n",
      "MSE on the training set: 3.9783219515305333\n",
      "Epoch: 540\n",
      "sum of abs dWs for layer 0: 22748.72063379664 and sum of dbs: 342.6463099438424\n",
      "sum of Ws for layer 0: 86.42198650478817 and sum of bs: 0.0015797037457698415\n",
      "sum of abs dWs for layer 1: 611237.6099783202 and sum of dbs: 230.97268416075252\n",
      "sum of Ws for layer 1: 321.310868245565 and sum of bs: 0.0016749884542029028\n",
      "sum of abs dWs for layer 2: 336756.2668922163 and sum of dbs: 454.27659029147753\n",
      "sum of Ws for layer 2: 340.85427031062545 and sum of bs: 0.00784681522760967\n",
      "sum of abs dWs for layer 3: 170579.17324736284 and sum of dbs: 154.1109762478295\n",
      "sum of Ws for layer 3: 7.491219219657153 and sum of bs: 0.0023730557666500553\n",
      "MSE on the training set: 3.1096074494166617\n",
      "Epoch: 550\n",
      "sum of abs dWs for layer 0: 20065.70091307956 and sum of dbs: 302.2741105434899\n",
      "sum of Ws for layer 0: 86.42212909863785 and sum of bs: 0.0015820164222349145\n",
      "sum of abs dWs for layer 1: 520459.4757632524 and sum of dbs: 196.6765619505921\n",
      "sum of Ws for layer 1: 321.31250840816153 and sum of bs: 0.0016765773214205667\n",
      "sum of abs dWs for layer 2: 316916.90507483145 and sum of dbs: 427.4935321871148\n",
      "sum of Ws for layer 2: 340.8552457185954 and sum of bs: 0.007851155438904885\n",
      "sum of abs dWs for layer 3: 150917.40436671735 and sum of dbs: 136.22409434871898\n",
      "sum of Ws for layer 3: 7.492311398890996 and sum of bs: 0.002374496758931522\n",
      "MSE on the training set: 2.4286865266155795\n",
      "Epoch: 560\n",
      "sum of abs dWs for layer 0: 17815.27332956449 and sum of dbs: 268.35468776124156\n",
      "sum of Ws for layer 0: 86.42225468987725 and sum of bs: 0.0015840786210158673\n",
      "sum of abs dWs for layer 1: 478486.47570113215 and sum of dbs: 180.80086791949657\n",
      "sum of Ws for layer 1: 321.3139651560759 and sum of bs: 0.0016780167680688976\n",
      "sum of abs dWs for layer 2: 263554.61999429273 and sum of dbs: 355.26845966439635\n",
      "sum of Ws for layer 2: 340.85611532180934 and sum of bs: 0.007854940407944945\n",
      "sum of abs dWs for layer 3: 133448.82713455835 and sum of dbs: 120.35773024814753\n",
      "sum of Ws for layer 3: 7.49327753553324 and sum of bs: 0.0023757701181928103\n",
      "MSE on the training set: 1.8962641616677447\n",
      "Epoch: 570\n",
      "sum of abs dWs for layer 0: 15687.526257589243 and sum of dbs: 236.2860072244201\n",
      "sum of Ws for layer 0: 86.42236621343027 and sum of bs: 0.0015858916594986556\n",
      "sum of abs dWs for layer 1: 406126.4963026107 and sum of dbs: 153.4467731145418\n",
      "sum of Ws for layer 1: 321.3152515856382 and sum of bs: 0.0016792685243673853\n",
      "sum of abs dWs for layer 2: 247811.17940074956 and sum of dbs: 333.89266940460203\n",
      "sum of Ws for layer 2: 340.85688260389315 and sum of bs: 0.00785829849312512\n",
      "sum of abs dWs for layer 3: 117982.71964561325 and sum of dbs: 106.32936491069037\n",
      "sum of Ws for layer 3: 7.494131932637097 and sum of bs: 0.002376895140965831\n",
      "MSE on the training set: 1.479949326771733\n",
      "Epoch: 580\n",
      "sum of abs dWs for layer 0: 13893.926438541932 and sum of dbs: 209.01936931787588\n",
      "sum of Ws for layer 0: 86.4224647438565 and sum of bs: 0.0015875012270517434\n",
      "sum of abs dWs for layer 1: 373399.06413435837 and sum of dbs: 140.8238178480644\n",
      "sum of Ws for layer 1: 321.31638588872306 and sum of bs: 0.0016803907040129937\n",
      "sum of abs dWs for layer 2: 204341.21651244926 and sum of dbs: 272.97292498000206\n",
      "sum of Ws for layer 2: 340.8575603062891 and sum of bs: 0.007861290960378346\n",
      "sum of abs dWs for layer 3: 104269.54548711436 and sum of dbs: 93.9072009329689\n",
      "sum of Ws for layer 3: 7.494887257168029 and sum of bs: 0.0023778888545137765\n",
      "MSE on the training set: 1.1549062099084713\n",
      "Epoch: 590\n",
      "sum of abs dWs for layer 0: 12507.801520435703 and sum of dbs: 187.36411796487673\n",
      "sum of Ws for layer 0: 86.42255179866301 and sum of bs: 0.0015889298610201495\n",
      "sum of abs dWs for layer 1: 335469.41994900786 and sum of dbs: 125.94468746226016\n",
      "sum of Ws for layer 1: 321.31739303781666 and sum of bs: 0.0016813669224672854\n",
      "sum of abs dWs for layer 2: 205630.2044753722 and sum of dbs: 275.21683103435066\n",
      "sum of Ws for layer 2: 340.85815936040996 and sum of bs: 0.00786390903821459\n",
      "sum of abs dWs for layer 3: 92139.06739430391 and sum of dbs: 82.92815562985012\n",
      "sum of Ws for layer 3: 7.495554823811567 and sum of bs: 0.002378766421322073\n",
      "MSE on the training set: 0.9008163064220929\n",
      "Epoch: 600\n",
      "sum of abs dWs for layer 0: 10833.662296314327 and sum of dbs: 163.09550656084764\n",
      "sum of Ws for layer 0: 86.42262888960424 and sum of bs: 0.0015901797514478566\n",
      "sum of abs dWs for layer 1: 280396.9062809017 and sum of dbs: 105.89970224781904\n",
      "sum of Ws for layer 1: 321.318278178145 and sum of bs: 0.0016822440932559991\n",
      "sum of abs dWs for layer 2: 171013.59852175147 and sum of dbs: 230.02921175166776\n",
      "sum of Ws for layer 2: 340.85869005281165 and sum of bs: 0.007866234229459267\n",
      "sum of abs dWs for layer 3: 81400.09077253005 and sum of dbs: 73.22049000363864\n",
      "sum of Ws for layer 3: 7.496144721283862 and sum of bs: 0.002379541323081972\n",
      "MSE on the training set: 0.702420260532593\n",
      "Epoch: 610\n",
      "sum of abs dWs for layer 0: 9572.319664643775 and sum of dbs: 144.0906074800326\n",
      "sum of Ws for layer 0: 86.42269688017987 and sum of bs: 0.0015912945367242387\n",
      "sum of abs dWs for layer 1: 247733.30530761415 and sum of dbs: 93.54762326234734\n",
      "sum of Ws for layer 1: 321.3190667383758 and sum of bs: 0.0016830128802993003\n",
      "sum of abs dWs for layer 2: 151075.28538600542 and sum of dbs: 203.13842344441807\n",
      "sum of Ws for layer 2: 340.85915778782254 and sum of bs: 0.007868257306660265\n",
      "sum of abs dWs for layer 3: 71904.70098666509 and sum of dbs: 64.64311668874802\n",
      "sum of Ws for layer 3: 7.496665853215842 and sum of bs: 0.0023802254320773506\n",
      "MSE on the training set: 0.5474910548867176\n",
      "Epoch: 620\n",
      "sum of abs dWs for layer 0: 8762.190733021951 and sum of dbs: 131.50774811979113\n",
      "sum of Ws for layer 0: 86.42275717041551 and sum of bs: 0.0015922681078387058\n",
      "sum of abs dWs for layer 1: 225616.72539610695 and sum of dbs: 84.88740582631544\n",
      "sum of Ws for layer 1: 321.31975784470796 and sum of bs: 0.0016836891825550882\n",
      "sum of abs dWs for layer 2: 142443.19606934584 and sum of dbs: 190.5993823046943\n",
      "sum of Ws for layer 2: 340.85957069067933 and sum of bs: 0.007870077017340599\n",
      "sum of abs dWs for layer 3: 63502.61529774268 and sum of dbs: 57.05855201064111\n",
      "sum of Ws for layer 3: 7.4971261588516676 and sum of bs: 0.002380829306898679\n",
      "MSE on the training set: 0.42679997840306705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 630\n",
      "sum of abs dWs for layer 0: 7530.679967138607 and sum of dbs: 112.33048696198611\n",
      "sum of Ws for layer 0: 86.42281031363403 and sum of bs: 0.0015931328189869916\n",
      "sum of abs dWs for layer 1: 200945.29526425988 and sum of dbs: 74.15428634526754\n",
      "sum of Ws for layer 1: 321.3203691296411 and sum of bs: 0.0016842942146646368\n",
      "sum of abs dWs for layer 2: 123051.3125071991 and sum of dbs: 162.90799610952277\n",
      "sum of Ws for layer 2: 340.85993605948454 and sum of bs: 0.00787166837133475\n",
      "sum of abs dWs for layer 3: 56065.869921629754 and sum of dbs: 50.35092445634797\n",
      "sum of Ws for layer 3: 7.497532671683527 and sum of bs: 0.0023813622888906637\n",
      "MSE on the training set: 0.33260164074464216\n",
      "Epoch: 640\n",
      "sum of abs dWs for layer 0: 6755.082341341331 and sum of dbs: 100.77832092545005\n",
      "sum of Ws for layer 0: 86.42285719766173 and sum of bs: 0.0015938991690624786\n",
      "sum of abs dWs for layer 1: 177731.01573162852 and sum of dbs: 65.47779610704596\n",
      "sum of Ws for layer 1: 321.32091232623173 and sum of bs: 0.0016848235421670858\n",
      "sum of abs dWs for layer 2: 102885.00208581054 and sum of dbs: 135.95563822475512\n",
      "sum of Ws for layer 2: 340.86025824209145 and sum of bs: 0.00787305861974435\n",
      "sum of abs dWs for layer 3: 49496.39161765926 and sum of dbs: 44.42847352967193\n",
      "sum of Ws for layer 3: 7.497891625615854 and sum of bs: 0.0023818326421373237\n",
      "MSE on the training set: 0.259204333383392\n",
      "Epoch: 650\n",
      "sum of abs dWs for layer 0: 5775.477099858806 and sum of dbs: 86.99194589055303\n",
      "sum of Ws for layer 0: 86.42289863510064 and sum of bs: 0.0015945750964682828\n",
      "sum of abs dWs for layer 1: 152307.37505312124 and sum of dbs: 57.17664154931126\n",
      "sum of Ws for layer 1: 321.3213880683142 and sum of bs: 0.0016852841861653347\n",
      "sum of abs dWs for layer 2: 93458.67271610606 and sum of dbs: 124.67994792919768\n",
      "sum of Ws for layer 2: 340.86054507173424 and sum of bs: 0.007874307681444488\n",
      "sum of abs dWs for layer 3: 43699.41721311222 and sum of dbs: 39.20498705198999\n",
      "sum of Ws for layer 3: 7.4982085586386304 and sum of bs: 0.002382247695353783\n",
      "MSE on the training set: 0.20181550893010605\n",
      "Epoch: 660\n",
      "sum of abs dWs for layer 0: 5141.177396872486 and sum of dbs: 77.0952379813937\n",
      "sum of Ws for layer 0: 86.42293514043786 and sum of bs: 0.0015951742835806787\n",
      "sum of abs dWs for layer 1: 141097.885624912 and sum of dbs: 52.871768632535606\n",
      "sum of Ws for layer 1: 321.3218119301423 and sum of bs: 0.0016856965635600328\n",
      "sum of abs dWs for layer 2: 78390.75562436113 and sum of dbs: 104.2426334039945\n",
      "sum of Ws for layer 2: 340.860796764896 and sum of bs: 0.007875389651865896\n",
      "sum of abs dWs for layer 3: 38571.378820699814 and sum of dbs: 34.5864419370051\n",
      "sum of Ws for layer 3: 7.498488339247365 and sum of bs: 0.0023826138857291138\n",
      "MSE on the training set: 0.15719767592285336\n",
      "Epoch: 670\n",
      "sum of abs dWs for layer 0: 4466.903061613417 and sum of dbs: 67.14619682122239\n",
      "sum of Ws for layer 0: 86.4229675245648 and sum of bs: 0.001595696549687712\n",
      "sum of abs dWs for layer 1: 115670.78753726254 and sum of dbs: 43.62220627968555\n",
      "sum of Ws for layer 1: 321.3221820891288 and sum of bs: 0.0016860616070971507\n",
      "sum of abs dWs for layer 2: 67197.54985685763 and sum of dbs: 90.10246681426446\n",
      "sum of Ws for layer 2: 340.86101879561073 and sum of bs: 0.007876357369631543\n",
      "sum of abs dWs for layer 3: 34044.20704398509 and sum of dbs: 30.51111986998209\n",
      "sum of Ws for layer 3: 7.498735311740371 and sum of bs: 0.0023829369491958495\n",
      "MSE on the training set: 0.12239913562891463\n",
      "Epoch: 680\n",
      "sum of abs dWs for layer 0: 3980.4926744058334 and sum of dbs: 59.53693149761051\n",
      "sum of Ws for layer 0: 86.42299607491245 and sum of bs: 0.001596161376611256\n",
      "sum of abs dWs for layer 1: 105866.89344122406 and sum of dbs: 39.603568680959214\n",
      "sum of Ws for layer 1: 321.32251007377306 and sum of bs: 0.0016863825728304234\n",
      "sum of abs dWs for layer 2: 65176.34055367289 and sum of dbs: 86.55380591752511\n",
      "sum of Ws for layer 2: 340.8612146438139 and sum of bs: 0.007877210479120746\n",
      "sum of abs dWs for layer 3: 30052.227449291946 and sum of dbs: 26.917762388503338\n",
      "sum of Ws for layer 3: 7.498953295577066 and sum of bs: 0.0023832219242684153\n",
      "MSE on the training set: 0.09531066038832414\n",
      "Epoch: 690\n",
      "sum of abs dWs for layer 0: 3528.236540971733 and sum of dbs: 52.556909527947774\n",
      "sum of Ws for layer 0: 86.42302119556956 and sum of bs: 0.0015965719953895557\n",
      "sum of abs dWs for layer 1: 93360.0689671734 and sum of dbs: 35.14611582925207\n",
      "sum of Ws for layer 1: 321.32280161647964 and sum of bs: 0.001686667981562276\n",
      "sum of abs dWs for layer 2: 51786.74874220083 and sum of dbs: 68.67527788983995\n",
      "sum of Ws for layer 2: 340.8613871368229 and sum of bs: 0.007877954073980472\n",
      "sum of abs dWs for layer 3: 26520.02352509462 and sum of dbs: 23.740064555019778\n",
      "sum of Ws for layer 3: 7.499145684589679 and sum of bs: 0.0023834732840733053\n",
      "MSE on the training set: 0.07423658973935755\n",
      "Epoch: 700\n",
      "sum of abs dWs for layer 0: 3205.716202951114 and sum of dbs: 47.59076047617622\n",
      "sum of Ws for layer 0: 86.42304348374566 and sum of bs: 0.0015969302995215494\n",
      "sum of abs dWs for layer 1: 81188.37447282781 and sum of dbs: 30.326914658462286\n",
      "sum of Ws for layer 1: 321.3230560222353 and sum of bs: 0.0016869162182516434\n",
      "sum of abs dWs for layer 2: 49157.813190964975 and sum of dbs: 65.06138781241016\n",
      "sum of Ws for layer 2: 340.86154021871687 and sum of bs: 0.007878616202893889\n",
      "sum of abs dWs for layer 3: 23401.661806887438 and sum of dbs: 20.935418349359857\n",
      "sum of Ws for layer 3: 7.49931547481436 and sum of bs: 0.0023836949742400003\n",
      "MSE on the training set: 0.05778908987290455\n",
      "Epoch: 710\n",
      "sum of abs dWs for layer 0: 2856.6568447637565 and sum of dbs: 42.41813606767742\n",
      "sum of Ws for layer 0: 86.4230630876197 and sum of bs: 0.0015972489949071529\n",
      "sum of abs dWs for layer 1: 70735.05843759117 and sum of dbs: 26.426876996621836\n",
      "sum of Ws for layer 1: 321.3232817581953 and sum of bs: 0.0016871387217154436\n",
      "sum of abs dWs for layer 2: 40631.59883806325 and sum of dbs: 53.811708894053986\n",
      "sum of Ws for layer 2: 340.86167504848765 and sum of bs: 0.007879196880737132\n",
      "sum of abs dWs for layer 3: 20651.126700070763 and sum of dbs: 18.46206480544943\n",
      "sum of Ws for layer 3: 7.499465302355903 and sum of bs: 0.002383890464398834\n",
      "MSE on the training set: 0.0449937113262117\n",
      "Epoch: 720\n",
      "sum of abs dWs for layer 0: 2389.1812747312892 and sum of dbs: 35.719815702916776\n",
      "sum of Ws for layer 0: 86.42308040730252 and sum of bs: 0.001597530257345609\n",
      "sum of abs dWs for layer 1: 61134.026853600306 and sum of dbs: 22.97773541077118\n",
      "sum of Ws for layer 1: 321.3234806972557 and sum of bs: 0.0016873327532703462\n",
      "sum of abs dWs for layer 2: 39351.768690637255 and sum of dbs: 51.45418572317902\n",
      "sum of Ws for layer 2: 340.8617941788375 and sum of bs: 0.007879709931389648\n",
      "sum of abs dWs for layer 3: 18220.565234767684 and sum of dbs: 16.276799764975014\n",
      "sum of Ws for layer 3: 7.499597515673051 and sum of bs: 0.002384062843109419\n",
      "MSE on the training set: 0.035022917653331403\n",
      "Epoch: 730\n",
      "sum of abs dWs for layer 0: 2142.681262620759 and sum of dbs: 32.05114204714224\n",
      "sum of Ws for layer 0: 86.42309567233559 and sum of bs: 0.0015977786867870118\n",
      "sum of abs dWs for layer 1: 55448.4106613716 and sum of dbs: 20.827817702169234\n",
      "sum of Ws for layer 1: 321.32365661991633 and sum of bs: 0.0016875034083916978\n",
      "sum of abs dWs for layer 2: 33774.3443596499 and sum of dbs: 45.0270312503796\n",
      "sum of Ws for layer 2: 340.86189949806277 and sum of bs: 0.007880160839063647\n",
      "sum of abs dWs for layer 3: 16079.342954974582 and sum of dbs: 14.351918555454896\n",
      "sum of Ws for layer 3: 7.4997141835049135 and sum of bs: 0.0023842148291259137\n",
      "MSE on the training set: 0.02725652901410685\n",
      "Epoch: 740\n",
      "sum of abs dWs for layer 0: 1846.369012236005 and sum of dbs: 27.603472703033876\n",
      "sum of Ws for layer 0: 86.42310919042484 and sum of bs: 0.0015979963343316526\n",
      "sum of abs dWs for layer 1: 48974.44146761867 and sum of dbs: 18.381864567256958\n",
      "sum of Ws for layer 1: 321.3238109967527 and sum of bs: 0.0016876552199929164\n",
      "sum of abs dWs for layer 2: 30641.215923227795 and sum of dbs: 40.13713657615612\n",
      "sum of Ws for layer 2: 340.86199213294844 and sum of bs: 0.007880560481900691\n",
      "sum of abs dWs for layer 3: 14185.122695924545 and sum of dbs: 12.649704817394731\n",
      "sum of Ws for layer 3: 7.499817121284429 and sum of bs: 0.0023843488090460633\n",
      "MSE on the training set: 0.02121928958067899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 750\n",
      "sum of abs dWs for layer 0: 1633.231286024785 and sum of dbs: 24.450381148731378\n",
      "sum of Ws for layer 0: 86.42312107729805 and sum of bs: 0.0015981892369188197\n",
      "sum of abs dWs for layer 1: 43504.49566984249 and sum of dbs: 16.245643184905433\n",
      "sum of Ws for layer 1: 321.3239480189355 and sum of bs: 0.0016877890426006408\n",
      "sum of abs dWs for layer 2: 26862.625764749213 and sum of dbs: 35.21413828274817\n",
      "sum of Ws for layer 2: 340.86207387984945 and sum of bs: 0.007880909985425476\n",
      "sum of abs dWs for layer 3: 12515.616272625346 and sum of dbs: 11.149556554915307\n",
      "sum of Ws for layer 3: 7.499907947999512 and sum of bs: 0.0023844669084091314\n",
      "MSE on the training set: 0.016515926902191728\n",
      "Epoch: 760\n",
      "sum of abs dWs for layer 0: 1426.0789000922737 and sum of dbs: 21.287657583529302\n",
      "sum of Ws for layer 0: 86.42313154800897 and sum of bs: 0.001598359497133317\n",
      "sum of abs dWs for layer 1: 39850.58030010062 and sum of dbs: 14.94122297546569\n",
      "sum of Ws for layer 1: 321.3240692600994 and sum of bs: 0.0016879085372545903\n",
      "sum of abs dWs for layer 2: 22068.37727867135 and sum of dbs: 28.848870354928263\n",
      "sum of Ws for layer 2: 340.86214560390385 and sum of bs: 0.007881217418068695\n",
      "sum of abs dWs for layer 3: 11043.33872785717 and sum of dbs: 9.82674233633536\n",
      "sum of Ws for layer 3: 7.499988084308266 and sum of bs: 0.0023845709924969383\n",
      "MSE on the training set: 0.01285786178970058\n",
      "Epoch: 770\n",
      "sum of abs dWs for layer 0: 1307.502011005872 and sum of dbs: 19.401153453020903\n",
      "sum of Ws for layer 0: 86.42314082208146 and sum of bs: 0.0015985090262291132\n",
      "sum of abs dWs for layer 1: 33424.14472371633 and sum of dbs: 12.446386725134461\n",
      "sum of Ws for layer 1: 321.324175557207 and sum of bs: 0.0016880112493363963\n",
      "sum of abs dWs for layer 2: 20844.72855670507 and sum of dbs: 27.49430962369738\n",
      "sum of Ws for layer 2: 340.8622094454255 and sum of bs: 0.007881489677920761\n",
      "sum of abs dWs for layer 3: 9743.225663604378 and sum of dbs: 8.658535428583665\n",
      "sum of Ws for layer 3: 7.500058786209696 and sum of bs: 0.0023846627097223864\n",
      "MSE on the training set: 0.01000378387887684\n",
      "Epoch: 780\n",
      "sum of abs dWs for layer 0: 1088.5443174816255 and sum of dbs: 16.353766369250152\n",
      "sum of Ws for layer 0: 86.42314898208934 and sum of bs: 0.0015986410221651506\n",
      "sum of abs dWs for layer 1: 31662.612847101882 and sum of dbs: 11.777109800928246\n",
      "sum of Ws for layer 1: 321.32426959705526 and sum of bs: 0.0016881042769493703\n",
      "sum of abs dWs for layer 2: 17351.384373570552 and sum of dbs: 22.5416295933356\n",
      "sum of Ws for layer 2: 340.8622654473728 and sum of bs: 0.007881728720444353\n",
      "sum of abs dWs for layer 3: 8595.036791525548 and sum of dbs: 7.627387856714351\n",
      "sum of Ws for layer 3: 7.500121160970772 and sum of bs: 0.0023847435134918176\n",
      "MSE on the training set: 0.007789397226861807\n",
      "Epoch: 790\n",
      "sum of abs dWs for layer 0: 1006.1642074128055 and sum of dbs: 14.945908086343843\n",
      "sum of Ws for layer 0: 86.4231562003946 and sum of bs: 0.0015987570381333885\n",
      "sum of abs dWs for layer 1: 26204.010186970198 and sum of dbs: 9.738557927918915\n",
      "sum of Ws for layer 1: 321.3243523749077 and sum of bs: 0.0016881837491083888\n",
      "sum of abs dWs for layer 2: 16034.168159761364 and sum of dbs: 21.140192915106393\n",
      "sum of Ws for layer 2: 340.86231510973494 and sum of bs: 0.007881940393795289\n",
      "sum of abs dWs for layer 3: 7583.322164389874 and sum of dbs: 6.718434674455753\n",
      "sum of Ws for layer 3: 7.500176191650455 and sum of bs: 0.002384814692574493\n",
      "MSE on the training set: 0.006060129600595044\n",
      "Epoch: 800\n",
      "sum of abs dWs for layer 0: 895.5129410562131 and sum of dbs: 13.231682345191407\n",
      "sum of Ws for layer 0: 86.42316256425613 and sum of bs: 0.0015988594889695328\n",
      "sum of abs dWs for layer 1: 23047.541626160975 and sum of dbs: 8.529457884205405\n",
      "sum of Ws for layer 1: 321.3244254668815 and sum of bs: 0.0016882551669757011\n",
      "sum of abs dWs for layer 2: 14478.376093568006 and sum of dbs: 18.913559512933364\n",
      "sum of Ws for layer 2: 340.86235877589405 and sum of bs: 0.007882125557266124\n",
      "sum of abs dWs for layer 3: 6689.9617129037715 and sum of dbs: 5.916097754737981\n",
      "sum of Ws for layer 3: 7.500224738057971 and sum of bs: 0.0023848773756290544\n",
      "MSE on the training set: 0.004717075634566431\n",
      "Epoch: 810\n",
      "sum of abs dWs for layer 0: 777.9395238490034 and sum of dbs: 11.525232808402464\n",
      "sum of Ws for layer 0: 86.42316817762345 and sum of bs: 0.0015989496339364321\n",
      "sum of abs dWs for layer 1: 19927.308917319184 and sum of dbs: 7.46535071377491\n",
      "sum of Ws for layer 1: 321.3244898110007 and sum of bs: 0.0016883181157283113\n",
      "sum of abs dWs for layer 2: 12319.707109657718 and sum of dbs: 15.921064622189983\n",
      "sum of Ws for layer 2: 340.86239736888683 and sum of bs: 0.00788228915367977\n",
      "sum of abs dWs for layer 3: 5900.916110713658 and sum of dbs: 5.207737338887263\n",
      "sum of Ws for layer 3: 7.500267565091495 and sum of bs: 0.0023849325653050217\n",
      "MSE on the training set: 0.003671596610313507\n",
      "Epoch: 820\n",
      "sum of abs dWs for layer 0: 691.6439615337019 and sum of dbs: 10.226411252908665\n",
      "sum of Ws for layer 0: 86.42317312247792 and sum of bs: 0.0015990292485784055\n",
      "sum of abs dWs for layer 1: 18394.834509967888 and sum of dbs: 6.836748213523556\n",
      "sum of Ws for layer 1: 321.3245469126552 and sum of bs: 0.001688373973249339\n",
      "sum of abs dWs for layer 2: 10543.259158600806 and sum of dbs: 13.595952553810228\n",
      "sum of Ws for layer 2: 340.86243130373657 and sum of bs: 0.00788243170485399\n",
      "sum of abs dWs for layer 3: 5205.722449068395 and sum of dbs: 4.583418949352486\n",
      "sum of Ws for layer 3: 7.500305346420238 and sum of bs: 0.0023849811446752228\n",
      "MSE on the training set: 0.0028580443561400408\n",
      "Epoch: 830\n",
      "sum of abs dWs for layer 0: 599.326893036574 and sum of dbs: 8.841320689849717\n",
      "sum of Ws for layer 0: 86.4231774959541 and sum of bs: 0.0015990989608258102\n",
      "sum of abs dWs for layer 1: 15784.216886561324 and sum of dbs: 5.793305702282948\n",
      "sum of Ws for layer 1: 321.32459696600864 and sum of bs: 0.0016884227885194908\n",
      "sum of abs dWs for layer 2: 10087.485222514868 and sum of dbs: 12.931250571471224\n",
      "sum of Ws for layer 2: 340.862461327085 and sum of bs: 0.007882558238613356\n",
      "sum of abs dWs for layer 3: 4592.488362192413 and sum of dbs: 4.032673631902727\n",
      "sum of Ws for layer 3: 7.500338676394554 and sum of bs: 0.0023850238925857527\n",
      "MSE on the training set: 0.002224642061651721\n",
      "Epoch: 840\n",
      "sum of abs dWs for layer 0: 522.8292816998741 and sum of dbs: 7.724519154965597\n",
      "sum of Ws for layer 0: 86.42318134822781 and sum of bs: 0.0015991604235841405\n",
      "sum of abs dWs for layer 1: 13797.97391565861 and sum of dbs: 5.097510292570526\n",
      "sum of Ws for layer 1: 321.3246412102245 and sum of bs: 0.0016884658821256185\n",
      "sum of abs dWs for layer 2: 8806.784165072939 and sum of dbs: 11.204831698748016\n",
      "sum of Ws for layer 2: 340.8624878000884 and sum of bs: 0.00788266920939526\n",
      "sum of abs dWs for layer 3: 4051.1558296612293 and sum of dbs: 3.5466512378574455\n",
      "sum of Ws for layer 3: 7.500368079276008 and sum of bs: 0.002385061496196686\n",
      "MSE on the training set: 0.0017319840823536597\n",
      "Epoch: 850\n",
      "sum of abs dWs for layer 0: 472.53281216915104 and sum of dbs: 6.932981145853315\n",
      "sum of Ws for layer 0: 86.42318474148722 and sum of bs: 0.0015992146601976115\n",
      "sum of abs dWs for layer 1: 12591.366511645745 and sum of dbs: 4.639279432449081\n",
      "sum of Ws for layer 1: 321.3246804398475 and sum of bs: 0.001688503990676698\n",
      "sum of abs dWs for layer 2: 7158.528260462963 and sum of dbs: 9.16390083572148\n",
      "sum of Ws for layer 2: 340.86251110043696 and sum of bs: 0.007882766200611605\n",
      "sum of abs dWs for layer 3: 3573.790816686131 and sum of dbs: 3.1180711411770745\n",
      "sum of Ws for layer 3: 7.500394017296321 and sum of bs: 0.0023850945613479624\n",
      "MSE on the training set: 0.0013485233766214893\n",
      "Epoch: 860\n",
      "sum of abs dWs for layer 0: 411.4149973982399 and sum of dbs: 6.008541878921055\n",
      "sum of Ws for layer 0: 86.42318774259776 and sum of bs: 0.0015992621058097817\n",
      "sum of abs dWs for layer 1: 10748.68594206098 and sum of dbs: 3.940158518490206\n",
      "sum of Ws for layer 1: 321.32471482171684 and sum of bs: 0.0016885371544826827\n",
      "sum of abs dWs for layer 2: 6804.048161052309 and sum of dbs: 8.673494751796754\n",
      "sum of Ws for layer 2: 340.8625317255171 and sum of bs: 0.00788285219427801\n",
      "sum of abs dWs for layer 3: 3152.7017136468417 and sum of dbs: 2.7399125719702093\n",
      "sum of Ws for layer 3: 7.5004168987263204 and sum of bs: 0.002385123622878247\n",
      "MSE on the training set: 0.0010499775578892761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 870\n",
      "sum of abs dWs for layer 0: 363.58333025834764 and sum of dbs: 5.294904074540009\n",
      "sum of Ws for layer 0: 86.42319038799076 and sum of bs: 0.0015993038298453561\n",
      "sum of abs dWs for layer 1: 9435.110820128977 and sum of dbs: 3.4594252346085153\n",
      "sum of Ws for layer 1: 321.3247452105185 and sum of bs: 0.001688566382998647\n",
      "sum of abs dWs for layer 2: 5876.821787551182 and sum of dbs: 7.445956082583643\n",
      "sum of Ws for layer 2: 340.86254989457495 and sum of bs: 0.007882927555990965\n",
      "sum of abs dWs for layer 3: 2781.007145461853 and sum of dbs: 2.4062485827620037\n",
      "sum of Ws for layer 3: 7.5004370834534875 and sum of bs: 0.0023851491524261945\n",
      "MSE on the training set: 0.0008177812487023768\n",
      "Epoch: 880\n",
      "sum of abs dWs for layer 0: 323.0467427910596 and sum of dbs: 4.682276038313635\n",
      "sum of Ws for layer 0: 86.42319271855584 and sum of bs: 0.0015993405785180046\n",
      "sum of abs dWs for layer 1: 8729.37306231973 and sum of dbs: 3.168116234589167\n",
      "sum of Ws for layer 1: 321.3247720969232 and sum of bs: 0.0016885922222780454\n",
      "sum of abs dWs for layer 2: 4909.958349049481 and sum of dbs: 6.210201095428567\n",
      "sum of Ws for layer 2: 340.86256589233994 and sum of bs: 0.007882993420645877\n",
      "sum of abs dWs for layer 3: 2453.266941255042 and sum of dbs: 2.1120207212589808\n",
      "sum of Ws for layer 3: 7.500454889302149 and sum of bs: 0.00238517156634612\n",
      "MSE on the training set: 0.0006371050501208101\n",
      "Epoch: 890\n",
      "sum of abs dWs for layer 0: 283.9746234078178 and sum of dbs: 4.080329623025494\n",
      "sum of Ws for layer 0: 86.42319477865796 and sum of bs: 0.0015993726930544619\n",
      "sum of abs dWs for layer 1: 7384.8009921075245 and sum of dbs: 2.6661950645355614\n",
      "sum of Ws for layer 1: 321.3247957040506 and sum of bs: 0.0016886146615172587\n",
      "sum of abs dWs for layer 2: 4625.731476392373 and sum of dbs: 5.825754702159068\n",
      "sum of Ws for layer 2: 340.8625800444943 and sum of bs: 0.007883051651584858\n",
      "sum of abs dWs for layer 3: 2164.174859684492 and sum of dbs: 1.8524211768723187\n",
      "sum of Ws for layer 3: 7.500470596562207 and sum of bs: 0.002385191231766485\n",
      "MSE on the training set: 0.0004964273401525785\n",
      "Epoch: 900\n",
      "sum of abs dWs for layer 0: 249.9018368316398 and sum of dbs: 3.5760282907453105\n",
      "sum of Ws for layer 0: 86.423196595086 and sum of bs: 0.0015994008711381746\n",
      "sum of abs dWs for layer 1: 6515.31302230004 and sum of dbs: 2.3432026040535394\n",
      "sum of Ws for layer 1: 321.3248165454596 and sum of bs: 0.0016886343849416877\n",
      "sum of abs dWs for layer 2: 4047.1387869898426 and sum of dbs: 5.045279239689232\n",
      "sum of Ws for layer 2: 340.86259251946524 and sum of bs: 0.007883102652268766\n",
      "sum of abs dWs for layer 3: 1909.0019171888769 and sum of dbs: 1.6233736547575162\n",
      "sum of Ws for layer 3: 7.500484452393581 and sum of bs: 0.0023852084725520008\n",
      "MSE on the training set: 0.0003870050360131952\n",
      "Epoch: 910\n",
      "sum of abs dWs for layer 0: 221.75550620886514 and sum of dbs: 3.149695160329014\n",
      "sum of Ws for layer 0: 86.42319819466258 and sum of bs: 0.0015994256110634494\n",
      "sum of abs dWs for layer 1: 6010.480106692739 and sum of dbs: 2.141959245809811\n",
      "sum of Ws for layer 1: 321.3248350109689 and sum of bs: 0.0016886518169080755\n",
      "sum of abs dWs for layer 2: 3364.215818928008 and sum of dbs: 4.180502750997572\n",
      "sum of Ws for layer 2: 340.8626035009827 and sum of bs: 0.007883147092903777\n",
      "sum of abs dWs for layer 3: 1683.9872196184583 and sum of dbs: 1.4213868860105177\n",
      "sum of Ws for layer 3: 7.500496675035327 and sum of bs: 0.002385223574544341\n",
      "MSE on the training set: 0.0003018769963993222\n",
      "Epoch: 920\n",
      "sum of abs dWs for layer 0: 194.81914510371897 and sum of dbs: 2.7392201369116758\n",
      "sum of Ws for layer 0: 86.42319960885575 and sum of bs: 0.0015994471832982637\n",
      "sum of abs dWs for layer 1: 5071.722090714518 and sum of dbs: 1.7907958395727726\n",
      "sum of Ws for layer 1: 321.32485121711454 and sum of bs: 0.0016886668906915014\n",
      "sum of abs dWs for layer 2: 3169.875320975905 and sum of dbs: 3.909432504618528\n",
      "sum of Ws for layer 2: 340.86261321475405 and sum of bs: 0.007883186276126986\n",
      "sum of abs dWs for layer 3: 1485.5379436337137 and sum of dbs: 1.243187983431618\n",
      "sum of Ws for layer 3: 7.500507456969447 and sum of bs: 0.0023852367898000416\n",
      "MSE on the training set: 0.00023559298139797384\n",
      "Epoch: 930\n",
      "sum of abs dWs for layer 0: 171.45146695486025 and sum of dbs: 2.392076736147188\n",
      "sum of Ws for layer 0: 86.42320085568319 and sum of bs: 0.0015994660552150913\n",
      "sum of abs dWs for layer 1: 4479.272164515292 and sum of dbs: 1.5699387533244546\n",
      "sum of Ws for layer 1: 321.32486552365174 and sum of bs: 0.0016886801000125075\n",
      "sum of abs dWs for layer 2: 2768.76253487134 and sum of dbs: 3.373457240000761\n",
      "sum of Ws for layer 2: 340.8626217804451 and sum of bs: 0.007883220503255052\n",
      "sum of abs dWs for layer 3: 1310.370358295257 and sum of dbs: 1.085963230766489\n",
      "sum of Ws for layer 3: 7.500516967901898 and sum of bs: 0.0023852483406702945\n",
      "MSE on the training set: 0.00018403050511087268\n",
      "Epoch: 940\n",
      "sum of abs dWs for layer 0: 151.85500985221887 and sum of dbs: 2.096024824420043\n",
      "sum of Ws for layer 0: 86.4232019537934 and sum of bs: 0.0015994825765056663\n",
      "sum of abs dWs for layer 1: 4135.619303924511 and sum of dbs: 1.4315367141657311\n",
      "sum of Ws for layer 1: 321.3248781947001 and sum of bs: 0.0016886917430146717\n",
      "sum of abs dWs for layer 2: 2320.032182350947 and sum of dbs: 2.802612259678017\n",
      "sum of Ws for layer 2: 340.8626293185399 and sum of bs: 0.007883250222567943\n",
      "sum of abs dWs for layer 3: 1155.9036838757886 and sum of dbs: 0.9473132327241124\n",
      "sum of Ws for layer 3: 7.500525357654398 and sum of bs: 0.002385258423380016\n",
      "MSE on the training set: 0.0001439238336389804\n",
      "Epoch: 950\n",
      "sum of abs dWs for layer 0: 133.7121702789852 and sum of dbs: 1.822238105262885\n",
      "sum of Ws for layer 0: 86.42320292473717 and sum of bs: 0.0015994969330026653\n",
      "sum of abs dWs for layer 1: 3504.399207876126 and sum of dbs: 1.201025379282123\n",
      "sum of Ws for layer 1: 321.3248893090922 and sum of bs: 0.0016887017718767734\n",
      "sum of abs dWs for layer 2: 2133.4905274723324 and sum of dbs: 2.5361870607234445\n",
      "sum of Ws for layer 2: 340.86263598889906 and sum of bs: 0.007883276314297749\n",
      "sum of abs dWs for layer 3: 1019.6307430332678 and sum of dbs: 0.8249760036071281\n",
      "sum of Ws for layer 3: 7.500532758377031 and sum of bs: 0.0023852672109688205\n",
      "MSE on the training set: 0.00011269254475994759\n",
      "Epoch: 960\n",
      "sum of abs dWs for layer 0: 117.84360724921218 and sum of dbs: 1.583645154458055\n",
      "sum of Ws for layer 0: 86.42320377982743 and sum of bs: 0.0015995094535948369\n",
      "sum of abs dWs for layer 1: 3155.431608299657 and sum of dbs: 1.067696936475242\n",
      "sum of Ws for layer 1: 321.3248991526982 and sum of bs: 0.0016887105872797805\n",
      "sum of abs dWs for layer 2: 1847.4896469219934 and sum of dbs: 2.1665551653792754\n",
      "sum of Ws for layer 2: 340.86264186013034 and sum of bs: 0.007883298903022671\n",
      "sum of abs dWs for layer 3: 899.4356095092531 and sum of dbs: 0.7170831821395257\n",
      "sum of Ws for layer 3: 7.50053928665203 and sum of bs: 0.002385274856144964\n",
      "MSE on the training set: 8.840329215043953e-05\n",
      "Epoch: 970\n",
      "sum of abs dWs for layer 0: 103.86746902627722 and sum of dbs: 1.372524480876494\n",
      "sum of Ws for layer 0: 86.4232045347472 and sum of bs: 0.0015995203124656735\n",
      "sum of abs dWs for layer 1: 2759.7809245000617 and sum of dbs: 0.919132786683154\n",
      "sum of Ws for layer 1: 321.32490781932836 and sum of bs: 0.0016887182034324806\n",
      "sum of abs dWs for layer 2: 1644.41466653584 and sum of dbs: 1.8980101827803908\n",
      "sum of Ws for layer 2: 340.8626470447074 and sum of bs: 0.007883318581075072\n",
      "sum of abs dWs for layer 3: 793.4026066606308 and sum of dbs: 0.6218987105435632\n",
      "sum of Ws for layer 3: 7.500545045328513 and sum of bs: 0.002385281493576165\n",
      "MSE on the training set: 6.949966515979581e-05\n",
      "Epoch: 980\n",
      "sum of abs dWs for layer 0: 91.7798653942058 and sum of dbs: 1.1891122611935223\n",
      "sum of Ws for layer 0: 86.42320520039303 and sum of bs: 0.0015995297211836004\n",
      "sum of abs dWs for layer 1: 2445.6915292260437 and sum of dbs: 0.7986543062238298\n",
      "sum of Ws for layer 1: 321.324915471487 and sum of bs: 0.0016887248040595216\n",
      "sum of abs dWs for layer 2: 1442.0020843937048 and sum of dbs: 1.6362327939022268\n",
      "sum of Ws for layer 2: 340.86265161593985 and sum of bs: 0.007883335639507235\n",
      "sum of abs dWs for layer 3: 699.8720151291388 and sum of dbs: 0.5379407152585028\n",
      "sum of Ws for layer 3: 7.500550125136051 and sum of bs: 0.0023852872420714504\n",
      "MSE on the training set: 5.4792077094512936e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 990\n",
      "sum of abs dWs for layer 0: 81.11017009583047 and sum of dbs: 1.0270334191319914\n",
      "sum of Ws for layer 0: 86.42320578778198 and sum of bs: 0.00159953784061615\n",
      "sum of abs dWs for layer 1: 2145.7528722518423 and sum of dbs: 0.6856864618764729\n",
      "sum of Ws for layer 1: 321.3249222179735 and sum of bs: 0.0016887304888322302\n",
      "sum of abs dWs for layer 2: 1273.458250900885 and sum of dbs: 1.4164157938527295\n",
      "sum of Ws for layer 2: 340.86265564868506 and sum of bs: 0.007883350413771893\n",
      "sum of abs dWs for layer 3: 617.3654229876962 and sum of dbs: 0.4638786178746797\n",
      "sum of Ws for layer 3: 7.500554606097756 and sum of bs: 0.0023852922064355395\n",
      "MSE on the training set: 4.3347186524195425e-05\n",
      "Epoch: 1000\n",
      "sum of abs dWs for layer 0: 71.23476066808713 and sum of dbs: 0.8792580624745183\n",
      "sum of Ws for layer 0: 86.42320630583465 and sum of bs: 0.0015995448284967643\n",
      "sum of abs dWs for layer 1: 1889.3189329993631 and sum of dbs: 0.587909461048179\n",
      "sum of Ws for layer 1: 321.32492816904767 and sum of bs: 0.0016887353811356326\n",
      "sum of abs dWs for layer 2: 1134.073543298414 and sum of dbs: 1.2310032896102943\n",
      "sum of Ws for layer 2: 340.86265920669314 and sum of bs: 0.007883363161378935\n",
      "sum of abs dWs for layer 3: 544.5844089466842 and sum of dbs: 0.39854717214111673\n",
      "sum of Ws for layer 3: 7.500558558802613 and sum of bs: 0.002385296479108651\n",
      "MSE on the training set: 3.444182675376399e-05\n",
      "Epoch: 1010\n",
      "sum of abs dWs for layer 0: 62.87777561170712 and sum of dbs: 0.7526325570154986\n",
      "sum of Ws for layer 0: 86.42320676278362 and sum of bs: 0.00159955081560171\n",
      "sum of abs dWs for layer 1: 1659.4446163509558 and sum of dbs: 0.5007297979651297\n",
      "sum of Ws for layer 1: 321.3249334199299 and sum of bs: 0.0016887395659941579\n",
      "sum of abs dWs for layer 2: 1004.0720234304581 and sum of dbs: 1.0612142240342008\n",
      "sum of Ws for layer 2: 340.8626623445841 and sum of bs: 0.007883374119833378\n",
      "sum of abs dWs for layer 3: 480.3826050822546 and sum of dbs: 0.34091626459643976\n",
      "sum of Ws for layer 3: 7.500562045527401 and sum of bs: 0.0023853001416382622\n",
      "MSE on the training set: 2.7512463445827562e-05\n",
      "Epoch: 1020\n",
      "sum of abs dWs for layer 0: 55.59582596429872 and sum of dbs: 0.6418922398190346\n",
      "sum of Ws for layer 0: 86.42320716587034 and sum of bs: 0.001599555922105728\n",
      "sum of abs dWs for layer 1: 1458.742433011018 and sum of dbs: 0.4247441123587146\n",
      "sum of Ws for layer 1: 321.32493805249925 and sum of bs: 0.0016887431254695713\n",
      "sum of abs dWs for layer 2: 884.7522487879129 and sum of dbs: 0.9064558099256042\n",
      "sum of Ws for layer 2: 340.86266511198914 and sum of bs: 0.007883383506969474\n",
      "sum of abs dWs for layer 3: 423.75061218321264 and sum of dbs: 0.29008242976531684\n",
      "sum of Ws for layer 3: 7.500565121203338 and sum of bs: 0.0023853032659507194\n",
      "MSE on the training set: 2.2120611276372515e-05\n",
      "Epoch: 1030\n",
      "sum of abs dWs for layer 0: 48.89817087492142 and sum of dbs: 0.5413710083692812\n",
      "sum of Ws for layer 0: 86.42320752146509 and sum of bs: 0.001599560247980819\n",
      "sum of abs dWs for layer 1: 1288.8689230854436 and sum of dbs: 0.3595553525213983\n",
      "sum of Ws for layer 1: 321.3249421376349 and sum of bs: 0.001688746135351128\n",
      "sum of abs dWs for layer 2: 784.0264578301444 and sum of dbs: 0.7738326623350958\n",
      "sum of Ws for layer 2: 340.86266755273516 and sum of bs: 0.0078833915162428\n",
      "sum of abs dWs for layer 3: 373.795206662198 and sum of dbs: 0.24524287728773553\n",
      "sum of Ws for layer 3: 7.500567834294453 and sum of bs: 0.0023853059155229334\n",
      "MSE on the training set: 1.7925090870759822e-05\n",
      "Epoch: 1040\n",
      "sum of abs dWs for layer 0: 43.247597713858625 and sum of dbs: 0.45537193407303367\n",
      "sum of Ws for layer 0: 86.42320783514064 and sum of bs: 0.0015995638882380292\n",
      "sum of abs dWs for layer 1: 1135.5779985299814 and sum of dbs: 0.3014096725074942\n",
      "sum of Ws for layer 1: 321.3249457423208 and sum of bs: 0.0016887486563550492\n",
      "sum of abs dWs for layer 2: 689.4615667984995 and sum of dbs: 0.6514680801665473\n",
      "sum of Ws for layer 2: 340.86266970482643 and sum of bs: 0.007883398302172162\n",
      "sum of abs dWs for layer 3: 329.7294064736462 and sum of dbs: 0.20569055092666422\n",
      "sum of Ws for layer 3: 7.500570227543156 and sum of bs: 0.0023853081463192584\n",
      "MSE on the training set: 1.466044125644293e-05\n",
      "Epoch: 1050\n",
      "sum of abs dWs for layer 0: 37.977474015752954 and sum of dbs: 0.3765379850768258\n",
      "sum of Ws for layer 0: 86.42320811188443 and sum of bs: 0.001599566919482501\n",
      "sum of abs dWs for layer 1: 1006.3730511809517 and sum of dbs: 0.25146107482377683\n",
      "sum of Ws for layer 1: 321.32494892056354 and sum of bs: 0.001688750750888255\n",
      "sum of abs dWs for layer 2: 609.8908857501999 and sum of dbs: 0.5472084342648939\n",
      "sum of Ws for layer 2: 340.8626716030292 and sum of bs: 0.00788340401527861\n",
      "sum of abs dWs for layer 3: 290.8583641790552 and sum of dbs: 0.1708012564365451\n",
      "sum of Ws for layer 3: 7.500572338659786 and sum of bs: 0.002385310007728144\n",
      "MSE on the training set: 1.212012306104372e-05\n",
      "Epoch: 1060\n",
      "sum of abs dWs for layer 0: 33.661976045808935 and sum of dbs: 0.3104990791291923\n",
      "sum of Ws for layer 0: 86.42320835615183 and sum of bs: 0.0015995694136729291\n",
      "sum of abs dWs for layer 1: 883.1437538283997 and sum of dbs: 0.20503811723843113\n",
      "sum of Ws for layer 1: 321.3249517235022 and sum of bs: 0.0016887524613733956\n",
      "sum of abs dWs for layer 2: 537.2538144350692 and sum of dbs: 0.45308161634310407\n",
      "sum of Ws for layer 2: 340.8626732783988 and sum of bs: 0.00788340877006414\n",
      "sum of abs dWs for layer 3: 256.5710514706139 and sum of dbs: 0.140027136944971\n",
      "sum of Ws for layer 3: 7.500574200904792 and sum of bs: 0.002385311543296557\n",
      "MSE on the training set: 1.0143438738724712e-05\n",
      "Epoch: 1070\n",
      "sum of abs dWs for layer 0: 29.684936794662782 and sum of dbs: 0.2506674669654947\n",
      "sum of Ws for layer 0: 86.42320857151913 and sum of bs: 0.001599571436925924\n",
      "sum of abs dWs for layer 1: 781.3642874950169 and sum of dbs: 0.16592189228709409\n",
      "sum of Ws for layer 1: 321.3249541974319 and sum of bs: 0.0016887538400554916\n",
      "sum of abs dWs for layer 2: 472.0000827524674 and sum of dbs: 0.36878206220784726\n",
      "sum of Ws for layer 2: 340.8626747567953 and sum of bs: 0.007883412678984715\n",
      "sum of abs dWs for layer 3: 226.32537374439778 and sum of dbs: 0.11287999643497892\n",
      "sum of Ws for layer 3: 7.500575843620096 and sum of bs: 0.002385312791445083\n",
      "MSE on the training set: 8.605335578338433e-06\n",
      "Epoch: 1080\n",
      "sum of abs dWs for layer 0: 26.173612579892648 and sum of dbs: 0.19797649505513779\n",
      "sum of Ws for layer 0: 86.42320876151132 and sum of bs: 0.0015995730441078293\n",
      "sum of abs dWs for layer 1: 687.9944135065301 and sum of dbs: 0.13039618224929062\n",
      "sum of Ws for layer 1: 321.3249563793779 and sum of bs: 0.0016887549235222142\n",
      "sum of abs dWs for layer 2: 417.4173742954725 and sum of dbs: 0.2973451760237275\n",
      "sum of Ws for layer 2: 340.8626760608438 and sum of bs: 0.007883415850100205\n",
      "sum of abs dWs for layer 3: 199.645354433458 and sum of dbs: 0.08893333041325394\n",
      "sum of Ws for layer 3: 7.500577292688227 and sum of bs: 0.00238531378606247\n",
      "MSE on the training set: 7.408486925079125e-06\n",
      "Epoch: 1090\n",
      "sum of abs dWs for layer 0: 23.081013200736862 and sum of dbs: 0.15172588456658184\n",
      "sum of Ws for layer 0: 86.42320892910787 and sum of bs: 0.0015995742849499622\n",
      "sum of abs dWs for layer 1: 606.5449635207531 and sum of dbs: 0.10007198142787138\n",
      "sum of Ws for layer 1: 321.3249583042083 and sum of bs: 0.0016887557464312837\n",
      "sum of abs dWs for layer 2: 368.71799950859855 and sum of dbs: 0.23373267950522014\n",
      "sum of Ws for layer 2: 340.8626772110757 and sum of bs: 0.00788341836883176\n",
      "sum of abs dWs for layer 3: 176.11117352445385 and sum of dbs: 0.0678105999313593\n",
      "sum of Ws for layer 3: 7.5005785709392505 and sum of bs: 0.0023853145570398404\n",
      "MSE on the training set: 6.477172519315344e-06\n",
      "Epoch: 1100\n",
      "sum of abs dWs for layer 0: 20.35013874707627 and sum of dbs: 0.11130208315896963\n",
      "sum of Ws for layer 0: 86.42320907695212 and sum of bs: 0.0015995752021451144\n",
      "sum of abs dWs for layer 1: 534.8993819781164 and sum of dbs: 0.07454973756554015\n",
      "sum of Ws for layer 1: 321.3249600018679 and sum of bs: 0.001688756339279202\n",
      "sum of abs dWs for layer 2: 325.79246712038884 and sum of dbs: 0.17767265750070657\n",
      "sum of Ws for layer 2: 340.86267822549576 and sum of bs: 0.007883420315949288\n",
      "sum of abs dWs for layer 3: 155.3526493551031 and sum of dbs: 0.049179841159491845\n",
      "sum of Ws for layer 3: 7.500579698513808 and sum of bs: 0.0023853151307490213\n",
      "MSE on the training set: 5.752466709356424e-06\n",
      "Epoch: 1110\n",
      "sum of abs dWs for layer 0: 17.951730229875576 and sum of dbs: 0.07654562842641391\n",
      "sum of Ws for layer 0: 86.42320920740971 and sum of bs: 0.001599575833251842\n",
      "sum of abs dWs for layer 1: 471.53720882463506 and sum of dbs: 0.05221917337221825\n",
      "sum of Ws for layer 1: 321.3249614986592 and sum of bs: 0.001688756730381341\n",
      "sum of abs dWs for layer 2: 288.493710667817 and sum of dbs: 0.1288373718210655\n",
      "sum of Ws for layer 2: 340.8626791197114 and sum of bs: 0.00788342176044086\n",
      "sum of abs dWs for layer 3: 137.04161990866655 and sum of dbs: 0.03274589778531247\n",
      "sum of Ws for layer 3: 7.500580693180196 and sum of bs: 0.0023853155304576707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the training set: 5.188521100698012e-06\n",
      "Epoch: 1120\n",
      "sum of abs dWs for layer 0: 15.830335828154555 and sum of dbs: 0.048467720623793197\n",
      "sum of Ws for layer 0: 86.42320932252225 and sum of bs: 0.001599576211902222\n",
      "sum of abs dWs for layer 1: 416.6020985216145 and sum of dbs: 0.03594705142083443\n",
      "sum of Ws for layer 1: 321.32496281842054 and sum of bs: 0.0016887569439853218\n",
      "sum of abs dWs for layer 2: 254.58420323971768 and sum of dbs: 0.08470512605598875\n",
      "sum of Ws for layer 2: 340.86267990784586 and sum of bs: 0.007883422761363645\n",
      "sum of abs dWs for layer 3: 120.89032172472467 and sum of dbs: 0.018250986798174162\n",
      "sum of Ws for layer 3: 7.50058157061439 and sum of bs: 0.0023853157766954985\n",
      "MSE on the training set: 4.749656605632992e-06\n",
      "Epoch: 1130\n",
      "sum of abs dWs for layer 0: 13.930654311992882 and sum of dbs: 0.030509795728541435\n",
      "sum of Ws for layer 0: 86.4232094240998 and sum of bs: 0.0015995763686464403\n",
      "sum of abs dWs for layer 1: 368.85030261070034 and sum of dbs: 0.026485361128296164\n",
      "sum of Ws for layer 1: 321.3249639819327 and sum of bs: 0.0016887569997645051\n",
      "sum of abs dWs for layer 2: 224.3300426338414 and sum of dbs: 0.04553545820124212\n",
      "sum of Ws for layer 2: 340.86268060298846 and sum of bs: 0.007883423371263007\n",
      "sum of abs dWs for layer 3: 106.6443040009703 and sum of dbs: 0.005466717185646154\n",
      "sum of Ws for layer 3: 7.500582344643597 and sum of bs: 0.002385315887570328\n",
      "MSE on the training set: 4.408108303020833e-06\n",
      "Epoch: 1140\n",
      "sum of abs dWs for layer 0: 12.372016392310332 and sum of dbs: 0.030122335534430816\n",
      "sum of Ws for layer 0: 86.42320951361917 and sum of bs: 0.0015995763312650806\n",
      "sum of abs dWs for layer 1: 324.15991926180845 and sum of dbs: 0.029509918984360315\n",
      "sum of Ws for layer 1: 321.3249650095727 and sum of bs: 0.0016887569159375602\n",
      "sum of abs dWs for layer 2: 195.92156194103939 and sum of dbs: 0.03711743202486994\n",
      "sum of Ws for layer 2: 340.8626812167414 and sum of bs: 0.007883423628410333\n",
      "sum of abs dWs for layer 3: 94.07818919888065 and sum of dbs: 0.005809922985701498\n",
      "sum of Ws for layer 3: 7.500583027461896 and sum of bs: 0.0023853158790482615\n",
      "MSE on the training set: 4.1423106550955245e-06\n",
      "Epoch: 1150\n",
      "sum of abs dWs for layer 0: 10.858984696688859 and sum of dbs: 0.04293756859284127\n",
      "sum of Ws for layer 0: 86.42320959267309 and sum of bs: 0.0015995761177417427\n",
      "sum of abs dWs for layer 1: 286.3269917471423 and sum of dbs: 0.03870004510555779\n",
      "sum of Ws for layer 1: 321.3249659158914 and sum of bs: 0.001688756707595896\n",
      "sum of abs dWs for layer 2: 174.68043136089187 and sum of dbs: 0.06388642826588685\n",
      "sum of Ws for layer 2: 340.86268175864245 and sum of bs: 0.007883423570295298\n",
      "sum of abs dWs for layer 3: 82.99215091463105 and sum of dbs: 0.01575907681393591\n",
      "sum of Ws for layer 3: 7.500583629821229 and sum of bs: 0.002385315765201124\n",
      "MSE on the training set: 3.9354781171317915e-06\n",
      "Epoch: 1160\n",
      "sum of abs dWs for layer 0: 9.571112106353624 and sum of dbs: 0.05781788472952862\n",
      "sum of Ws for layer 0: 86.42320966241039 and sum of bs: 0.0015995757521429317\n",
      "sum of abs dWs for layer 1: 252.53828666542054 and sum of dbs: 0.048242780631494175\n",
      "sum of Ws for layer 1: 321.3249667153141 and sum of bs: 0.0016887563913308948\n",
      "sum of abs dWs for layer 2: 154.525548070012 and sum of dbs: 0.08617191368557651\n",
      "sum of Ws for layer 2: 340.86268223647414 and sum of bs: 0.007883423243417951\n",
      "sum of abs dWs for layer 3: 73.2142885051763 and sum of dbs: 0.024533851180290774\n",
      "sum of Ws for layer 3: 7.500584161205782 and sum of bs: 0.0023853155584422823\n",
      "MSE on the training set: 3.774508509095921e-06\n",
      "Epoch: 1170\n",
      "sum of abs dWs for layer 0: 8.441410317386193 and sum of dbs: 0.07346018682624426\n",
      "sum of Ws for layer 0: 86.42320972395551 and sum of bs: 0.0015995752520483173\n",
      "sum of abs dWs for layer 1: 223.244537435341 and sum of dbs: 0.05714811351034816\n",
      "sum of Ws for layer 1: 321.3249674202657 and sum of bs: 0.0016887559796102676\n",
      "sum of abs dWs for layer 2: 136.27400425129406 and sum of dbs: 0.10536802738758208\n",
      "sum of Ws for layer 2: 340.8626826574359 and sum of bs: 0.007883422680104437\n",
      "sum of abs dWs for layer 3: 64.5897066514875 and sum of dbs: 0.032273470716132305\n",
      "sum of Ws for layer 3: 7.5005846299861325 and sum of bs: 0.002385315269731361\n",
      "MSE on the training set: 3.6492244258934844e-06\n",
      "Epoch: 1180\n",
      "sum of abs dWs for layer 0: 7.483638915016496 and sum of dbs: 0.08783631780557272\n",
      "sum of Ws for layer 0: 86.42320977827524 and sum of bs: 0.00159957463335368\n",
      "sum of abs dWs for layer 1: 195.31176088652052 and sum of dbs: 0.06609330592275837\n",
      "sum of Ws for layer 1: 321.32496804198064 and sum of bs: 0.0016887554829968638\n",
      "sum of abs dWs for layer 2: 120.48985522592162 and sum of dbs: 0.12270793862099952\n",
      "sum of Ws for layer 2: 340.8626830282525 and sum of bs: 0.00788342190803417\n",
      "sum of abs dWs for layer 3: 56.980831504144795 and sum of dbs: 0.039101733535487855\n",
      "sum of Ws for layer 3: 7.5005850435424 and sum of bs: 0.0023853149087303196\n",
      "MSE on the training set: 3.5517077873798043e-06\n",
      "Epoch: 1190\n",
      "sum of abs dWs for layer 0: 6.554312001091163 and sum of dbs: 0.10094040540047913\n",
      "sum of Ws for layer 0: 86.42320982621135 and sum of bs: 0.001599573909814301\n",
      "sum of abs dWs for layer 1: 172.5700550846616 and sum of dbs: 0.0745991648380453\n",
      "sum of Ws for layer 1: 321.32496859002026 and sum of bs: 0.0016887549131916683\n",
      "sum of abs dWs for layer 2: 107.84381167417243 and sum of dbs: 0.1391114582602108\n",
      "sum of Ws for layer 2: 340.86268335526034 and sum of bs: 0.007883420953169618\n",
      "sum of abs dWs for layer 3: 50.27010525448921 and sum of dbs: 0.04512357066539478\n",
      "sum of Ws for layer 3: 7.500585408392297 and sum of bs: 0.0023853144839747495\n",
      "MSE on the training set: 3.4757923510455815e-06\n",
      "Epoch: 1200\n",
      "sum of abs dWs for layer 0: 5.778063343293313 and sum of dbs: 0.11219005916411345\n",
      "sum of Ws for layer 0: 86.42320986842533 and sum of bs: 0.001599573096090929\n",
      "sum of abs dWs for layer 1: 153.11130411248095 and sum of dbs: 0.08154723613724912\n",
      "sum of Ws for layer 1: 321.32496907459233 and sum of bs: 0.0016887542775816202\n",
      "sum of abs dWs for layer 2: 94.40363346063788 and sum of dbs: 0.15147122094416607\n",
      "sum of Ws for layer 2: 340.86268364414536 and sum of bs: 0.007883419831308608\n",
      "sum of abs dWs for layer 3: 44.35124462260387 and sum of dbs: 0.050434677757607654\n",
      "sum of Ws for layer 3: 7.500585730279179 and sum of bs: 0.0023853140029834676\n",
      "MSE on the training set: 3.4166967403420394e-06\n",
      "Epoch: 1210\n",
      "sum of abs dWs for layer 0: 5.1363715487671975 and sum of dbs: 0.12160723935151713\n",
      "sum of Ws for layer 0: 86.42320990568193 and sum of bs: 0.001599572199779384\n",
      "sum of abs dWs for layer 1: 134.46733058126048 and sum of dbs: 0.08785099851767406\n",
      "sum of Ws for layer 1: 321.3249695019705 and sum of bs: 0.0016887535836070744\n",
      "sum of abs dWs for layer 2: 82.16960286566322 and sum of dbs: 0.16204508997068046\n",
      "sum of Ws for layer 2: 340.8626838991196 and sum of bs: 0.007883418562926629\n",
      "sum of abs dWs for layer 3: 39.1303644620893 and sum of dbs: 0.05511954368190703\n",
      "sum of Ws for layer 3: 7.500586014266908 and sum of bs: 0.00238531347238436\n",
      "MSE on the training set: 3.3706998386103696e-06\n",
      "Epoch: 1220\n",
      "sum of abs dWs for layer 0: 4.500605678758718 and sum of dbs: 0.13075456319968642\n",
      "sum of Ws for layer 0: 86.42320993859067 and sum of bs: 0.001599571229615548\n",
      "sum of abs dWs for layer 1: 119.47322424690873 and sum of dbs: 0.09371396776824487\n",
      "sum of Ws for layer 1: 321.3249698786541 and sum of bs: 0.0016887528388767202\n",
      "sum of abs dWs for layer 2: 73.6756762586183 and sum of dbs: 0.17348797612817168\n",
      "sum of Ws for layer 2: 340.8626841241418 and sum of bs: 0.00788341716411933\n",
      "sum of abs dWs for layer 3: 34.52271285802049 and sum of dbs: 0.05925459788703484\n",
      "sum of Ws for layer 3: 7.500586264817097 and sum of bs: 0.002385312898014668\n",
      "MSE on the training set: 3.33490521873669e-06\n",
      "Epoch: 1230\n",
      "sum of abs dWs for layer 0: 3.9619355904445026 and sum of dbs: 0.13856801194336824\n",
      "sum of Ws for layer 0: 86.42320996764839 and sum of bs: 0.00159957019619332\n",
      "sum of abs dWs for layer 1: 104.9116222297603 and sum of dbs: 0.09873541463817988\n",
      "sum of Ws for layer 1: 321.3249702106882 and sum of bs: 0.0016887520486035712\n",
      "sum of abs dWs for layer 2: 65.46018292141673 and sum of dbs: 0.18292680287658838\n",
      "sum of Ws for layer 2: 340.86268432232146 and sum of bs: 0.007883415654632653\n",
      "sum of abs dWs for layer 3: 30.458781661040017 and sum of dbs: 0.06290155523351243\n",
      "sum of Ws for layer 3: 7.5005864858743765 and sum of bs: 0.002385312285038374\n",
      "MSE on the training set: 3.307035336358521e-06\n",
      "Epoch: 1240\n",
      "sum of abs dWs for layer 0: 3.5291142230896724 and sum of dbs: 0.1451169161722773\n",
      "sum of Ws for layer 0: 86.42320999330937 and sum of bs: 0.0015995691069234806\n",
      "sum of abs dWs for layer 1: 92.58973228706819 and sum of dbs: 0.10305129132832784\n",
      "sum of Ws for layer 1: 321.32497050351594 and sum of bs: 0.001688751218277956\n",
      "sum of abs dWs for layer 2: 57.06835491994093 and sum of dbs: 0.19020313980146836\n",
      "sum of Ws for layer 2: 340.8626844968415 and sum of bs: 0.007883414047529208\n",
      "sum of abs dWs for layer 3: 26.87527817233666 and sum of dbs: 0.06611719463416814\n",
      "sum of Ws for layer 3: 7.500586680911869 and sum of bs: 0.0023853116380016205\n",
      "MSE on the training set: 3.2853310770617064e-06\n",
      "Epoch: 1250\n",
      "sum of abs dWs for layer 0: 3.105242508630626 and sum of dbs: 0.15135439162789274\n",
      "sum of Ws for layer 0: 86.42321001595269 and sum of bs: 0.001599567967956732\n",
      "sum of abs dWs for layer 1: 80.6557966675043 and sum of dbs: 0.10703944468872428\n",
      "sum of Ws for layer 1: 321.3249707616095 and sum of bs: 0.0016887503538359746\n",
      "sum of abs dWs for layer 2: 51.03265770914925 and sum of dbs: 0.1978907583473805\n",
      "sum of Ws for layer 2: 340.8626846503689 and sum of bs: 0.007883412354224979\n",
      "sum of abs dWs for layer 3: 23.7115648478463 and sum of dbs: 0.06895628594255471\n",
      "sum of Ws for layer 3: 7.5005868529976585 and sum of bs: 0.002385310960921827\n",
      "MSE on the training set: 3.2684291117070478e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1260\n",
      "sum of abs dWs for layer 0: 2.759967609668962 and sum of dbs: 0.15655076041289662\n",
      "sum of Ws for layer 0: 86.42321003590622 and sum of bs: 0.001599566786851981\n",
      "sum of abs dWs for layer 1: 71.49779749796578 and sum of dbs: 0.11041915148490224\n",
      "sum of Ws for layer 1: 321.32497098982157 and sum of bs: 0.001688749457540896\n",
      "sum of abs dWs for layer 2: 44.895330774931146 and sum of dbs: 0.20391444619397786\n",
      "sum of Ws for layer 2: 340.8626847861739 and sum of bs: 0.007883410582695429\n",
      "sum of abs dWs for layer 3: 20.92316933972577 and sum of dbs: 0.07145835104475963\n",
      "sum of Ws for layer 3: 7.500587004834808 and sum of bs: 0.002385310257337202\n",
      "MSE on the training set: 3.255264959820873e-06\n",
      "Epoch: 1270\n",
      "sum of abs dWs for layer 0: 2.4153544263730096 and sum of dbs: 0.16165451270526005\n",
      "sum of Ws for layer 0: 86.42321005344392 and sum of bs: 0.0015995655680113947\n",
      "sum of abs dWs for layer 1: 63.21158781852264 and sum of dbs: 0.11328204290051581\n",
      "sum of Ws for layer 1: 321.32497119172035 and sum of bs: 0.0016887485355299603\n",
      "sum of abs dWs for layer 2: 39.34412697709736 and sum of dbs: 0.20910807561356518\n",
      "sum of Ws for layer 2: 340.8626849061958 and sum of bs: 0.007883408741755695\n",
      "sum of abs dWs for layer 3: 18.462111655583666 and sum of dbs: 0.07366679885884904\n",
      "sum of Ws for layer 3: 7.500587138819005 and sum of bs: 0.0023853095303847487\n",
      "MSE on the training set: 3.245013180315298e-06\n",
      "Epoch: 1280\n",
      "sum of abs dWs for layer 0: 2.1421717481861537 and sum of dbs: 0.16583969806034154\n",
      "sum of Ws for layer 0: 86.42321006893488 and sum of bs: 0.0015995643152017128\n",
      "sum of abs dWs for layer 1: 56.223138422653925 and sum of dbs: 0.11600541303701312\n",
      "sum of Ws for layer 1: 321.32497136971654 and sum of bs: 0.001688747588447499\n",
      "sum of abs dWs for layer 2: 34.40109358787338 and sum of dbs: 0.21364751741424912\n",
      "sum of Ws for layer 2: 340.8626850119814 and sum of bs: 0.0078834068411107\n",
      "sum of abs dWs for layer 3: 16.291736793463564 and sum of dbs: 0.075614332986579\n",
      "sum of Ws for layer 3: 7.500587257043833 and sum of bs: 0.0023853087828036345\n",
      "MSE on the training set: 3.23702719400369e-06\n",
      "Epoch: 1290\n",
      "sum of abs dWs for layer 0: 1.852676273098921 and sum of dbs: 0.17002868664853127\n",
      "sum of Ws for layer 0: 86.42321008265145 and sum of bs: 0.0015995630298743517\n",
      "sum of abs dWs for layer 1: 50.270372836914184 and sum of dbs: 0.11850915086766223\n",
      "sum of Ws for layer 1: 321.3249715263593 and sum of bs: 0.0016887466203174236\n",
      "sum of abs dWs for layer 2: 31.049347306702227 and sum of dbs: 0.21857210465612517\n",
      "sum of Ws for layer 2: 340.8626851056374 and sum of bs: 0.00788340488279596\n",
      "sum of abs dWs for layer 3: 14.375080416611926 and sum of dbs: 0.07733445438970914\n",
      "sum of Ws for layer 3: 7.5005873613681455 and sum of bs: 0.0023853080170267936\n",
      "MSE on the training set: 3.230817842503127e-06\n",
      "Epoch: 1300\n",
      "sum of abs dWs for layer 0: 1.6624602719357549 and sum of dbs: 0.17306303593063335\n",
      "sum of Ws for layer 0: 86.42321009475543 and sum of bs: 0.0015995617185240387\n",
      "sum of abs dWs for layer 1: 43.22773090008504 and sum of dbs: 0.12068650730658885\n",
      "sum of Ws for layer 1: 321.32497166467437 and sum of bs: 0.001688745633006197\n",
      "sum of abs dWs for layer 2: 27.21727074260675 and sum of dbs: 0.22214044975430455\n",
      "sum of Ws for layer 2: 340.8626851880862 and sum of bs: 0.007883402877766916\n",
      "sum of abs dWs for layer 3: 12.683691576021115 and sum of dbs: 0.07885244508945725\n",
      "sum of Ws for layer 3: 7.500587453414044 and sum of bs: 0.0023853072351754452\n",
      "MSE on the training set: 3.225982319811409e-06\n",
      "Epoch: 1310\n",
      "sum of abs dWs for layer 0: 1.471536366289529 and sum of dbs: 0.17597105761141096\n",
      "sum of Ws for layer 0: 86.42321010546459 and sum of bs: 0.0015995603839264261\n",
      "sum of abs dWs for layer 1: 38.44513054681383 and sum of dbs: 0.12256830002383978\n",
      "sum of Ws for layer 1: 321.3249717863607 and sum of bs: 0.0016887446284996823\n",
      "sum of abs dWs for layer 2: 24.03485894753334 and sum of dbs: 0.22544960755956064\n",
      "sum of Ws for layer 2: 340.8626852607899 and sum of bs: 0.007883400833241955\n",
      "sum of abs dWs for layer 3: 11.193949315790048 and sum of dbs: 0.08018932029676039\n",
      "sum of Ws for layer 3: 7.500587534640246 and sum of bs: 0.0023853064391603294\n",
      "MSE on the training set: 3.2222123966006183e-06\n",
      "Epoch: 1320\n",
      "sum of abs dWs for layer 0: 1.2918537639171672 and sum of dbs: 0.17865474538797033\n",
      "sum of Ws for layer 0: 86.42321011489996 and sum of bs: 0.001599559028561174\n",
      "sum of abs dWs for layer 1: 33.64853172426131 and sum of dbs: 0.1243724202635331\n",
      "sum of Ws for layer 1: 321.3249718938327 and sum of bs: 0.0016887436106285368\n",
      "sum of abs dWs for layer 2: 21.18046178530179 and sum of dbs: 0.2283355907461922\n",
      "sum of Ws for layer 2: 340.86268532460144 and sum of bs: 0.007883398752319483\n",
      "sum of abs dWs for layer 3: 9.877429344283083 and sum of dbs: 0.08137082794888832\n",
      "sum of Ws for layer 3: 7.5005876063201615 and sum of bs: 0.002385305630648686\n",
      "MSE on the training set: 3.219276076234777e-06\n",
      "Epoch: 1330\n",
      "sum of abs dWs for layer 0: 1.1384626638096378 and sum of dbs: 0.18097379800639316\n",
      "sum of Ws for layer 0: 86.42321012323488 and sum of bs: 0.0015995576551047566\n",
      "sum of abs dWs for layer 1: 29.54715485079859 and sum of dbs: 0.12592438056299976\n",
      "sum of Ws for layer 1: 321.32497198863643 and sum of bs: 0.0016887425795167458\n",
      "sum of abs dWs for layer 2: 19.000112651821503 and sum of dbs: 0.23119316353754304\n",
      "sum of Ws for layer 2: 340.8626853807287 and sum of bs: 0.007883396639802852\n",
      "sum of abs dWs for layer 3: 8.715716171400715 and sum of dbs: 0.08241339707068104\n",
      "sum of Ws for layer 3: 7.500587669566062 and sum of bs: 0.0023853048110959274\n",
      "MSE on the training set: 3.216987232995352e-06\n",
      "Epoch: 1340\n",
      "sum of abs dWs for layer 0: 1.0058237975875488 and sum of dbs: 0.18298672702275476\n",
      "sum of Ws for layer 0: 86.42321013060356 and sum of bs: 0.0015995562656759095\n",
      "sum of abs dWs for layer 1: 26.526878106387752 and sum of dbs: 0.12722210245935411\n",
      "sum of Ws for layer 1: 321.32497207215556 and sum of bs: 0.0016887415369554013\n",
      "sum of abs dWs for layer 2: 16.700544673471633 and sum of dbs: 0.23336697369092668\n",
      "sum of Ws for layer 2: 340.86268543009953 and sum of bs: 0.007883394499792161\n",
      "sum of abs dWs for layer 3: 7.692033941439565 and sum of dbs: 0.08333202827826994\n",
      "sum of Ws for layer 3: 7.500587725379297 and sum of bs: 0.0023853039818135416\n",
      "MSE on the training set: 3.2152015130010597e-06\n",
      "Epoch: 1350\n",
      "sum of abs dWs for layer 0: 0.8930292041359232 and sum of dbs: 0.18475318108892375\n",
      "sum of Ws for layer 0: 86.42321013711394 and sum of bs: 0.0015995548624253455\n",
      "sum of abs dWs for layer 1: 23.250988189342003 and sum of dbs: 0.12837213218248816\n",
      "sum of Ws for layer 1: 321.3249721457533 and sum of bs: 0.001688740483885559\n",
      "sum of abs dWs for layer 2: 15.010446092410174 and sum of dbs: 0.23558874904299057\n",
      "sum of Ws for layer 2: 340.86268547373794 and sum of bs: 0.0078833923353231\n",
      "sum of abs dWs for layer 3: 6.789848319725169 and sum of dbs: 0.08414158259301899\n",
      "sum of Ws for layer 3: 7.500587774641248 and sum of bs: 0.0023853031439558196\n",
      "MSE on the training set: 3.2138072811539407e-06\n",
      "Epoch: 1360\n",
      "sum of abs dWs for layer 0: 0.79735298983926 and sum of dbs: 0.1862124516979355\n",
      "sum of Ws for layer 0: 86.42321014282372 and sum of bs: 0.001599553447507659\n",
      "sum of abs dWs for layer 1: 20.972680468663967 and sum of dbs: 0.12959553967447018\n",
      "sum of Ws for layer 1: 321.3249722110096 and sum of bs: 0.0016887394220008946\n",
      "sum of abs dWs for layer 2: 13.500399839808505 and sum of dbs: 0.2375336837855328\n",
      "sum of Ws for layer 2: 340.8626855126671 and sum of bs: 0.007883390147808035\n",
      "sum of abs dWs for layer 3: 5.994269412944935 and sum of dbs: 0.08485546786918252\n",
      "sum of Ws for layer 3: 7.500587818132269 and sum of bs: 0.0023853022985445707\n",
      "MSE on the training set: 3.2127198131241373e-06\n",
      "Epoch: 1370\n",
      "sum of abs dWs for layer 0: 0.6975839497829315 and sum of dbs: 0.18776390411909447\n",
      "sum of Ws for layer 0: 86.42321014779874 and sum of bs: 0.0015995520220479019\n",
      "sum of abs dWs for layer 1: 18.67389391729614 and sum of dbs: 0.13028253017343536\n",
      "sum of Ws for layer 1: 321.3249722693869 and sum of bs: 0.0016887383540553913\n",
      "sum of abs dWs for layer 2: 10.982912246386697 and sum of dbs: 0.2381857232330203\n",
      "sum of Ws for layer 2: 340.8626855467475 and sum of bs: 0.007883387938186649\n",
      "sum of abs dWs for layer 3: 5.289249831065765 and sum of dbs: 0.08548816947098893\n",
      "sum of Ws for layer 3: 7.500587856515437 and sum of bs: 0.0023853014464471243\n",
      "MSE on the training set: 3.2118725596854705e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1380\n",
      "sum of abs dWs for layer 0: 0.6177439289221403 and sum of dbs: 0.1890060477751469\n",
      "sum of Ws for layer 0: 86.42321015219642 and sum of bs: 0.0015995505868008403\n",
      "sum of abs dWs for layer 1: 16.57072697877932 and sum of dbs: 0.13111495507201754\n",
      "sum of Ws for layer 1: 321.32497232070307 and sum of bs: 0.0016887372787404505\n",
      "sum of abs dWs for layer 2: 9.825041936032346 and sum of dbs: 0.23972472268810366\n",
      "sum of Ws for layer 2: 340.86268557679887 and sum of bs: 0.0078833857124182\n",
      "sum of abs dWs for layer 3: 4.667140158996985 and sum of dbs: 0.08604645326155162\n",
      "sum of Ws for layer 3: 7.500587890382859 and sum of bs: 0.002385300588438295\n",
      "MSE on the training set: 3.2112111778380447e-06\n",
      "Epoch: 1390\n",
      "sum of abs dWs for layer 0: 0.547372344685405 and sum of dbs: 0.19010082797812305\n",
      "sum of Ws for layer 0: 86.4232101560772 and sum of bs: 0.001599549142931888\n",
      "sum of abs dWs for layer 1: 14.71698210402409 and sum of dbs: 0.13184862350922738\n",
      "sum of Ws for layer 1: 321.32497236589836 and sum of bs: 0.0016887361972400309\n",
      "sum of abs dWs for layer 2: 8.80447257916132 and sum of dbs: 0.24108112370002666\n",
      "sum of Ws for layer 2: 340.8626856032353 and sum of bs: 0.007883383472044649\n",
      "sum of abs dWs for layer 3: 4.11881156558902 and sum of dbs: 0.08653850480821519\n",
      "sum of Ws for layer 3: 7.5005879202662955 and sum of bs: 0.002385299725214367\n",
      "MSE on the training set: 3.2106938012388687e-06\n",
      "Epoch: 1400\n",
      "sum of abs dWs for layer 0: 0.5037275999432204 and sum of dbs: 0.19077654929452198\n",
      "sum of Ws for layer 0: 86.4232101595482 and sum of bs: 0.0015995476899852209\n",
      "sum of abs dWs for layer 1: 12.915417220249546 and sum of dbs: 0.1325082968078276\n",
      "sum of Ws for layer 1: 321.32497240558973 and sum of bs: 0.0016887351094401711\n",
      "sum of abs dWs for layer 2: 7.553733927037566 and sum of dbs: 0.24195909248971484\n",
      "sum of Ws for layer 2: 340.8626856268087 and sum of bs: 0.007883381215542832\n",
      "sum of abs dWs for layer 3: 3.6358490378701336 and sum of dbs: 0.08697191185849609\n",
      "sum of Ws for layer 3: 7.500587946643173 and sum of bs: 0.002385298857400459\n",
      "MSE on the training set: 3.210292854536176e-06\n",
      "Epoch: 1410\n",
      "sum of abs dWs for layer 0: 0.4365728019646751 and sum of dbs: 0.1918813661821444\n",
      "sum of Ws for layer 0: 86.4232101626187 and sum of bs: 0.001599546230592214\n",
      "sum of abs dWs for layer 1: 11.691528766704115 and sum of dbs: 0.13307749436399727\n",
      "sum of Ws for layer 1: 321.3249724404577 and sum of bs: 0.0016887340164241796\n",
      "sum of abs dWs for layer 2: 7.111732853799223 and sum of dbs: 0.24333118844778553\n",
      "sum of Ws for layer 2: 340.86268564782824 and sum of bs: 0.007883378946717087\n",
      "sum of abs dWs for layer 3: 3.2092981582082456 and sum of dbs: 0.0873547177567722\n",
      "sum of Ws for layer 3: 7.500587969924662 and sum of bs: 0.00238529798553441\n",
      "MSE on the training set: 3.2099799407417424e-06\n",
      "Epoch: 1420\n",
      "sum of abs dWs for layer 0: 0.37944442406073875 and sum of dbs: 0.19256005165264836\n",
      "sum of Ws for layer 0: 86.42321016534454 and sum of bs: 0.0015995447648691238\n",
      "sum of abs dWs for layer 1: 9.791931819746798 and sum of dbs: 0.1335078207847215\n",
      "sum of Ws for layer 1: 321.32497247102606 and sum of bs: 0.001688732919527237\n",
      "sum of abs dWs for layer 2: 5.999528073453687 and sum of dbs: 0.24388805594328652\n",
      "sum of Ws for layer 2: 340.86268566642116 and sum of bs: 0.007883376667849875\n",
      "sum of abs dWs for layer 3: 2.8337736219565772 and sum of dbs: 0.08769170638521025\n",
      "sum of Ws for layer 3: 7.500587990480351 and sum of bs: 0.002385297110100207\n",
      "MSE on the training set: 3.2097344971049487e-06\n",
      "Epoch: 1430\n",
      "sum of abs dWs for layer 0: 0.33720406635378636 and sum of dbs: 0.19322569427705394\n",
      "sum of Ws for layer 0: 86.42321016773315 and sum of bs: 0.0015995432942024134\n",
      "sum of abs dWs for layer 1: 8.667505870686135 and sum of dbs: 0.13395386699829265\n",
      "sum of Ws for layer 1: 321.324972498359 and sum of bs: 0.0016887318191147413\n",
      "sum of abs dWs for layer 2: 5.380825575463602 and sum of dbs: 0.24471360814884777\n",
      "sum of Ws for layer 2: 340.8626856827248 and sum of bs: 0.007883374378689635\n",
      "sum of abs dWs for layer 3: 2.5005906706288927 and sum of dbs: 0.08799071871887748\n",
      "sum of Ws for layer 3: 7.500588008629083 and sum of bs: 0.0023852962315152773\n",
      "MSE on the training set: 3.2095430953326193e-06\n",
      "Epoch: 1440\n",
      "sum of abs dWs for layer 0: 0.29597806388081505 and sum of dbs: 0.19378333085946867\n",
      "sum of Ws for layer 0: 86.42321016983496 and sum of bs: 0.001599541818708389\n",
      "sum of abs dWs for layer 1: 8.231086758780648 and sum of dbs: 0.1342672489714662\n",
      "sum of Ws for layer 1: 321.32497252255826 and sum of bs: 0.0016887307158192691\n",
      "sum of abs dWs for layer 2: 4.461907490027718 and sum of dbs: 0.2451062493358704\n",
      "sum of Ws for layer 2: 340.86268569715696 and sum of bs: 0.007883372080628224\n",
      "sum of abs dWs for layer 3: 2.206592249838825 and sum of dbs: 0.08825456221483667\n",
      "sum of Ws for layer 3: 7.500588024641261 and sum of bs: 0.002385295350133365\n",
      "MSE on the training set: 3.209393952276991e-06\n",
      "Epoch: 1450\n",
      "sum of abs dWs for layer 0: 0.24723360430267113 and sum of dbs: 0.19458656806402555\n",
      "sum of Ws for layer 0: 86.42321017169479 and sum of bs: 0.0015995403388783338\n",
      "sum of abs dWs for layer 1: 7.078204787287982 and sum of dbs: 0.13464059414901458\n",
      "sum of Ws for layer 1: 321.3249725436365 and sum of bs: 0.0016887296096748127\n",
      "sum of abs dWs for layer 2: 4.545880151767488 and sum of dbs: 0.24625959392837363\n",
      "sum of Ws for layer 2: 340.86268570985 and sum of bs: 0.007883369776222535\n",
      "sum of abs dWs for layer 3: 1.9468737785365742 and sum of dbs: 0.08848764147056029\n",
      "sum of Ws for layer 3: 7.500588038767565 and sum of bs: 0.0023852944662826475\n",
      "MSE on the training set: 3.2092771332943493e-06\n",
      "Epoch: 1460\n",
      "sum of abs dWs for layer 0: 0.22775175184249882 and sum of dbs: 0.1947776441029853\n",
      "sum of Ws for layer 0: 86.42321017332566 and sum of bs: 0.001599538856001693\n",
      "sum of abs dWs for layer 1: 6.087408442314694 and sum of dbs: 0.13492576345755541\n",
      "sum of Ws for layer 1: 321.324972562569 and sum of bs: 0.0016887285008601056\n",
      "sum of abs dWs for layer 2: 3.177177310743915 and sum of dbs: 0.2459776098929888\n",
      "sum of Ws for layer 2: 340.8626857210412 and sum of bs: 0.007883367464443134\n",
      "sum of abs dWs for layer 3: 1.7178569156609311 and sum of dbs: 0.0886931647836318\n",
      "sum of Ws for layer 3: 7.500588051230152 and sum of bs: 0.0023852935802540077\n",
      "MSE on the training set: 3.209185522581569e-06\n",
      "Epoch: 1470\n",
      "sum of abs dWs for layer 0: 0.20092254383325578 and sum of dbs: 0.19518201162731885\n",
      "sum of Ws for layer 0: 86.42321017476598 and sum of bs: 0.001599537369793002\n",
      "sum of abs dWs for layer 1: 5.370301332288141 and sum of dbs: 0.13519686790776747\n",
      "sum of Ws for layer 1: 321.3249725791656 and sum of bs: 0.001688727389762496\n",
      "sum of abs dWs for layer 2: 2.8028635287950263 and sum of dbs: 0.24648047042690804\n",
      "sum of Ws for layer 2: 340.8626857309257 and sum of bs: 0.007883365147542013\n",
      "sum of abs dWs for layer 3: 1.5154557812012395 and sum of dbs: 0.08887479902566274\n",
      "sum of Ws for layer 3: 7.50058806222402 and sum of bs: 0.002385292692302747\n",
      "MSE on the training set: 3.2091134206151834e-06\n",
      "Epoch: 1480\n",
      "sum of abs dWs for layer 0: 0.1772505659254916 and sum of dbs: 0.19553878216886197\n",
      "sum of Ws for layer 0: 86.42321017603811 and sum of bs: 0.0015995358808528022\n",
      "sum of abs dWs for layer 1: 4.737582558629541 and sum of dbs: 0.1354360611481877\n",
      "sum of Ws for layer 1: 321.3249725938125 and sum of bs: 0.0016887262766580472\n",
      "sum of abs dWs for layer 2: 2.4725985613160404 and sum of dbs: 0.24692414041039132\n",
      "sum of Ws for layer 2: 340.8626857396209 and sum of bs: 0.00788336282579438\n",
      "sum of abs dWs for layer 3: 1.3368729796076624 and sum of dbs: 0.08903505352902896\n",
      "sum of Ws for layer 3: 7.500588071921949 and sum of bs: 0.002385291802655102\n",
      "MSE on the training set: 3.20905675570985e-06\n",
      "Epoch: 1490\n",
      "sum of abs dWs for layer 0: 0.16314882314342943 and sum of dbs: 0.1960038615014629\n",
      "sum of Ws for layer 0: 86.42321017716017 and sum of bs: 0.0015995343894639837\n",
      "sum of abs dWs for layer 1: 5.068734582951203 and sum of dbs: 0.13563595182772614\n",
      "sum of Ws for layer 1: 321.32497260652724 and sum of bs: 0.0016887251619672045\n",
      "sum of abs dWs for layer 2: 2.555906019299989 and sum of dbs: 0.2476497415407208\n",
      "sum of Ws for layer 2: 340.86268574742655 and sum of bs: 0.007883360500194806\n",
      "sum of abs dWs for layer 3: 1.1800815912194644 and sum of dbs: 0.08917575077978034\n",
      "sum of Ws for layer 3: 7.500588080478375 and sum of bs: 0.0023852909115132824\n",
      "MSE on the training set: 3.2090119893992052e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1500\n",
      "sum of abs dWs for layer 0: 0.13796313485194536 and sum of dbs: 0.19613086670924024\n",
      "sum of Ws for layer 0: 86.42321017815 and sum of bs: 0.0015995328957120515\n",
      "sum of abs dWs for layer 1: 3.687484625724854 and sum of dbs: 0.13583301757002347\n",
      "sum of Ws for layer 1: 321.3249726180027 and sum of bs: 0.0016887240455518682\n",
      "sum of abs dWs for layer 2: 1.9244710631578328 and sum of dbs: 0.24766043972526552\n",
      "sum of Ws for layer 2: 340.862685754206 and sum of bs: 0.007883358169590664\n",
      "sum of abs dWs for layer 3: 1.0404862797816912 and sum of dbs: 0.0893010063672956\n",
      "sum of Ws for layer 3: 7.5005880880269205 and sum of bs: 0.0023852900190522485\n",
      "MSE on the training set: 3.208976261593731e-06\n",
      "Epoch: 1510\n",
      "sum of abs dWs for layer 0: 0.12170200231637011 and sum of dbs: 0.19637591467708768\n",
      "sum of Ws for layer 0: 86.4232101790247 and sum of bs: 0.0015995314002061481\n",
      "sum of abs dWs for layer 1: 3.252847361057409 and sum of dbs: 0.1359973068518649\n",
      "sum of Ws for layer 1: 321.32497262804645 and sum of bs: 0.0016887229278716768\n",
      "sum of abs dWs for layer 2: 1.697600189266249 and sum of dbs: 0.2479651740064197\n",
      "sum of Ws for layer 2: 340.86268576011656 and sum of bs: 0.007883355836590505\n",
      "sum of abs dWs for layer 3: 0.9178113372728611 and sum of dbs: 0.089411077033635\n",
      "sum of Ws for layer 3: 7.5005880946854795 and sum of bs: 0.002385289125426356\n",
      "MSE on the training set: 3.2089479780437997e-06\n",
      "Epoch: 1520\n",
      "sum of abs dWs for layer 0: 0.12524460663105824 and sum of dbs: 0.1967427236572412\n",
      "sum of Ws for layer 0: 86.42321017980031 and sum of bs: 0.0015995299028558863\n",
      "sum of abs dWs for layer 1: 4.00766962614458 and sum of dbs: 0.13613173175173915\n",
      "sum of Ws for layer 1: 321.3249726368619 and sum of bs: 0.0016887218088764908\n",
      "sum of abs dWs for layer 2: 1.869925263021169 and sum of dbs: 0.2485680168268654\n",
      "sum of Ws for layer 2: 340.8626857654654 and sum of bs: 0.00788335350010539\n",
      "sum of abs dWs for layer 3: 0.8099100623578215 and sum of dbs: 0.08950788842817944\n",
      "sum of Ws for layer 3: 7.500588100559124 and sum of bs: 0.002385288230773183\n",
      "MSE on the training set: 3.208925092952986e-06\n",
      "Epoch: 1530\n",
      "sum of abs dWs for layer 0: 0.13643111930048163 and sum of dbs: 0.196632167018689\n",
      "sum of Ws for layer 0: 86.42321018047987 and sum of bs: 0.0015995284042020167\n",
      "sum of abs dWs for layer 1: 3.8522807820395126 and sum of dbs: 0.1363929266630265\n",
      "sum of Ws for layer 1: 321.32497264467287 and sum of bs: 0.0016887206886718191\n",
      "sum of abs dWs for layer 2: 2.1264794783533794 and sum of dbs: 0.24919620904800294\n",
      "sum of Ws for layer 2: 340.86268577011583 and sum of bs: 0.007883351161627397\n",
      "sum of abs dWs for layer 3: 0.7141716146787437 and sum of dbs: 0.0895937791199667\n",
      "sum of Ws for layer 3: 7.500588105740032 and sum of bs: 0.0023852873352134224\n",
      "MSE on the training set: 3.2089067674762256e-06\n",
      "Epoch: 1540\n",
      "sum of abs dWs for layer 0: 0.08359952617522855 and sum of dbs: 0.1969500378142276\n",
      "sum of Ws for layer 0: 86.4232101810798 and sum of bs: 0.0015995269041361854\n",
      "sum of abs dWs for layer 1: 2.2344216217391626 and sum of dbs: 0.1363822189643235\n",
      "sum of Ws for layer 1: 321.3249726516089 and sum of bs: 0.0016887195679117593\n",
      "sum of abs dWs for layer 2: 1.1660048671595944 and sum of dbs: 0.2486791350270865\n",
      "sum of Ws for layer 2: 340.86268577421095 and sum of bs: 0.00788334881994197\n",
      "sum of abs dWs for layer 3: 0.630364014565395 and sum of dbs: 0.08966896131060816\n",
      "sum of Ws for layer 3: 7.500588110311031 and sum of bs: 0.0023852864388555274\n",
      "MSE on the training set: 3.2088916561846575e-06\n",
      "Epoch: 1550\n",
      "sum of abs dWs for layer 0: 0.10793962456016432 and sum of dbs: 0.19718453436858996\n",
      "sum of Ws for layer 0: 86.42321018159869 and sum of bs: 0.0015995254031898744\n",
      "sum of abs dWs for layer 1: 2.9031838935371104 and sum of dbs: 0.13652127542955306\n",
      "sum of Ws for layer 1: 321.32497265777283 and sum of bs: 0.001688718446144392\n",
      "sum of abs dWs for layer 2: 1.4331273007038525 and sum of dbs: 0.24922649243076758\n",
      "sum of Ws for layer 2: 340.862685777878 and sum of bs: 0.007883346476745691\n",
      "sum of abs dWs for layer 3: 0.5563971284512221 and sum of dbs: 0.08973531150376932\n",
      "sum of Ws for layer 3: 7.500588114344015 and sum of bs: 0.0023852855417936286\n",
      "MSE on the training set: 3.2088792247870866e-06\n",
      "Epoch: 1560\n",
      "sum of abs dWs for layer 0: 0.0651254751424505 and sum of dbs: 0.1972283491962551\n",
      "sum of Ws for layer 0: 86.42321018207302 and sum of bs: 0.0015995239006290128\n",
      "sum of abs dWs for layer 1: 1.7406361545487894 and sum of dbs: 0.136568807415567\n",
      "sum of Ws for layer 1: 321.32497266293666 and sum of bs: 0.001688717323824835\n",
      "sum of abs dWs for layer 2: 0.9082599641808213 and sum of dbs: 0.2490252330607569\n",
      "sum of Ws for layer 2: 340.8626857810642 and sum of bs: 0.00788334413210059\n",
      "sum of abs dWs for layer 3: 0.49099468813529323 and sum of dbs: 0.0897939727150795\n",
      "sum of Ws for layer 3: 7.500588117902422 and sum of bs: 0.0023852846441107887\n",
      "MSE on the training set: 3.2088688738464556e-06\n",
      "Epoch: 1570\n",
      "sum of abs dWs for layer 0: 0.05746414027154111 and sum of dbs: 0.19734375048092714\n",
      "sum of Ws for layer 0: 86.42321018248384 and sum of bs: 0.0015995223975188732\n",
      "sum of abs dWs for layer 1: 1.5358594218999964 and sum of dbs: 0.13664617559320744\n",
      "sum of Ws for layer 1: 321.32497266771344 and sum of bs: 0.0016887162006179008\n",
      "sum of abs dWs for layer 2: 0.8013711177265681 and sum of dbs: 0.2491687416318683\n",
      "sum of Ws for layer 2: 340.8626857839404 and sum of bs: 0.00788334178510677\n",
      "sum of abs dWs for layer 3: 0.4331971281274176 and sum of dbs: 0.08984580835363157\n",
      "sum of Ws for layer 3: 7.50058812104275 and sum of bs: 0.0023852837458809848\n",
      "MSE on the training set: 3.2088601081593866e-06\n",
      "Epoch: 1580\n",
      "sum of abs dWs for layer 0: 0.050743012075033365 and sum of dbs: 0.19744497763606367\n",
      "sum of Ws for layer 0: 86.42321018285476 and sum of bs: 0.0015995208934927547\n",
      "sum of abs dWs for layer 1: 1.3562130948615363 and sum of dbs: 0.13671404078292285\n",
      "sum of Ws for layer 1: 321.3249726717921 and sum of bs: 0.001688715076873642\n",
      "sum of abs dWs for layer 2: 0.7075997776320537 and sum of dbs: 0.24929462357331408\n",
      "sum of Ws for layer 2: 340.86268578644797 and sum of bs: 0.007883339437156994\n",
      "sum of abs dWs for layer 3: 0.38249254273403166 and sum of dbs: 0.0898912772203033\n",
      "sum of Ws for layer 3: 7.500588123812167 and sum of bs: 0.0023852828471659752\n",
      "MSE on the training set: 3.208852609406491e-06\n",
      "Epoch: 1590\n",
      "sum of abs dWs for layer 0: 0.06871162205946699 and sum of dbs: 0.1975868741960872\n",
      "sum of Ws for layer 0: 86.4232101831678 and sum of bs: 0.0015995193891533814\n",
      "sum of abs dWs for layer 1: 1.930526278320803 and sum of dbs: 0.13688092274542424\n",
      "sum of Ws for layer 1: 321.3249726755532 and sum of bs: 0.0016887139526440228\n",
      "sum of abs dWs for layer 2: 0.9897404666627958 and sum of dbs: 0.24973671399800626\n",
      "sum of Ws for layer 2: 340.8626857886678 and sum of bs: 0.007883337088104255\n",
      "sum of abs dWs for layer 3: 0.3371482792533208 and sum of dbs: 0.08993193521708276\n",
      "sum of Ws for layer 3: 7.500588126255784 and sum of bs: 0.002385281948024824\n",
      "MSE on the training set: 3.2088460833501487e-06\n",
      "Epoch: 1600\n",
      "sum of abs dWs for layer 0: 0.08650487967081691 and sum of dbs: 0.19757917353485618\n",
      "sum of Ws for layer 0: 86.42321018344964 and sum of bs: 0.0015995178839798465\n",
      "sum of abs dWs for layer 1: 2.620977053305782 and sum of dbs: 0.13679118186715344\n",
      "sum of Ws for layer 1: 321.32497267888596 and sum of bs: 0.0016887128279762268\n",
      "sum of abs dWs for layer 2: 0.9231561231944794 and sum of dbs: 0.2498431143393852\n",
      "sum of Ws for layer 2: 340.8626857906296 and sum of bs: 0.007883334737358035\n",
      "sum of abs dWs for layer 3: 0.2970360287784986 and sum of dbs: 0.08996789639531605\n",
      "sum of Ws for layer 3: 7.500588128411333 and sum of bs: 0.0023852810485069363\n",
      "MSE on the training set: 3.2088403418937542e-06\n",
      "Epoch: 1610\n",
      "sum of abs dWs for layer 0: 0.034880884916823125 and sum of dbs: 0.1976838152686082\n",
      "sum of Ws for layer 0: 86.42321018370382 and sum of bs: 0.0015995163779536513\n",
      "sum of abs dWs for layer 1: 0.9322406912356765 and sum of dbs: 0.1368741620593325\n",
      "sum of Ws for layer 1: 321.3249726817207 and sum of bs: 0.001688711702953235\n",
      "sum of abs dWs for layer 2: 0.48629572434233537 and sum of dbs: 0.2495916310677245\n",
      "sum of Ws for layer 2: 340.8626857922707 and sum of bs: 0.00788333238611667\n",
      "sum of abs dWs for layer 3: 0.2628277257587102 and sum of dbs: 0.08999855710716083\n",
      "sum of Ws for layer 3: 7.500588130315314 and sum of bs: 0.002385280148660101\n",
      "MSE on the training set: 3.208835035247062e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1620\n",
      "sum of abs dWs for layer 0: 0.030902380843600523 and sum of dbs: 0.19774369586263082\n",
      "sum of Ws for layer 0: 86.4232101839288 and sum of bs: 0.001599514871630405\n",
      "sum of abs dWs for layer 1: 0.8259008596458042 and sum of dbs: 0.13691430661399986\n",
      "sum of Ws for layer 1: 321.32497268417825 and sum of bs: 0.001688710577325531\n",
      "sum of abs dWs for layer 2: 0.4307887256056536 and sum of dbs: 0.249666095355248\n",
      "sum of Ws for layer 2: 340.8626857938583 and sum of bs: 0.007883330034041219\n",
      "sum of abs dWs for layer 3: 0.2328136581137469 and sum of dbs: 0.09002545382072104\n",
      "sum of Ws for layer 3: 7.500588131993462 and sum of bs: 0.0023852792485180214\n",
      "MSE on the training set: 3.2088303181795928e-06\n",
      "Epoch: 1630\n",
      "sum of abs dWs for layer 0: 0.027172586189628544 and sum of dbs: 0.19779982700466395\n",
      "sum of Ws for layer 0: 86.42321018411818 and sum of bs: 0.0015995133649974714\n",
      "sum of abs dWs for layer 1: 0.7262086819106434 and sum of dbs: 0.13695193735694483\n",
      "sum of Ws for layer 1: 321.32497268654146 and sum of bs: 0.0016887094518300036\n",
      "sum of abs dWs for layer 2: 0.3787516525904049 and sum of dbs: 0.24973589691200304\n",
      "sum of Ws for layer 2: 340.8626857951696 and sum of bs: 0.007883327680626112\n",
      "sum of abs dWs for layer 3: 0.2046758689834739 and sum of dbs: 0.0900506663502949\n",
      "sum of Ws for layer 3: 7.500588133477415 and sum of bs: 0.002385278348122119\n",
      "MSE on the training set: 3.2088258703070754e-06\n",
      "Epoch: 1640\n",
      "sum of abs dWs for layer 0: 0.024033965369074223 and sum of dbs: 0.1978470458898961\n",
      "sum of Ws for layer 0: 86.42321018429921 and sum of bs: 0.001599511857856518\n",
      "sum of abs dWs for layer 1: 0.6423177513584016 and sum of dbs: 0.13698359291723666\n",
      "sum of Ws for layer 1: 321.32497268841655 and sum of bs: 0.0016887083258321574\n",
      "sum of abs dWs for layer 2: 0.3349624750885086 and sum of dbs: 0.2497946153845079\n",
      "sum of Ws for layer 2: 340.8626857963409 and sum of bs: 0.007883325327260682\n",
      "sum of abs dWs for layer 3: 0.18099792998801445 and sum of dbs: 0.09007187564721089\n",
      "sum of Ws for layer 3: 7.5005881347830785 and sum of bs: 0.0023852774474932108\n",
      "MSE on the training set: 3.208821756017018e-06\n",
      "Epoch: 1650\n",
      "sum of abs dWs for layer 0: 0.021239717234931545 and sum of dbs: 0.19788907316556476\n",
      "sum of Ws for layer 0: 86.42321018445044 and sum of bs: 0.0015995103503982887\n",
      "sum of abs dWs for layer 1: 0.5676314195636206 and sum of dbs: 0.13701176778289983\n",
      "sum of Ws for layer 1: 321.3249726901385 and sum of bs: 0.001688707199621128\n",
      "sum of abs dWs for layer 2: 0.29597789101853145 and sum of dbs: 0.24984687768891592\n",
      "sum of Ws for layer 2: 340.86268579740795 and sum of bs: 0.007883322973029069\n",
      "sum of abs dWs for layer 3: 0.15991795874567782 and sum of dbs: 0.09009075296459756\n",
      "sum of Ws for layer 3: 7.500588135937052 and sum of bs: 0.002385276546666136\n",
      "MSE on the training set: 3.2088178457766665e-06\n",
      "Epoch: 1660\n",
      "sum of abs dWs for layer 0: 0.018683109730341853 and sum of dbs: 0.1979275178679075\n",
      "sum of Ws for layer 0: 86.42321018458064 and sum of bs: 0.0015995088427429212\n",
      "sum of abs dWs for layer 1: 0.4992968883853311 and sum of dbs: 0.13703754072383073\n",
      "sum of Ws for layer 1: 321.32497269176287 and sum of bs: 0.0016887060733550086\n",
      "sum of abs dWs for layer 2: 0.2603088039536032 and sum of dbs: 0.249894684787304\n",
      "sum of Ws for layer 2: 340.8626857983089 and sum of bs: 0.007883320617997548\n",
      "sum of abs dWs for layer 3: 0.1406307623769112 and sum of dbs: 0.0901080210545747\n",
      "sum of Ws for layer 3: 7.500588136955293 and sum of bs: 0.002385275645661784\n",
      "MSE on the training set: 3.2088140812037506e-06\n",
      "Epoch: 1670\n",
      "sum of abs dWs for layer 0: 0.01643691177261968 and sum of dbs: 0.19796128297791646\n",
      "sum of Ws for layer 0: 86.42321018469517 and sum of bs: 0.0015995073348171846\n",
      "sum of abs dWs for layer 1: 0.4392591688828259 and sum of dbs: 0.13706017625099987\n",
      "sum of Ws for layer 1: 321.32497269319185 and sum of bs: 0.0016887049468851157\n",
      "sum of abs dWs for layer 2: 0.2289704658983362 and sum of dbs: 0.24993667245002893\n",
      "sum of Ws for layer 2: 340.8626857991014 and sum of bs: 0.00788331826253859\n",
      "sum of abs dWs for layer 3: 0.12368531385092553 and sum of dbs: 0.09012318715348745\n",
      "sum of Ws for layer 3: 7.500588137850312 and sum of bs: 0.0023852747444965366\n",
      "MSE on the training set: 3.208810454898908e-06\n",
      "Epoch: 1680\n",
      "sum of abs dWs for layer 0: 0.014686114972294212 and sum of dbs: 0.19798757966517838\n",
      "sum of Ws for layer 0: 86.42321018479853 and sum of bs: 0.0015995058268540342\n",
      "sum of abs dWs for layer 1: 0.3924628272440406 and sum of dbs: 0.1370778046111\n",
      "sum of Ws for layer 1: 321.32497269433577 and sum of bs: 0.0016887038199254272\n",
      "sum of abs dWs for layer 2: 0.20454382914936392 and sum of dbs: 0.24996937254694093\n",
      "sum of Ws for layer 2: 340.8626857998674 and sum of bs: 0.007883315907755163\n",
      "sum of abs dWs for layer 3: 0.11047720070767922 and sum of dbs: 0.09013499856310878\n",
      "sum of Ws for layer 3: 7.500588138644068 and sum of bs: 0.0023852738431991453\n",
      "MSE on the training set: 3.2088069736645023e-06\n",
      "Epoch: 1690\n",
      "sum of abs dWs for layer 0: 0.012937214453156806 and sum of dbs: 0.1980138477446511\n",
      "sum of Ws for layer 0: 86.4232101848962 and sum of bs: 0.001599504318292305\n",
      "sum of abs dWs for layer 1: 0.3457171705368781 and sum of dbs: 0.13709541378474693\n",
      "sum of Ws for layer 1: 321.32497269538294 and sum of bs: 0.0016887026930848359\n",
      "sum of abs dWs for layer 2: 0.18014364879441466 and sum of dbs: 0.2500020370709639\n",
      "sum of Ws for layer 2: 340.8626858004039 and sum of bs: 0.007883313552015046\n",
      "sum of abs dWs for layer 3: 0.09728339323987895 and sum of dbs: 0.09014679712668922\n",
      "sum of Ws for layer 3: 7.5005881393468945 and sum of bs: 0.002385272941783139\n",
      "MSE on the training set: 3.2088035292227126e-06\n",
      "Epoch: 1700\n",
      "sum of abs dWs for layer 0: 0.011388651479016111 and sum of dbs: 0.19803709562439295\n",
      "sum of Ws for layer 0: 86.42321018497553 and sum of bs: 0.0015995028097594826\n",
      "sum of abs dWs for layer 1: 0.304326255317401 and sum of dbs: 0.13711099807741395\n",
      "sum of Ws for layer 1: 321.32497269637247 and sum of bs: 0.0016887015661573863\n",
      "sum of abs dWs for layer 2: 0.1585385229942725 and sum of dbs: 0.25003094574300333\n",
      "sum of Ws for layer 2: 340.8626858009523 and sum of bs: 0.0078833111955963\n",
      "sum of abs dWs for layer 3: 0.08560094396595185 and sum of dbs: 0.09015723906967281\n",
      "sum of Ws for layer 3: 7.500588139964976 and sum of bs: 0.0023852720402566174\n",
      "MSE on the training set: 3.2088001480494364e-06\n",
      "Epoch: 1710\n",
      "sum of abs dWs for layer 0: 0.010028106728122758 and sum of dbs: 0.19805750902666763\n",
      "sum of Ws for layer 0: 86.42321018504538 and sum of bs: 0.0015995013010631884\n",
      "sum of abs dWs for layer 1: 0.26796080339756695 and sum of dbs: 0.13712468200612773\n",
      "sum of Ws for layer 1: 321.32497269724365 and sum of bs: 0.001688700439106728\n",
      "sum of abs dWs for layer 2: 0.1395565759550929 and sum of dbs: 0.25005632952272233\n",
      "sum of Ws for layer 2: 340.86268580143496 and sum of bs: 0.007883308838919108\n",
      "sum of abs dWs for layer 3: 0.07533691519633663 and sum of dbs: 0.09016640781495174\n",
      "sum of Ws for layer 3: 7.500588140508422 and sum of bs: 0.002385271138632815\n",
      "MSE on the training set: 3.2087968175465695e-06\n",
      "Epoch: 1720\n",
      "sum of abs dWs for layer 0: 0.008932970225582156 and sum of dbs: 0.19807392126382767\n",
      "sum of Ws for layer 0: 86.42321018511092 and sum of bs: 0.001599499792126165\n",
      "sum of abs dWs for layer 1: 0.23868934155815702 and sum of dbs: 0.13713568337280044\n",
      "sum of Ws for layer 1: 321.32497269789565 and sum of bs: 0.0016886993117812329\n",
      "sum of abs dWs for layer 2: 0.12427753210848681 and sum of dbs: 0.25007673754212\n",
      "sum of Ws for layer 2: 340.8626858019077 and sum of bs: 0.007883306482349093\n",
      "sum of abs dWs for layer 3: 0.06707514181239692 and sum of dbs: 0.09017377930493312\n",
      "sum of Ws for layer 3: 7.500588140987868 and sum of bs: 0.0023852702369256593\n",
      "MSE on the training set: 3.2087935367743016e-06\n",
      "Epoch: 1730\n",
      "sum of abs dWs for layer 0: 0.007870580857682451 and sum of dbs: 0.19808983981127046\n",
      "sum of Ws for layer 0: 86.42321018516571 and sum of bs: 0.0015994982831709476\n",
      "sum of abs dWs for layer 1: 0.21029316467861145 and sum of dbs: 0.13714635374427558\n",
      "sum of Ws for layer 1: 321.324972698579 and sum of bs: 0.0016886981845354223\n",
      "sum of abs dWs for layer 2: 0.10945536731608772 and sum of dbs: 0.25009653162164963\n",
      "sum of Ws for layer 2: 340.8626858022861 and sum of bs: 0.00788330412526254\n",
      "sum of abs dWs for layer 3: 0.059060414736976895 and sum of dbs: 0.09018092904004193\n",
      "sum of Ws for layer 3: 7.5005881414129565 and sum of bs: 0.0023852693351477743\n",
      "MSE on the training set: 3.2087902732002393e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1740\n",
      "sum of abs dWs for layer 0: 0.007064535060701531 and sum of dbs: 0.19810189385100205\n",
      "sum of Ws for layer 0: 86.42321018522152 and sum of bs: 0.0015994967740952809\n",
      "sum of abs dWs for layer 1: 0.18874869176510317 and sum of dbs: 0.13715443317603765\n",
      "sum of Ws for layer 1: 321.3249726990909 and sum of bs: 0.001688697056927314\n",
      "sum of abs dWs for layer 2: 0.0982096378205022 and sum of dbs: 0.2501115198845545\n",
      "sum of Ws for layer 2: 340.86268580268086 and sum of bs: 0.007883301768346338\n",
      "sum of abs dWs for layer 3: 0.05297955858522093 and sum of dbs: 0.09018634290342407\n",
      "sum of Ws for layer 3: 7.500588141790221 and sum of bs: 0.002385268433307716\n",
      "MSE on the training set: 3.208787044092086e-06\n",
      "Epoch: 1750\n",
      "sum of abs dWs for layer 0: 0.0062290006458847955 and sum of dbs: 0.19811439243506737\n",
      "sum of Ws for layer 0: 86.42321018526488 and sum of bs: 0.0015994952649434143\n",
      "sum of abs dWs for layer 1: 0.16641602947253573 and sum of dbs: 0.1371628106469036\n",
      "sum of Ws for layer 1: 321.3249726996314 and sum of bs: 0.0016886959295332852\n",
      "sum of abs dWs for layer 2: 0.08655249119929104 and sum of dbs: 0.2501270609747008\n",
      "sum of Ws for layer 2: 340.86268580297985 and sum of bs: 0.007883299410948877\n",
      "sum of abs dWs for layer 3: 0.046676238576643686 and sum of dbs: 0.09019195645080952\n",
      "sum of Ws for layer 3: 7.500588142125254 and sum of bs: 0.0023852675314128087\n",
      "MSE on the training set: 3.208783820423029e-06\n",
      "Epoch: 1760\n",
      "sum of abs dWs for layer 0: 0.005494912170508116 and sum of dbs: 0.19812536166600406\n",
      "sum of Ws for layer 0: 86.4232101853031 and sum of bs: 0.0015994937557036882\n",
      "sum of abs dWs for layer 1: 0.1467948747975197 and sum of dbs: 0.13717016276915284\n",
      "sum of Ws for layer 1: 321.324972700108 and sum of bs: 0.0016886948020730342\n",
      "sum of abs dWs for layer 2: 0.07631069053920553 and sum of dbs: 0.2501407001957659\n",
      "sum of Ws for layer 2: 340.86268580324344 and sum of bs: 0.007883297053412501\n",
      "sum of abs dWs for layer 3: 0.04113823268217055 and sum of dbs: 0.09019688303952458\n",
      "sum of Ws for layer 3: 7.50058814242002 and sum of bs: 0.0023852666294656177\n",
      "MSE on the training set: 3.208780611511895e-06\n",
      "Epoch: 1770\n",
      "sum of abs dWs for layer 0: 0.004849952731311344 and sum of dbs: 0.19813498722850348\n",
      "sum of Ws for layer 0: 86.42321018533684 and sum of bs: 0.0015994922463868594\n",
      "sum of abs dWs for layer 1: 0.12955601419766397 and sum of dbs: 0.13717661403433556\n",
      "sum of Ws for layer 1: 321.3249727005285 and sum of bs: 0.0016886936745546684\n",
      "sum of abs dWs for layer 2: 0.06731239377894876 and sum of dbs: 0.25015266846104867\n",
      "sum of Ws for layer 2: 340.86268580347576 and sum of bs: 0.00788329469575422\n",
      "sum of abs dWs for layer 3: 0.03627262146115003 and sum of dbs: 0.09020120607588518\n",
      "sum of Ws for layer 3: 7.500588142679403 and sum of bs: 0.002385265727472544\n",
      "MSE on the training set: 3.2087774139890886e-06\n",
      "Epoch: 1780\n",
      "sum of abs dWs for layer 0: 0.004283300788780392 and sum of dbs: 0.19814343226265954\n",
      "sum of Ws for layer 0: 86.42321018536664 and sum of bs: 0.0015994907370023804\n",
      "sum of abs dWs for layer 1: 0.11441020310554975 and sum of dbs: 0.137182273819068\n",
      "sum of Ws for layer 1: 321.32497270089976 and sum of bs: 0.0016886925469853115\n",
      "sum of abs dWs for layer 2: 0.05940662176286199 and sum of dbs: 0.2501631686478943\n",
      "sum of Ws for layer 2: 340.86268580368085 and sum of bs: 0.007883292337988975\n",
      "sum of abs dWs for layer 3: 0.031997766584843865 and sum of dbs: 0.09020499883956809\n",
      "sum of Ws for layer 3: 7.500588142907701 and sum of bs: 0.002385264825439214\n",
      "MSE on the training set: 3.2087742252371583e-06\n",
      "Epoch: 1790\n",
      "sum of abs dWs for layer 0: 0.0037854483480921322 and sum of dbs: 0.1981508401061477\n",
      "sum of Ws for layer 0: 86.42321018539296 and sum of bs: 0.0015994892275585548\n",
      "sum of abs dWs for layer 1: 0.10110330616042429 and sum of dbs: 0.13718723822351495\n",
      "sum of Ws for layer 1: 321.3249727012276 and sum of bs: 0.0016886914193712224\n",
      "sum of abs dWs for layer 2: 0.05246072155117619 and sum of dbs: 0.2501723790078061\n",
      "sum of Ws for layer 2: 340.8626858038617 and sum of bs: 0.007883289980129897\n",
      "sum of abs dWs for layer 3: 0.028241939182766556 and sum of dbs: 0.09020832571534992\n",
      "sum of Ws for layer 3: 7.500588143108686 and sum of bs: 0.002385263923370568\n",
      "MSE on the training set: 3.2087710433032e-06\n",
      "Epoch: 1800\n",
      "sum of abs dWs for layer 0: 0.0033480428116588416 and sum of dbs: 0.19815733667978988\n",
      "sum of Ws for layer 0: 86.42321018541622 and sum of bs: 0.0015994877180626782\n",
      "sum of abs dWs for layer 1: 0.08941207014855417 and sum of dbs: 0.137191591670782\n",
      "sum of Ws for layer 1: 321.32497270151737 and sum of bs: 0.001688690291717899\n",
      "sum of abs dWs for layer 2: 0.04635815998999536 and sum of dbs: 0.2501804561330808\n",
      "sum of Ws for layer 2: 340.8626858040215 and sum of bs: 0.007883287622188517\n",
      "sum of abs dWs for layer 3: 0.024942126768986685 and sum of dbs: 0.09021124326466122\n",
      "sum of Ws for layer 3: 7.500588143285679 and sum of bs: 0.0023852630212709484\n",
      "MSE on the training set: 3.20876786660036e-06\n",
      "Epoch: 1810\n",
      "sum of abs dWs for layer 0: 0.002963744427045704 and sum of dbs: 0.19816303263447888\n",
      "sum of Ws for layer 0: 86.4232101854368 and sum of bs: 0.0015994862085211608\n",
      "sum of abs dWs for layer 1: 0.07914031378605131 and sum of dbs: 0.13719540834631244\n",
      "sum of Ws for layer 1: 321.3249727017738 and sum of bs: 0.0016886891640301729\n",
      "sum of abs dWs for layer 2: 0.040996534863640105 and sum of dbs: 0.25018753762667467\n",
      "sum of Ws for layer 2: 340.8626858041627 and sum of bs: 0.007883285264174967\n",
      "sum of abs dWs for layer 3: 0.02204295782082071 and sum of dbs: 0.09021380118994726\n",
      "sum of Ws for layer 3: 7.500588143441588 and sum of bs: 0.00238526211914417\n",
      "MSE on the training set: 3.2087646939473367e-06\n",
      "Epoch: 1820\n",
      "sum of abs dWs for layer 0: 0.0026261053164853146 and sum of dbs: 0.19816802517460091\n",
      "sum of Ws for layer 0: 86.42321018545505 and sum of bs: 0.0015994846989396342\n",
      "sum of abs dWs for layer 1: 0.07011569434754442 and sum of dbs: 0.137198753420394\n",
      "sum of Ws for layer 1: 321.32497270200076 and sum of bs: 0.0016886880363122876\n",
      "sum of abs dWs for layer 2: 0.03628588714759358 and sum of dbs: 0.2501937443697718\n",
      "sum of Ws for layer 2: 340.86268580428765 and sum of bs: 0.007883282906098155\n",
      "sum of abs dWs for layer 3: 0.019495789170422332 and sum of dbs: 0.09021604315371794\n",
      "sum of Ws for layer 3: 7.500588143578977 and sum of bs: 0.0023852612169935833\n",
      "MSE on the training set: 3.208761524415009e-06\n",
      "Epoch: 1830\n",
      "sum of abs dWs for layer 0: 0.002329460592239803 and sum of dbs: 0.1981723997020869\n",
      "sum of Ws for layer 0: 86.42321018547123 and sum of bs: 0.0015994831893230465\n",
      "sum of abs dWs for layer 1: 0.06218679733251068 and sum of dbs: 0.13720168415040562\n",
      "sum of Ws for layer 1: 321.3249727022019 and sum of bs: 0.0016886869085679726\n",
      "sum of abs dWs for layer 2: 0.032147181879762524 and sum of dbs: 0.2501991825662886\n",
      "sum of Ws for layer 2: 340.8626858043982 and sum of bs: 0.0078832805479659\n",
      "sum of abs dWs for layer 3: 0.017257884572620952 and sum of dbs: 0.09021800751702644\n",
      "sum of Ws for layer 3: 7.500588143700091 and sum of bs: 0.002385260314822134\n",
      "MSE on the training set: 3.2087583573014176e-06\n",
      "Epoch: 1840\n",
      "sum of abs dWs for layer 0: 0.002068832883191079 and sum of dbs: 0.1981762312534387\n",
      "sum of Ws for layer 0: 86.42321018548557 and sum of bs: 0.0015994816796757443\n",
      "sum of abs dWs for layer 1: 0.05522058459502735 and sum of dbs: 0.1372042508442664\n",
      "sum of Ws for layer 1: 321.3249727023803 and sum of bs: 0.001688685780800505\n",
      "sum of abs dWs for layer 2: 0.028510976141681577 and sum of dbs: 0.25020394552992725\n",
      "sum of Ws for layer 2: 340.86268580449615 and sum of bs: 0.007883278189785074\n",
      "sum of abs dWs for layer 3: 0.01529169444797554 and sum of dbs: 0.09021972798495664\n",
      "sum of Ws for layer 3: 7.500588143806907 and sum of bs: 0.0023852594126324077\n",
      "MSE on the training set: 3.208755192033934e-06\n",
      "Epoch: 1850\n",
      "sum of abs dWs for layer 0: 0.0018398491086972248 and sum of dbs: 0.19817958575537162\n",
      "sum of Ws for layer 0: 86.42321018549833 and sum of bs: 0.001599480170001548\n",
      "sum of abs dWs for layer 1: 0.04910016982570753 and sum of dbs: 0.13720649770227583\n",
      "sum of Ws for layer 1: 321.32497270253884 and sum of bs: 0.0016886846530127625\n",
      "sum of abs dWs for layer 2: 0.02531625791009267 and sum of dbs: 0.2502081152456649\n",
      "sum of Ws for layer 2: 340.86268580458307 and sum of bs: 0.007883275831561717\n",
      "sum of abs dWs for layer 3: 0.013564228019818289 and sum of dbs: 0.09022123417063557\n",
      "sum of Ws for layer 3: 7.500588143901163 and sum of bs: 0.0023852585104266787\n",
      "MSE on the training set: 3.2087520282114564e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1860\n",
      "sum of abs dWs for layer 0: 0.0017388274664520828 and sum of dbs: 0.19818101116770573\n",
      "sum of Ws for layer 0: 86.42321018551371 and sum of bs: 0.0015994786602066728\n",
      "sum of abs dWs for layer 1: 0.04640000329565145 and sum of dbs: 0.13720745121289474\n",
      "sum of Ws for layer 1: 321.3249727025649 and sum of bs: 0.001688683525040717\n",
      "sum of abs dWs for layer 2: 0.02390683229354394 and sum of dbs: 0.250209885987523\n",
      "sum of Ws for layer 2: 340.862685804708 and sum of bs: 0.00788327347363504\n",
      "sum of abs dWs for layer 3: 0.012802115414231839 and sum of dbs: 0.09022187384110505\n",
      "sum of Ws for layer 3: 7.500588143985957 and sum of bs: 0.002385257608209004\n",
      "MSE on the training set: 3.2087488674790977e-06\n",
      "Epoch: 1870\n",
      "sum of abs dWs for layer 0: 0.0015499109509779082 and sum of dbs: 0.19818376163319423\n",
      "sum of Ws for layer 0: 86.42321018552444 and sum of bs: 0.0015994771504989676\n",
      "sum of abs dWs for layer 1: 0.041350530137707085 and sum of dbs: 0.1372092930971166\n",
      "sum of Ws for layer 1: 321.3249727026981 and sum of bs: 0.0016886823972277099\n",
      "sum of abs dWs for layer 2: 0.02127112148376816 and sum of dbs: 0.2502133045371589\n",
      "sum of Ws for layer 2: 340.8626858047809 and sum of bs: 0.007883271115358682\n",
      "sum of abs dWs for layer 3: 0.011376918735862655 and sum of dbs: 0.09022310870418333\n",
      "sum of Ws for layer 3: 7.500588144064306 and sum of bs: 0.002385256705983338\n",
      "MSE on the training set: 3.208745705210139e-06\n",
      "Epoch: 1880\n",
      "sum of abs dWs for layer 0: 0.001383931785981951 and sum of dbs: 0.19818616630614236\n",
      "sum of Ws for layer 0: 86.423210185534 and sum of bs: 0.001599475640771972\n",
      "sum of abs dWs for layer 1: 0.036914140172436626 and sum of dbs: 0.1372109031461302\n",
      "sum of Ws for layer 1: 321.324972702817 and sum of bs: 0.0016886812694001593\n",
      "sum of abs dWs for layer 2: 0.018955426304384168 and sum of dbs: 0.25021629306698895\n",
      "sum of Ws for layer 2: 340.8626858048457 and sum of bs: 0.007883268757051812\n",
      "sum of abs dWs for layer 3: 0.01012476277344577 and sum of dbs: 0.09022418824330361\n",
      "sum of Ws for layer 3: 7.500588144133551 and sum of bs: 0.0023852558037461926\n",
      "MSE on the training set: 3.2087425437066636e-06\n",
      "Epoch: 1890\n",
      "sum of abs dWs for layer 0: 0.0012381045733683605 and sum of dbs: 0.1981882671769723\n",
      "sum of Ws for layer 0: 86.42321018554256 and sum of bs: 0.0015994741310281192\n",
      "sum of abs dWs for layer 1: 0.033016383617172 and sum of dbs: 0.13721230951224417\n",
      "sum of Ws for layer 1: 321.32497270292316 and sum of bs: 0.001688680141559899\n",
      "sum of abs dWs for layer 2: 0.01692088564417601 and sum of dbs: 0.25021890379535555\n",
      "sum of Ws for layer 2: 340.86268580490355 and sum of bs: 0.007883266398718279\n",
      "sum of abs dWs for layer 3: 0.009024634320534415 and sum of dbs: 0.09022513131982882\n",
      "sum of Ws for layer 3: 7.500588144194796 and sum of bs: 0.002385254901499017\n",
      "MSE on the training set: 3.208739382786882e-06\n",
      "Epoch: 1900\n",
      "sum of abs dWs for layer 0: 0.001109983202365657 and sum of dbs: 0.19819010112348948\n",
      "sum of Ws for layer 0: 86.42321018555025 and sum of bs: 0.001599472621269545\n",
      "sum of abs dWs for layer 1: 0.029591879363342635 and sum of dbs: 0.13721353692003393\n",
      "sum of Ws for layer 1: 321.32497270301826 and sum of bs: 0.0016886790137085389\n",
      "sum of abs dWs for layer 2: 0.015133372035718616 and sum of dbs: 0.25022118258266607\n",
      "sum of Ws for layer 2: 340.8626858049551 and sum of bs: 0.007883264040361456\n",
      "sum of abs dWs for layer 3: 0.008058079774124299 and sum of dbs: 0.09022595449862386\n",
      "sum of Ws for layer 3: 7.500588144249013 and sum of bs: 0.002385253999243084\n",
      "MSE on the training set: 3.208736222306238e-06\n",
      "Epoch: 1910\n",
      "sum of abs dWs for layer 0: 0.0009974173984173768 and sum of dbs: 0.19819170056033075\n",
      "sum of Ws for layer 0: 86.42321018555711 and sum of bs: 0.0015994711114981277\n",
      "sum of abs dWs for layer 1: 0.026583153589296082 and sum of dbs: 0.1372146071017617\n",
      "sum of Ws for layer 1: 321.32497270310347 and sum of bs: 0.0016886778858474943\n",
      "sum of abs dWs for layer 2: 0.013562885437441455 and sum of dbs: 0.2502231697390322\n",
      "sum of Ws for layer 2: 340.8626858050013 and sum of bs: 0.00788326168198432\n",
      "sum of abs dWs for layer 3: 0.007208877336965213 and sum of dbs: 0.09022667233977648\n",
      "sum of Ws for layer 3: 7.500588144297054 and sum of bs: 0.0023852530969795094\n",
      "MSE on the training set: 3.208733062199216e-06\n",
      "Epoch: 1920\n",
      "sum of abs dWs for layer 0: 0.0008985191410588431 and sum of dbs: 0.1981930939526841\n",
      "sum of Ws for layer 0: 86.4232101855633 and sum of bs: 0.0015994696017155167\n",
      "sum of abs dWs for layer 1: 0.023939742157257464 and sum of dbs: 0.13721553914179732\n",
      "sum of Ws for layer 1: 321.3249727031801 and sum of bs: 0.0016886767579780085\n",
      "sum of abs dWs for layer 2: 0.012183084705005795 and sum of dbs: 0.25022490066311986\n",
      "sum of Ws for layer 2: 340.8626858050427 and sum of bs: 0.007883259323589471\n",
      "sum of abs dWs for layer 3: 0.006462783672033503 and sum of dbs: 0.09022729762935096\n",
      "sum of Ws for layer 3: 7.5005881443396705 and sum of bs: 0.0023852521947092756\n",
      "MSE on the training set: 3.208729902372399e-06\n",
      "Epoch: 1930\n",
      "sum of abs dWs for layer 0: 0.0008116281677036253 and sum of dbs: 0.1981943063277333\n",
      "sum of Ws for layer 0: 86.42321018556889 and sum of bs: 0.001599468091923161\n",
      "sum of abs dWs for layer 1: 0.021617268577978736 and sum of dbs: 0.13721634981951508\n",
      "sum of Ws for layer 1: 321.32497270324916 and sum of bs: 0.0016886756301011737\n",
      "sum of abs dWs for layer 2: 0.010970806309643446 and sum of dbs: 0.2502264064781705\n",
      "sum of Ws for layer 2: 340.8626858050799 and sum of bs: 0.007883256965179204\n",
      "sum of abs dWs for layer 3: 0.00580727366111985 and sum of dbs: 0.09022784160912037\n",
      "sum of Ws for layer 3: 7.500588144377519 and sum of bs: 0.0023852512924332444\n",
      "MSE on the training set: 3.208726742719325e-06\n",
      "Epoch: 1940\n",
      "sum of abs dWs for layer 0: 0.0007352868834991509 and sum of dbs: 0.19819535966088778\n",
      "sum of Ws for layer 0: 86.42321018557394 and sum of bs: 0.001599466582122335\n",
      "sum of abs dWs for layer 1: 0.019576773386334286 and sum of dbs: 0.13721705386823924\n",
      "sum of Ws for layer 1: 321.3249727033115 and sum of bs: 0.0016886745022179496\n",
      "sum of abs dWs for layer 2: 0.009905714287238541 and sum of dbs: 0.25022771451230796\n",
      "sum of Ws for layer 2: 340.8626858051134 and sum of bs: 0.007883254606755529\n",
      "sum of abs dWs for layer 3: 0.005231351123226571 and sum of dbs: 0.09022831415005408\n",
      "sum of Ws for layer 3: 7.500588144411181 and sum of bs: 0.0023852503901521742\n",
      "MSE on the training set: 3.2087235832445214e-06\n",
      "Epoch: 1950\n",
      "sum of abs dWs for layer 0: 0.0006682147068387669 and sum of dbs: 0.1981962732572752\n",
      "sum of Ws for layer 0: 86.42321018557853 and sum of bs: 0.0015994650723141562\n",
      "sum of abs dWs for layer 1: 0.01778402843288649 and sum of dbs: 0.1372176642310145\n",
      "sum of Ws for layer 1: 321.3249727033681 and sum of bs: 0.0016886733743291791\n",
      "sum of abs dWs for layer 2: 0.008969942314043456 and sum of dbs: 0.25022884877295354\n",
      "sum of Ws for layer 2: 340.86268580514366 and sum of bs: 0.007883252248320219\n",
      "sum of abs dWs for layer 3: 0.004725355275703227 and sum of dbs: 0.09022872392367759\n",
      "sum of Ws for layer 3: 7.5005881444411635 and sum of bs: 0.0023852494878667304\n",
      "MSE on the training set: 3.2087204238956488e-06\n",
      "Epoch: 1960\n",
      "sum of abs dWs for layer 0: 0.0006092861368837275 and sum of dbs: 0.198197064083964\n",
      "sum of Ws for layer 0: 86.42321018558272 and sum of bs: 0.0015994635624996085\n",
      "sum of abs dWs for layer 1: 0.01620895065951788 and sum of dbs: 0.13721819228334325\n",
      "sum of Ws for layer 1: 321.3249727034196 and sum of bs: 0.0016886722464356033\n",
      "sum of abs dWs for layer 2: 0.008147787710661974 and sum of dbs: 0.25022983035997115\n",
      "sum of Ws for layer 2: 340.86268580517105 and sum of bs: 0.007883249889874825\n",
      "sum of abs dWs for layer 3: 0.004280795274265713 and sum of dbs: 0.09022907855130116\n",
      "sum of Ws for layer 3: 7.500588144467913 and sum of bs: 0.002385248585577498\n",
      "MSE on the training set: 3.2087172646577853e-06\n",
      "Epoch: 1970\n",
      "sum of abs dWs for layer 0: 0.0005575122333344566 and sum of dbs: 0.19819774705082452\n",
      "sum of Ws for layer 0: 86.42321018558653 and sum of bs: 0.0015994620526795544\n",
      "sum of abs dWs for layer 1: 0.014825107079700124 and sum of dbs: 0.13721864802148775\n",
      "sum of Ws for layer 1: 321.3249727034665 and sum of bs: 0.0016886711185378732\n",
      "sum of abs dWs for layer 2: 0.007425453052950468 and sum of dbs: 0.2502306778149393\n",
      "sum of Ws for layer 2: 340.862685805196 and sum of bs: 0.007883247531420712\n",
      "sum of abs dWs for layer 3: 0.003890210495352034 and sum of dbs: 0.09022938473017766\n",
      "sum of Ws for layer 3: 7.500588144491822 and sum of bs: 0.002385247683284991\n",
      "MSE on the training set: 3.2087141054654185e-06\n",
      "Epoch: 1980\n",
      "sum of abs dWs for layer 0: 0.0005120243294436467 and sum of dbs: 0.19819833525193717\n",
      "sum of Ws for layer 0: 86.42321018559002 and sum of bs: 0.0015994605428547534\n",
      "sum of abs dWs for layer 1: 0.013609279450195483 and sum of dbs: 0.1372190402243212\n",
      "sum of Ws for layer 1: 321.3249727035095 and sum of bs: 0.0016886699906365603\n",
      "sum of abs dWs for layer 2: 0.006790818940431391 and sum of dbs: 0.25023140742136113\n",
      "sum of Ws for layer 2: 340.8626858052187 and sum of bs: 0.007883245172959082\n",
      "sum of abs dWs for layer 3: 0.0035470476661360923 and sum of dbs: 0.09022964834193888\n",
      "sum of Ws for layer 3: 7.500588144513236 and sum of bs: 0.002385246780989661\n",
      "MSE on the training set: 3.208710946363378e-06\n",
      "Epoch: 1990\n",
      "sum of abs dWs for layer 0: 0.0004720596561689932 and sum of dbs: 0.19819884018805609\n",
      "sum of Ws for layer 0: 86.42321018559323 and sum of bs: 0.001599459033025872\n",
      "sum of abs dWs for layer 1: 0.012541080024820477 and sum of dbs: 0.1372193766024776\n",
      "sum of Ws for layer 1: 321.324972703549 and sum of bs: 0.0016886688627321671\n",
      "sum of abs dWs for layer 2: 0.006233243428373501 and sum of dbs: 0.2502320334813152\n",
      "sum of Ws for layer 2: 340.8626858052395 and sum of bs: 0.007883242814490985\n",
      "sum of abs dWs for layer 3: 0.0032455524122669516 and sum of dbs: 0.09022987455252228\n",
      "sum of Ws for layer 3: 7.5005881445324585 and sum of bs: 0.0023852458786919046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the training set: 3.208707787297495e-06\n",
      "Epoch: 2000\n",
      "sum of abs dWs for layer 0: 0.00043694699514667916 and sum of dbs: 0.19819927197732046\n",
      "sum of Ws for layer 0: 86.4232101855962 and sum of bs: 0.0015994575231934956\n",
      "sum of abs dWs for layer 1: 0.01160256807857407 and sum of dbs: 0.13721966393962243\n",
      "sum of Ws for layer 1: 321.32497270358544 and sum of bs: 0.0016886677348251348\n",
      "sum of abs dWs for layer 2: 0.0057433618620423696 and sum of dbs: 0.2502325685774911\n",
      "sum of Ws for layer 2: 340.86268580525854 and sum of bs: 0.007883240456017349\n",
      "sum of abs dWs for layer 3: 0.0029806610230830454 and sum of dbs: 0.09023006790681894\n",
      "sum of Ws for layer 3: 7.500588144549753 and sum of bs: 0.002385244976392071\n",
      "MSE on the training set: 3.2087046282664797e-06\n",
      "Epoch: 2010\n",
      "sum of abs dWs for layer 0: 0.0004060975373887902 and sum of dbs: 0.19819963949655095\n",
      "sum of Ws for layer 0: 86.42321018559898 and sum of bs: 0.0015994560133581388\n",
      "sum of abs dWs for layer 1: 0.010778005573425271 and sum of dbs: 0.1372199081871843\n",
      "sum of Ws for layer 1: 321.3249727036192 and sum of bs: 0.001688666606915852\n",
      "sum of abs dWs for layer 2: 0.005312959339753256 and sum of dbs: 0.25023302374890316\n",
      "sum of Ws for layer 2: 340.86268580527616 and sum of bs: 0.007883238097538988\n",
      "sum of abs dWs for layer 3: 0.0027479314890892785 and sum of dbs: 0.09023023239214138\n",
      "sum of Ws for layer 3: 7.500588144565355 and sum of bs: 0.0023852440740904646\n",
      "MSE on the training set: 3.208701469269113e-06\n",
      "Epoch: 2020\n",
      "sum of abs dWs for layer 0: 0.00037899368076499575 and sum of dbs: 0.19819995054887832\n",
      "sum of Ws for layer 0: 86.42321018560152 and sum of bs: 0.0015994545035202545\n",
      "sum of abs dWs for layer 1: 0.010053557730724399 and sum of dbs: 0.1372201145767405\n",
      "sum of Ws for layer 1: 321.32497270365064 and sum of bs: 0.0016886654790046582\n",
      "sum of abs dWs for layer 2: 0.004934814418364536 and sum of dbs: 0.2502334086993486\n",
      "sum of Ws for layer 2: 340.86268580529236 and sum of bs: 0.007883235739056615\n",
      "sum of abs dWs for layer 3: 0.002543458989479712 and sum of dbs: 0.0902303715135194\n",
      "sum of Ws for layer 3: 7.5005881445794715 and sum of bs: 0.0023852431717873556\n",
      "MSE on the training set: 3.208698310274443e-06\n",
      "Epoch: 2030\n",
      "sum of abs dWs for layer 0: 0.0003551807963758608 and sum of dbs: 0.19820021198781346\n",
      "sum of Ws for layer 0: 86.42321018560393 and sum of bs: 0.0015994529936802387\n",
      "sum of abs dWs for layer 1: 0.009417072957938496 and sum of dbs: 0.1372202877031997\n",
      "sum of Ws for layer 1: 321.32497270368 and sum of bs: 0.0016886643510918536\n",
      "sum of abs dWs for layer 2: 0.0046025842400320965 and sum of dbs: 0.2502337319516991\n",
      "sum of Ws for layer 2: 340.8626858053076 and sum of bs: 0.007883233380570861\n",
      "sum of abs dWs for layer 3: 0.002363813777211196 and sum of dbs: 0.0902304883494301\n",
      "sum of Ws for layer 3: 7.500588144592281 and sum of bs: 0.0023852422694829803\n",
      "MSE on the training set: 3.208695151325898e-06\n",
      "Epoch: 2040\n",
      "sum of abs dWs for layer 0: 0.00033425891679347906 and sum of dbs: 0.19820042984372666\n",
      "sum of Ws for layer 0: 86.4232101856062 and sum of bs: 0.0015994514838384409\n",
      "sum of abs dWs for layer 1: 0.008857860686952239 and sum of dbs: 0.13722043160959937\n",
      "sum of Ws for layer 1: 321.3249727037075 and sum of bs: 0.001688663223177701\n",
      "sum of abs dWs for layer 2: 0.004310688568758775 and sum of dbs: 0.2502340010051872\n",
      "sum of Ws for layer 2: 340.8626858053217 and sum of bs: 0.007883231022082274\n",
      "sum of abs dWs for layer 3: 0.002205978474568364 and sum of dbs: 0.09023058560861033\n",
      "sum of Ws for layer 3: 7.5005881446039435 and sum of bs: 0.002385241367177547\n",
      "MSE on the training set: 3.2086919923970302e-06\n",
      "Epoch: 2050\n",
      "sum of abs dWs for layer 0: 0.00031587734659382655 and sum of dbs: 0.19820060940268058\n",
      "sum of Ws for layer 0: 86.42321018560833 and sum of bs: 0.0015994499739951677\n",
      "sum of abs dWs for layer 1: 0.008366547321176606 and sum of dbs: 0.13722054983995877\n",
      "sum of Ws for layer 1: 321.32497270373335 and sum of bs: 0.0016886620952624313\n",
      "sum of abs dWs for layer 2: 0.004054234598068692 and sum of dbs: 0.25023422243344134\n",
      "sum of Ws for layer 2: 340.8626858053349 and sum of bs: 0.00788322866359134\n",
      "sum of abs dWs for layer 3: 0.002067307414729626 and sum of dbs: 0.09023066566546699\n",
      "sum of Ws for layer 3: 7.500588144614597 and sum of bs: 0.0023852404648712373\n",
      "MSE on the training set: 3.2086888334526073e-06\n",
      "Epoch: 2060\n",
      "sum of abs dWs for layer 0: 0.00029972756584483015 and sum of dbs: 0.19820075531639725\n",
      "sum of Ws for layer 0: 86.42321018561034 and sum of bs: 0.0015994484641506886\n",
      "sum of abs dWs for layer 1: 0.007934886555752892 and sum of dbs: 0.13722064551300606\n",
      "sum of Ws for layer 1: 321.3249727037579 and sum of bs: 0.0016886609673462476\n",
      "sum of abs dWs for layer 2: 0.00382891794239519 and sum of dbs: 0.25023440202123787\n",
      "sum of Ws for layer 2: 340.8626858053474 and sum of bs: 0.007883226305098488\n",
      "sum of abs dWs for layer 3: 0.00194547310525658 and sum of dbs: 0.09023073060947212\n",
      "sum of Ws for layer 3: 7.500588144624365 and sum of bs: 0.002385239562564212\n",
      "MSE on the training set: 3.208685674539258e-06\n",
      "Epoch: 2070\n",
      "sum of abs dWs for layer 0: 0.00028553843396493485 and sum of dbs: 0.19820087167232026\n",
      "sum of Ws for layer 0: 86.42321018561226 and sum of bs: 0.00159944695430524\n",
      "sum of abs dWs for layer 1: 0.007555631183384944 and sum of dbs: 0.13722072136915123\n",
      "sum of Ws for layer 1: 321.3249727037812 and sum of bs: 0.0016886598394293285\n",
      "sum of abs dWs for layer 2: 0.0036309557226053444 and sum of dbs: 0.2502345448516297\n",
      "sum of Ws for layer 2: 340.8626858053591 and sum of bs: 0.007883223946604088\n",
      "sum of abs dWs for layer 3: 0.001838430045707161 and sum of dbs: 0.09023078227663385\n",
      "sum of Ws for layer 3: 7.500588144633354 and sum of bs: 0.0023852386602566107\n",
      "MSE on the training set: 3.2086825156229886e-06\n",
      "Epoch: 2080\n",
      "sum of abs dWs for layer 0: 0.0002730720301694201 and sum of dbs: 0.19820096205704224\n",
      "sum of Ws for layer 0: 86.4232101856141 and sum of bs: 0.0015994454444590298\n",
      "sum of abs dWs for layer 1: 0.007222421915369134 and sum of dbs: 0.13722077981301087\n",
      "sum of Ws for layer 1: 321.3249727038034 and sum of bs: 0.0016886587115118304\n",
      "sum of abs dWs for layer 2: 0.003457028533106799 and sum of dbs: 0.25023465538482237\n",
      "sum of Ws for layer 2: 340.8626858053703 and sum of bs: 0.00788322158810847\n",
      "sum of abs dWs for layer 3: 0.001744383347753422 and sum of dbs: 0.0902308222779869\n",
      "sum of Ws for layer 3: 7.500588144641659 and sum of bs: 0.0023852377579485586\n",
      "MSE on the training set: 3.2086793567140542e-06\n",
      "Epoch: 2090\n",
      "sum of abs dWs for layer 0: 0.0002621190234000297 and sum of dbs: 0.19820102962659708\n",
      "sum of Ws for layer 0: 86.42321018561584 and sum of bs: 0.0015994439346122412\n",
      "sum of abs dWs for layer 1: 0.006929663626249418 and sum of dbs: 0.13722082296053512\n",
      "sum of Ws for layer 1: 321.32497270382464 and sum of bs: 0.0016886575835938914\n",
      "sum of abs dWs for layer 2: 0.0033042158443159723 and sum of dbs: 0.25023473754558867\n",
      "sum of Ws for layer 2: 340.8626858053809 and sum of bs: 0.007883219229611924\n",
      "sum of abs dWs for layer 3: 0.0016617538056214586 and sum of dbs: 0.09023085203116642\n",
      "sum of Ws for layer 3: 7.500588144649364 and sum of bs: 0.0023852368556401644\n",
      "MSE on the training set: 3.20867619781076e-06\n",
      "Epoch: 2100\n",
      "sum of abs dWs for layer 0: 0.0002524960893575119 and sum of dbs: 0.1982010771455473\n",
      "sum of Ws for layer 0: 86.42321018561753 and sum of bs: 0.0015994424247650337\n",
      "sum of abs dWs for layer 1: 0.006672456314744373 and sum of dbs: 0.13722085266521367\n",
      "sum of Ws for layer 1: 321.324972703845 and sum of bs: 0.001688656455675632\n",
      "sum of abs dWs for layer 2: 0.0031699599658314044 and sum of dbs: 0.25023479477187605\n",
      "sum of Ws for layer 2: 340.8626858053911 and sum of bs: 0.0078832168711147\n",
      "sum of abs dWs for layer 3: 0.001589158410061974 and sum of dbs: 0.0902308727779655\n",
      "sum of Ws for layer 3: 7.5005881446565414 and sum of bs: 0.002385235953331523\n",
      "MSE on the training set: 3.208673038914928e-06\n",
      "Epoch: 2110\n",
      "sum of abs dWs for layer 0: 0.0002440415774448287 and sum of dbs: 0.19820110704895338\n",
      "sum of Ws for layer 0: 86.42321018561918 and sum of bs: 0.0015994409149175496\n",
      "sum of abs dWs for layer 1: 0.006446479287230382 and sum of dbs: 0.137220870559623\n",
      "sum of Ws for layer 1: 321.32497270386466 and sum of bs: 0.001688655327757159\n",
      "sum of abs dWs for layer 2: 0.0030520055928024295 and sum of dbs: 0.25023483009187103\n",
      "sum of Ws for layer 2: 340.8626858054008 and sum of bs: 0.007883214512617026\n",
      "sum of abs dWs for layer 3: 0.0015253776594958658 and sum of dbs: 0.09023088561217076\n",
      "sum of Ws for layer 3: 7.500588144663253 and sum of bs: 0.002385235051022718\n",
      "MSE on the training set: 3.2086698800209767e-06\n",
      "Epoch: 2120\n",
      "sum of abs dWs for layer 0: 0.0002366134414180997 and sum of dbs: 0.19820112147922042\n",
      "sum of Ws for layer 0: 86.42321018562075 and sum of bs: 0.0015994394050699117\n",
      "sum of abs dWs for layer 1: 0.006247935846918011 and sum of dbs: 0.13722087808013\n",
      "sum of Ws for layer 1: 321.32497270388365 and sum of bs: 0.001688654199838565\n",
      "sum of abs dWs for layer 2: 0.002948370934960893 and sum of dbs: 0.25023484616981995\n",
      "sum of Ws for layer 2: 340.8626858054102 and sum of bs: 0.007883212154119093\n",
      "sum of abs dWs for layer 3: 0.0014693399487441762 and sum of dbs: 0.090230891496113\n",
      "sum of Ws for layer 3: 7.500588144669559 and sum of bs: 0.002385234148713824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the training set: 3.2086667211515488e-06\n",
      "Epoch: 2130\n",
      "sum of abs dWs for layer 0: 0.00023008722096482008 and sum of dbs: 0.19820112231305367\n",
      "sum of Ws for layer 0: 86.42321018562228 and sum of bs: 0.0015994378952222292\n",
      "sum of abs dWs for layer 1: 0.006073499344310512 and sum of dbs: 0.13722087648496403\n",
      "sum of Ws for layer 1: 321.3249727039021 and sum of bs: 0.0016886530719199323\n",
      "sum of abs dWs for layer 2: 0.002857319556110804 and sum of dbs: 0.2502348453395514\n",
      "sum of Ws for layer 2: 340.8626858054193 and sum of bs: 0.00788320979562108\n",
      "sum of abs dWs for layer 3: 0.0014201063419136371 and sum of dbs: 0.09023089127277542\n",
      "sum of Ws for layer 3: 7.500588144675508 and sum of bs: 0.0023852332464049057\n",
      "MSE on the training set: 3.2086635622672254e-06\n",
      "Epoch: 2140\n",
      "sum of abs dWs for layer 0: 0.0002243533569746044 and sum of dbs: 0.19820111120094483\n",
      "sum of Ws for layer 0: 86.42321018562374 and sum of bs: 0.0015994363853745982\n",
      "sum of abs dWs for layer 1: 0.005920241418121522 and sum of dbs: 0.1372208668806906\n",
      "sum of Ws for layer 1: 321.32497270392014 and sum of bs: 0.0016886519440013335\n",
      "sum of abs dWs for layer 2: 0.0027773229175729348 and sum of dbs: 0.25023482965357946\n",
      "sum of Ws for layer 2: 340.8626858054281 and sum of bs: 0.007883207437123129\n",
      "sum of abs dWs for layer 3: 0.0013768503186621986 and sum of dbs: 0.09023088568353\n",
      "sum of Ws for layer 3: 7.50058814468114 and sum of bs: 0.0023852323440960196\n",
      "MSE on the training set: 3.2086604033917577e-06\n",
      "Epoch: 2150\n",
      "sum of abs dWs for layer 0: 0.00021931595429149767 and sum of dbs: 0.19820108959009214\n",
      "sum of Ws for layer 0: 86.42321018562522 and sum of bs: 0.0015994348755271023\n",
      "sum of abs dWs for layer 1: 0.005785598925382092 and sum of dbs: 0.13722085023757788\n",
      "sum of Ws for layer 1: 321.3249727039376 and sum of bs: 0.0016886508160828318\n",
      "sum of abs dWs for layer 2: 0.0027070431163842253 and sum of dbs: 0.25023480091160744\n",
      "sum of Ws for layer 2: 340.8626858054366 and sum of bs: 0.00788320507862538\n",
      "sum of abs dWs for layer 3: 0.0013388484403097393 and sum of dbs: 0.09023087537843283\n",
      "sum of Ws for layer 3: 7.500588144686496 and sum of bs: 0.002385231441787216\n",
      "MSE on the training set: 3.2086572445184886e-06\n",
      "Epoch: 2160\n",
      "sum of abs dWs for layer 0: 0.00021489006561992635 and sum of dbs: 0.1982010587604697\n",
      "sum of Ws for layer 0: 86.42321018562664 and sum of bs: 0.0015994333656798154\n",
      "sum of abs dWs for layer 1: 0.005667301344091323 and sum of dbs: 0.13722082741377936\n",
      "sum of Ws for layer 1: 321.3249727039547 and sum of bs: 0.0016886496881644824\n",
      "sum of abs dWs for layer 2: 0.0026452949911597488 and sum of dbs: 0.25023476070538203\n",
      "sum of Ws for layer 2: 340.86268580544504 and sum of bs: 0.007883202720127947\n",
      "sum of abs dWs for layer 3: 0.0013057023309315718 and sum of dbs: 0.09023086093242594\n",
      "sum of Ws for layer 3: 7.50058814469161 and sum of bs: 0.0023852305394785386\n",
      "MSE on the training set: 3.2086540856464527e-06\n",
      "Epoch: 2170\n",
      "sum of abs dWs for layer 0: 0.0002110015047988974 and sum of dbs: 0.1982010198302107\n",
      "sum of Ws for layer 0: 86.42321018562804 and sum of bs: 0.001599431855832802\n",
      "sum of abs dWs for layer 1: 0.0055633657822585095 and sum of dbs: 0.13722079915894297\n",
      "sum of Ws for layer 1: 321.3249727039715 and sum of bs: 0.0016886485602463344\n",
      "sum of abs dWs for layer 2: 0.0025910435168962965 and sum of dbs: 0.25023471042538864\n",
      "sum of Ws for layer 2: 340.86268580545317 and sum of bs: 0.007883200361630933\n",
      "sum of abs dWs for layer 3: 0.001276627180970567 and sum of dbs: 0.09023084284775516\n",
      "sum of Ws for layer 3: 7.50058814469651 and sum of bs: 0.002385229637170026\n",
      "MSE on the training set: 3.208650926793395e-06\n",
      "Epoch: 2180\n",
      "sum of abs dWs for layer 0: 0.00020758459462867708 and sum of dbs: 0.19820097378946142\n",
      "sum of Ws for layer 0: 86.42321018562943 and sum of bs: 0.00159943034598612\n",
      "sum of abs dWs for layer 1: 0.005472036780493929 and sum of dbs: 0.1372207661369081\n",
      "sum of Ws for layer 1: 321.32497270398795 and sum of bs: 0.0016886474323284306\n",
      "sum of abs dWs for layer 2: 0.0025433723833069785 and sum of dbs: 0.2502346513029508\n",
      "sum of Ws for layer 2: 340.8626858054612 and sum of bs: 0.007883198003134428\n",
      "sum of abs dWs for layer 3: 0.0012510786784412288 and sum of dbs: 0.09023082156917686\n",
      "sum of Ws for layer 3: 7.500588144701222 and sum of bs: 0.002385228734861712\n",
      "MSE on the training set: 3.2086477679293318e-06\n",
      "Epoch: 2190\n",
      "sum of abs dWs for layer 0: 0.00020458292339135104 and sum of dbs: 0.19820092148874313\n",
      "sum of Ws for layer 0: 86.42321018563077 and sum of bs: 0.0015994288361398186\n",
      "sum of abs dWs for layer 1: 0.0053918065327389205 and sum of dbs: 0.13722072891790352\n",
      "sum of Ws for layer 1: 321.3249727040042 and sum of bs: 0.0016886463044108093\n",
      "sum of abs dWs for layer 2: 0.0025014945495499866 and sum of dbs: 0.2502345843957577\n",
      "sum of Ws for layer 2: 340.862685805469 and sum of bs: 0.00788319564463851\n",
      "sum of abs dWs for layer 3: 0.001228635019108935 and sum of dbs: 0.09023079747873036\n",
      "sum of Ws for layer 3: 7.500588144705771 and sum of bs: 0.002385227832553626\n",
      "MSE on the training set: 3.2086446090647015e-06\n",
      "Epoch: 2200\n",
      "sum of abs dWs for layer 0: 0.00020194549314978027 and sum of dbs: 0.19820086369713646\n",
      "sum of Ws for layer 0: 86.42321018563212 and sum of bs: 0.001599427326293942\n",
      "sum of abs dWs for layer 1: 0.005321311935702236 and sum of dbs: 0.13722068801755605\n",
      "sum of Ws for layer 1: 321.3249727040202 and sum of bs: 0.0016886451764935033\n",
      "sum of abs dWs for layer 2: 0.00246469850638023 and sum of dbs: 0.2502345106602204\n",
      "sum of Ws for layer 2: 340.86268580547676 and sum of bs: 0.00788319328614325\n",
      "sum of abs dWs for layer 3: 0.0012089148789061303 and sum of dbs: 0.09023077092187318\n",
      "sum of Ws for layer 3: 7.500588144710175 and sum of bs: 0.002385226930245795\n",
      "MSE on the training set: 3.2086414502100132e-06\n",
      "Epoch: 2210\n",
      "sum of abs dWs for layer 0: 0.00019962856317735697 and sum of dbs: 0.19820080107375113\n",
      "sum of Ws for layer 0: 86.42321018563342 and sum of bs: 0.0015994258164485288\n",
      "sum of abs dWs for layer 1: 0.005259383861109731 and sum of dbs: 0.13722064387776317\n",
      "sum of Ws for layer 1: 321.324972704036 and sum of bs: 0.0016886440485765414\n",
      "sum of abs dWs for layer 2: 0.0024323739951565538 and sum of dbs: 0.2502344309159931\n",
      "sum of Ws for layer 2: 340.8626858054844 and sum of bs: 0.00788319092764871\n",
      "sum of abs dWs for layer 3: 0.0011915911977083196 and sum of dbs: 0.09023074219466587\n",
      "sum of Ws for layer 3: 7.5005881447144525 and sum of bs: 0.0023852260279382427\n",
      "MSE on the training set: 3.2086382913699215e-06\n",
      "Epoch: 2220\n",
      "sum of abs dWs for layer 0: 0.0001975928530474948 and sum of dbs: 0.1982007342108041\n",
      "sum of Ws for layer 0: 86.42321018563472 and sum of bs: 0.0015994243066036133\n",
      "sum of abs dWs for layer 1: 0.005204972398311436 and sum of dbs: 0.1372205968955744\n",
      "sum of Ws for layer 1: 321.3249727040516 and sum of bs: 0.0016886429206599495\n",
      "sum of abs dWs for layer 2: 0.0024039729861437638 and sum of dbs: 0.2502343458995434\n",
      "sum of Ws for layer 2: 340.8626858054919 and sum of bs: 0.007883188569154941\n",
      "sum of abs dWs for layer 3: 0.0011763702661474198 and sum of dbs: 0.0902307115631219\n",
      "sum of Ws for layer 3: 7.500588144718617 and sum of bs: 0.0023852251256309886\n",
      "MSE on the training set: 3.208635132516114e-06\n",
      "Epoch: 2230\n",
      "sum of abs dWs for layer 0: 0.00019580432324280435 and sum of dbs: 0.19820066362225483\n",
      "sum of Ws for layer 0: 86.42321018563601 and sum of bs: 0.001599422796759225\n",
      "sum of abs dWs for layer 1: 0.00515716771875623 and sum of dbs: 0.13722054741557177\n",
      "sum of Ws for layer 1: 321.32497270406714 and sum of bs: 0.0016886417927437503\n",
      "sum of abs dWs for layer 2: 0.0023790205692981 and sum of dbs: 0.2502342562500196\n",
      "sum of Ws for layer 2: 340.86268580549927 and sum of bs: 0.007883186210661992\n",
      "sum of abs dWs for layer 3: 0.0011629975624090727 and sum of dbs: 0.09023067925810291\n",
      "sum of Ws for layer 3: 7.500588144722684 and sum of bs: 0.002385224223324049\n",
      "MSE on the training set: 3.2086319736574123e-06\n",
      "Epoch: 2240\n",
      "sum of abs dWs for layer 0: 0.00019423247046351588 and sum of dbs: 0.19820058976635951\n",
      "sum of Ws for layer 0: 86.42321018563729 and sum of bs: 0.0015994212869153903\n",
      "sum of abs dWs for layer 1: 0.005115154512042766 and sum of dbs: 0.1372204957449912\n",
      "sum of Ws for layer 1: 321.3249727040825 and sum of bs: 0.001688640664827963\n",
      "sum of abs dWs for layer 2: 0.0023570911709605803 and sum of dbs: 0.25023416253729897\n",
      "sum of Ws for layer 2: 340.86268580550666 and sum of bs: 0.007883183852169903\n",
      "sum of abs dWs for layer 3: 0.0011512450058645592 and sum of dbs: 0.09023064548544957\n",
      "sum of Ws for layer 3: 7.500588144726664 and sum of bs: 0.002385223321017441\n",
      "MSE on the training set: 3.20862881481908e-06\n",
      "Epoch: 2250\n",
      "sum of abs dWs for layer 0: 0.0001928518175627611 and sum of dbs: 0.19820051302819885\n",
      "sum of Ws for layer 0: 86.42321018563855 and sum of bs: 0.0015994197770721323\n",
      "sum of abs dWs for layer 1: 0.0050782518097910725 and sum of dbs: 0.13722044214200857\n",
      "sum of Ws for layer 1: 321.32497270409766 and sum of bs: 0.0016886395369126055\n",
      "sum of abs dWs for layer 2: 0.002337829340947834 and sum of dbs: 0.25023406524025893\n",
      "sum of Ws for layer 2: 340.86268580551393 and sum of bs: 0.00788318149367871\n",
      "sum of abs dWs for layer 3: 0.0011409220976342043 and sum of dbs: 0.09023061041813334\n",
      "sum of Ws for layer 3: 7.500588144730569 and sum of bs: 0.0023852224187111784\n",
      "MSE on the training set: 3.2086256559768307e-06\n",
      "Epoch: 2260\n",
      "sum of abs dWs for layer 0: 0.00019163857235722102 and sum of dbs: 0.19820043376672788\n",
      "sum of Ws for layer 0: 86.42321018563982 and sum of bs: 0.0015994182672294706\n",
      "sum of abs dWs for layer 1: 0.005045823680320421 and sum of dbs: 0.1372203868472839\n",
      "sum of Ws for layer 1: 321.3249727041127 and sum of bs: 0.0016886384089976926\n",
      "sum of abs dWs for layer 2: 0.0023209031372277544 and sum of dbs: 0.2502339648052881\n",
      "sum of Ws for layer 2: 340.86268580552115 and sum of bs: 0.00788317913518845\n",
      "sum of abs dWs for layer 3: 0.0011318509377341397 and sum of dbs: 0.0902305742173906\n",
      "sum of Ws for layer 3: 7.500588144734408 and sum of bs: 0.0023852215164052726\n",
      "MSE on the training set: 3.2086224971631316e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2270\n",
      "sum of abs dWs for layer 0: 0.00019057269984107327 and sum of dbs: 0.19820035228324318\n",
      "sum of Ws for layer 0: 86.42321018564107 and sum of bs: 0.001599416757387424\n",
      "sum of abs dWs for layer 1: 0.005017334616044037 and sum of dbs: 0.13722033006282008\n",
      "sum of Ws for layer 1: 321.32497270412773 and sum of bs: 0.001688637281083238\n",
      "sum of abs dWs for layer 2: 0.0023060330368955308 and sum of dbs: 0.25023386160707206\n",
      "sum of Ws for layer 2: 340.8626858055283 and sum of bs: 0.007883176776699146\n",
      "sum of abs dWs for layer 3: 0.0011238817195283416 and sum of dbs: 0.09023053701855846\n",
      "sum of Ws for layer 3: 7.500588144738187 and sum of bs: 0.002385220614099734\n",
      "MSE on the training set: 3.20861933833289e-06\n",
      "Epoch: 2280\n",
      "sum of abs dWs for layer 0: 0.00018963618163318288 and sum of dbs: 0.19820026884990533\n",
      "sum of Ws for layer 0: 86.42321018564232 and sum of bs: 0.001599415247546007\n",
      "sum of abs dWs for layer 1: 0.004992303010916936 and sum of dbs: 0.13722027197108627\n",
      "sum of Ws for layer 1: 321.3249727041426 and sum of bs: 0.0016886361531692534\n",
      "sum of abs dWs for layer 2: 0.0022929676524373523 and sum of dbs: 0.25023375598406344\n",
      "sum of Ws for layer 2: 340.8626858055354 and sum of bs: 0.007883174418210823\n",
      "sum of abs dWs for layer 3: 0.0011168797151649736 and sum of dbs: 0.09023049894388671\n",
      "sum of Ws for layer 3: 7.500588144741916 and sum of bs: 0.0023852197117945726\n",
      "MSE on the training set: 3.2086161794955655e-06\n",
      "Epoch: 2290\n",
      "sum of abs dWs for layer 0: 0.0001888134731690001 and sum of dbs: 0.19820018370024894\n",
      "sum of Ws for layer 0: 86.42321018564355 and sum of bs: 0.001599413737705234\n",
      "sum of abs dWs for layer 1: 0.0049703133805385335 and sum of dbs: 0.13722021272865462\n",
      "sum of Ws for layer 1: 321.3249727041575 and sum of bs: 0.0016886350252557488\n",
      "sum of abs dWs for layer 2: 0.0022814901103534032 and sum of dbs: 0.2502336482266808\n",
      "sum of Ws for layer 2: 340.8626858055425 and sum of bs: 0.007883172059723505\n",
      "sum of abs dWs for layer 3: 0.0011107286941325607 and sum of dbs: 0.090230460098275\n",
      "sum of Ws for layer 3: 7.500588144745599 and sum of bs: 0.0023852188094897952\n",
      "MSE on the training set: 3.2086130206763307e-06\n",
      "Epoch: 2300\n",
      "sum of abs dWs for layer 0: 0.00018809053636526483 and sum of dbs: 0.19820009704746697\n",
      "sum of Ws for layer 0: 86.4232101856448 and sum of bs: 0.0015994122278651171\n",
      "sum of abs dWs for layer 1: 0.004950990506637214 and sum of dbs: 0.1372201524784596\n",
      "sum of Ws for layer 1: 321.3249727041723 and sum of bs: 0.0016886338973427337\n",
      "sum of abs dWs for layer 2: 0.0022714045551235676 and sum of dbs: 0.25023353860004593\n",
      "sum of Ws for layer 2: 340.8626858055496 and sum of bs: 0.007883169701237211\n",
      "sum of abs dWs for layer 3: 0.0011053236902319181 and sum of dbs: 0.09023042057748587\n",
      "sum of Ws for layer 3: 7.500588144749244 and sum of bs: 0.002385217907185411\n",
      "MSE on the training set: 3.208609861850809e-06\n",
      "Epoch: 2310\n",
      "sum of abs dWs for layer 0: 0.0001874553204087521 and sum of dbs: 0.19820000907164495\n",
      "sum of Ws for layer 0: 86.42321018564603 and sum of bs: 0.0015994107180256668\n",
      "sum of abs dWs for layer 1: 0.004934012287899665 and sum of dbs: 0.13722009134123875\n",
      "sum of Ws for layer 1: 321.3249727041871 and sum of bs: 0.0016886327694302162\n",
      "sum of abs dWs for layer 2: 0.0022625428571343064 and sum of dbs: 0.2502334273281093\n",
      "sum of Ws for layer 2: 340.8626858055566 and sum of bs: 0.007883167342751954\n",
      "sum of abs dWs for layer 3: 0.0011005745965929597 and sum of dbs: 0.09023038046241061\n",
      "sum of Ws for layer 3: 7.500588144752852 and sum of bs: 0.0023852170048814254\n",
      "MSE on the training set: 3.208606703026014e-06\n",
      "Epoch: 2320\n",
      "sum of abs dWs for layer 0: 0.00018689718258006212 and sum of dbs: 0.19819991993410674\n",
      "sum of Ws for layer 0: 86.42321018564726 and sum of bs: 0.0015994092081868922\n",
      "sum of abs dWs for layer 1: 0.004919094259362217 and sum of dbs: 0.1372200294251511\n",
      "sum of Ws for layer 1: 321.32497270420174 and sum of bs: 0.0016886316415182028\n",
      "sum of abs dWs for layer 2: 0.0022547565320688665 and sum of dbs: 0.25023331461148945\n",
      "sum of Ws for layer 2: 340.8626858055636 and sum of bs: 0.007883164984267752\n",
      "sum of abs dWs for layer 3: 0.0010964018349949845 and sum of dbs: 0.09023033982551307\n",
      "sum of Ws for layer 3: 7.500588144756431 and sum of bs: 0.0023852161025778437\n",
      "MSE on the training set: 3.2086035442082886e-06\n",
      "Epoch: 2330\n",
      "sum of abs dWs for layer 0: 0.00018640675664138997 and sum of dbs: 0.1981998297757217\n",
      "sum of Ws for layer 0: 86.4232101856485 and sum of bs: 0.0015994076983488017\n",
      "sum of abs dWs for layer 1: 0.004905986074625063 and sum of dbs: 0.13721996682464185\n",
      "sum of Ws for layer 1: 321.3249727042164 and sum of bs: 0.0016886305136067003\n",
      "sum of abs dWs for layer 2: 0.0022479149047371855 and sum of dbs: 0.2502332006253685\n",
      "sum of Ws for layer 2: 340.86268580557055 and sum of bs: 0.007883162625784618\n",
      "sum of abs dWs for layer 3: 0.0010927353717752773 and sum of dbs: 0.0902302987300693\n",
      "sum of Ws for layer 3: 7.500588144759981 and sum of bs: 0.0023852152002746705\n",
      "MSE on the training set: 3.2086003854002346e-06\n",
      "Epoch: 2340\n",
      "sum of abs dWs for layer 0: 0.0001859759464934374 and sum of dbs: 0.19819973871891974\n",
      "sum of Ws for layer 0: 86.42321018564972 and sum of bs: 0.0015994061885114023\n",
      "sum of abs dWs for layer 1: 0.004894471336314193 and sum of dbs: 0.1372199036217937\n",
      "sum of Ws for layer 1: 321.324972704231 and sum of bs: 0.0016886293856957137\n",
      "sum of abs dWs for layer 2: 0.0022419050205837376 and sum of dbs: 0.2502330855219974\n",
      "sum of Ws for layer 2: 340.8626858055775 and sum of bs: 0.007883160267302564\n",
      "sum of abs dWs for layer 3: 0.0010895146704253092 and sum of dbs: 0.09023025723107264\n",
      "sum of Ws for layer 3: 7.50058814476351 and sum of bs: 0.0023852142979719116\n",
      "MSE on the training set: 3.2085972266004692e-06\n",
      "Epoch: 2350\n",
      "sum of abs dWs for layer 0: 0.00018559767990282422 and sum of dbs: 0.19819964686961516\n",
      "sum of Ws for layer 0: 86.42321018565094 and sum of bs: 0.0015994046786747004\n",
      "sum of abs dWs for layer 1: 0.004884361013567469 and sum of dbs: 0.13721983988761638\n",
      "sum of Ws for layer 1: 321.32497270424557 and sum of bs: 0.0016886282577852478\n",
      "sum of abs dWs for layer 2: 0.00223662820975171 and sum of dbs: 0.25023296943308876\n",
      "sum of Ws for layer 2: 340.8626858055845 and sum of bs: 0.007883157908821596\n",
      "sum of abs dWs for layer 3: 0.0010866868501235654 and sum of dbs: 0.09023021537609795\n",
      "sum of Ws for layer 3: 7.5005881447670175 and sum of bs: 0.0023852133956695693\n",
      "MSE on the training set: 3.2085940677749685e-06\n",
      "Epoch: 2360\n",
      "sum of abs dWs for layer 0: 0.00018526531470503378 and sum of dbs: 0.19819955432886185\n",
      "sum of Ws for layer 0: 86.42321018565215 and sum of bs: 0.0015994031688387014\n",
      "sum of abs dWs for layer 1: 0.004875477570647597 and sum of dbs: 0.1372197756898612\n",
      "sum of Ws for layer 1: 321.3249727042602 and sum of bs: 0.0016886271298753063\n",
      "sum of abs dWs for layer 2: 0.0022319918025734105 and sum of dbs: 0.25023285248431104\n",
      "sum of Ws for layer 2: 340.86268580559135 and sum of bs: 0.007883155550341726\n",
      "sum of abs dWs for layer 3: 0.0010842022457714852 and sum of dbs: 0.09023017321053692\n",
      "sum of Ws for layer 3: 7.500588144770505 and sum of bs: 0.002385212493367647\n",
      "MSE on the training set: 3.2085909089673194e-06\n",
      "Epoch: 2370\n",
      "sum of abs dWs for layer 0: 0.00018497322269902313 and sum of dbs: 0.19819946118087708\n",
      "sum of Ws for layer 0: 86.42321018565337 and sum of bs: 0.001599401659003411\n",
      "sum of abs dWs for layer 1: 0.004867670573619577 and sum of dbs: 0.13721971108499117\n",
      "sum of Ws for layer 1: 321.3249727042746 and sum of bs: 0.001688626001965894\n",
      "sum of abs dWs for layer 2: 0.0022279172759296167 and sum of dbs: 0.2502327347803949\n",
      "sum of Ws for layer 2: 340.8626858055983 and sum of bs: 0.007883153191862962\n",
      "sum of abs dWs for layer 3: 0.0010820187739376957 and sum of dbs: 0.0902301307722184\n",
      "sum of Ws for layer 3: 7.500588144773978 and sum of bs: 0.002385211591066148\n",
      "MSE on the training set: 3.2085877501623037e-06\n",
      "Epoch: 2380\n",
      "sum of abs dWs for layer 0: 0.0001847165397376199 and sum of dbs: 0.19819936749915287\n",
      "sum of Ws for layer 0: 86.4232101856546 and sum of bs: 0.001599400149168832\n",
      "sum of abs dWs for layer 1: 0.004860810010675872 and sum of dbs: 0.13721964612227874\n",
      "sum of Ws for layer 1: 321.32497270428917 and sum of bs: 0.001688624874057013\n",
      "sum of abs dWs for layer 2: 0.0022243367665812164 and sum of dbs: 0.2502326164127334\n",
      "sum of Ws for layer 2: 340.8626858056052 and sum of bs: 0.00788315083338531\n",
      "sum of abs dWs for layer 3: 0.0010801000642054089 and sum of dbs: 0.0902300880941538\n",
      "sum of Ws for layer 3: 7.500588144777439 and sum of bs: 0.002385210688765075\n",
      "MSE on the training set: 3.208584591375982e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2390\n",
      "sum of abs dWs for layer 0: 0.00018449060909099635 and sum of dbs: 0.1981992733528817\n",
      "sum of Ws for layer 0: 86.42321018565579 and sum of bs: 0.0015993986393349692\n",
      "sum of abs dWs for layer 1: 0.00485477141395031 and sum of dbs: 0.13721958084811375\n",
      "sum of Ws for layer 1: 321.3249727043036 and sum of bs: 0.001688623746148667\n",
      "sum of abs dWs for layer 2: 0.002221185305097683 and sum of dbs: 0.25023249746737264\n",
      "sum of Ws for layer 2: 340.86268580561205 and sum of bs: 0.007883148474908775\n",
      "sum of abs dWs for layer 3: 0.0010784112970593954 and sum of dbs: 0.09023004520742312\n",
      "sum of Ws for layer 3: 7.500588144780884 and sum of bs: 0.0023852097864644297\n",
      "MSE on the training set: 3.208581432593076e-06\n",
      "Epoch: 2400\n",
      "sum of abs dWs for layer 0: 0.000184292230385739 and sum of dbs: 0.19819917879208385\n",
      "sum of Ws for layer 0: 86.42321018565701 and sum of bs: 0.001599397129501826\n",
      "sum of abs dWs for layer 1: 0.004849469241953162 and sum of dbs: 0.1372195152960321\n",
      "sum of Ws for layer 1: 321.32497270431804 and sum of bs: 0.0016886226182408581\n",
      "sum of abs dWs for layer 2: 0.0022184182407569185 and sum of dbs: 0.25023237800651676\n",
      "sum of Ws for layer 2: 340.8626858056189 and sum of bs: 0.007883146116433364\n",
      "sum of abs dWs for layer 3: 0.0010769285425285269 and sum of dbs: 0.09023000213449463\n",
      "sum of Ws for layer 3: 7.500588144784318 and sum of bs: 0.0023852088841642147\n",
      "MSE on the training set: 3.208578273784298e-06\n",
      "Epoch: 2410\n",
      "sum of abs dWs for layer 0: 0.00018411804659139414 and sum of dbs: 0.19819908386695084\n",
      "sum of Ws for layer 0: 86.42321018565822 and sum of bs: 0.0015993956196694043\n",
      "sum of abs dWs for layer 1: 0.00484481376598382 and sum of dbs: 0.1372194494996844\n",
      "sum of Ws for layer 1: 321.32497270433254 and sum of bs: 0.0016886214903335884\n",
      "sum of abs dWs for layer 2: 0.0022159887371767805 and sum of dbs: 0.25023225809258276\n",
      "sum of Ws for layer 2: 340.8626858056258 and sum of bs: 0.007883143757959078\n",
      "sum of abs dWs for layer 3: 0.0010756266992657097 and sum of dbs: 0.0902299588979134\n",
      "sum of Ws for layer 3: 7.500588144787745 and sum of bs: 0.0023852079818644303\n",
      "MSE on the training set: 3.2085751150085048e-06\n",
      "Epoch: 2420\n",
      "sum of abs dWs for layer 0: 0.00018396545397859145 and sum of dbs: 0.19819898861626867\n",
      "sum of Ws for layer 0: 86.42321018565943 and sum of bs: 0.0015993941098377078\n",
      "sum of abs dWs for layer 1: 0.004840735391989663 and sum of dbs: 0.1372193834850747\n",
      "sum of Ws for layer 1: 321.32497270434703 and sum of bs: 0.0016886203624268598\n",
      "sum of abs dWs for layer 2: 0.0022138604678522402 and sum of dbs: 0.2502321377738042\n",
      "sum of Ws for layer 2: 340.8626858056326 and sum of bs: 0.007883141399485927\n",
      "sum of abs dWs for layer 3: 0.0010744862985597567 and sum of dbs: 0.0902299155151015\n",
      "sum of Ws for layer 3: 7.5005881447911635 and sum of bs: 0.0023852070795650797\n",
      "MSE on the training set: 3.2085719562265364e-06\n",
      "Epoch: 2430\n",
      "sum of abs dWs for layer 0: 0.0001838311951243117 and sum of dbs: 0.19819889308887828\n",
      "sum of Ws for layer 0: 86.42321018566062 and sum of bs: 0.0015993926000067381\n",
      "sum of abs dWs for layer 1: 0.004837147053639472 and sum of dbs: 0.13721931728494813\n",
      "sum of Ws for layer 1: 321.32497270436136 and sum of bs: 0.0016886192345206745\n",
      "sum of abs dWs for layer 2: 0.002211987986102045 and sum of dbs: 0.2502320171109183\n",
      "sum of Ws for layer 2: 340.8626858056395 and sum of bs: 0.007883139041013908\n",
      "sum of abs dWs for layer 3: 0.0010734829838567454 and sum of dbs: 0.09022987200799738\n",
      "sum of Ws for layer 3: 7.5005881447945715 and sum of bs: 0.0023852061772661626\n",
      "MSE on the training set: 3.208568797430182e-06\n",
      "Epoch: 2440\n",
      "sum of abs dWs for layer 0: 0.00018371325458464694 and sum of dbs: 0.1981987973160895\n",
      "sum of Ws for layer 0: 86.42321018566186 and sum of bs: 0.0015993910901764977\n",
      "sum of abs dWs for layer 1: 0.004833994880936027 and sum of dbs: 0.1372192509202963\n",
      "sum of Ws for layer 1: 321.3249727043758 and sum of bs: 0.0016886181066150334\n",
      "sum of abs dWs for layer 2: 0.0022103431730005727 and sum of dbs: 0.2502318961428615\n",
      "sum of Ws for layer 2: 340.8626858056464 and sum of bs: 0.007883136682543028\n",
      "sum of abs dWs for layer 3: 0.0010726016851822907 and sum of dbs: 0.09022982839066492\n",
      "sum of Ws for layer 3: 7.500588144797977 and sum of bs: 0.0023852052749676832\n",
      "MSE on the training set: 3.2085656386509784e-06\n",
      "Epoch: 2450\n",
      "sum of abs dWs for layer 0: 0.0001836093190439441 and sum of dbs: 0.1981987013311589\n",
      "sum of Ws for layer 0: 86.42321018566304 and sum of bs: 0.0015993895803469878\n",
      "sum of abs dWs for layer 1: 0.004831217042206442 and sum of dbs: 0.13721918441341596\n",
      "sum of Ws for layer 1: 321.32497270439023 and sum of bs: 0.001688616978709938\n",
      "sum of abs dWs for layer 2: 0.002208893753813938 and sum of dbs: 0.2502317749109909\n",
      "sum of Ws for layer 2: 340.8626858056532 and sum of bs: 0.00788313432407329\n",
      "sum of abs dWs for layer 3: 0.0010718251053188543 and sum of dbs: 0.09022978467804238\n",
      "sum of Ws for layer 3: 7.500588144801374 and sum of bs: 0.0023852043726696396\n",
      "MSE on the training set: 3.2085624798948886e-06\n",
      "Epoch: 2460\n",
      "sum of abs dWs for layer 0: 0.00018351810136272092 and sum of dbs: 0.19819860515548618\n",
      "sum of Ws for layer 0: 86.42321018566425 and sum of bs: 0.00159938807051821\n",
      "sum of abs dWs for layer 1: 0.004828779134018421 and sum of dbs: 0.13721911777865428\n",
      "sum of Ws for layer 1: 321.3249727044046 and sum of bs: 0.0016886158508053893\n",
      "sum of abs dWs for layer 2: 0.0022076217707126095 and sum of dbs: 0.2502316534419186\n",
      "sum of Ws for layer 2: 340.86268580566 and sum of bs: 0.007883131965604695\n",
      "sum of abs dWs for layer 3: 0.0010711436200026652 and sum of dbs: 0.09022974087974209\n",
      "sum of Ws for layer 3: 7.500588144804768 and sum of bs: 0.0023852034703720337\n",
      "MSE on the training set: 3.2085593211177518e-06\n",
      "Epoch: 2470\n",
      "sum of abs dWs for layer 0: 0.00018343821095103598 and sum of dbs: 0.19819850880890577\n",
      "sum of Ws for layer 0: 86.42321018566545 and sum of bs: 0.0015993865606901657\n",
      "sum of abs dWs for layer 1: 0.004826643987843506 and sum of dbs: 0.13721905102930937\n",
      "sum of Ws for layer 1: 321.324972704419 and sum of bs: 0.0016886147229013887\n",
      "sum of abs dWs for layer 2: 0.0022065078225730903 and sum of dbs: 0.25023153176031027\n",
      "sum of Ws for layer 2: 340.86268580566684 and sum of bs: 0.007883129607137246\n",
      "sum of abs dWs for layer 3: 0.0010705468314516874 and sum of dbs: 0.09022969700467334\n",
      "sum of Ws for layer 3: 7.500588144808156 and sum of bs: 0.002385202568074866\n",
      "MSE on the training set: 3.2085561623472616e-06\n",
      "Epoch: 2480\n",
      "sum of abs dWs for layer 0: 0.00018336790465600883 and sum of dbs: 0.1981984123180354\n",
      "sum of Ws for layer 0: 86.42321018566665 and sum of bs: 0.001599385050862856\n",
      "sum of abs dWs for layer 1: 0.004824765011658805 and sum of dbs: 0.13721898418322687\n",
      "sum of Ws for layer 1: 321.32497270443343 and sum of bs: 0.001688613594997937\n",
      "sum of abs dWs for layer 2: 0.002205527589403099 and sum of dbs: 0.2502314098992671\n",
      "sum of Ws for layer 2: 340.86268580567366 and sum of bs: 0.007883127248670943\n",
      "sum of abs dWs for layer 3: 0.001070021705691049 and sum of dbs: 0.09022965306479239\n",
      "sum of Ws for layer 3: 7.500588144811543 and sum of bs: 0.0023852016657781375\n",
      "MSE on the training set: 3.2085530035586533e-06\n",
      "Epoch: 2490\n",
      "sum of abs dWs for layer 0: 0.00018330601877300685 and sum of dbs: 0.19819831569950136\n",
      "sum of Ws for layer 0: 86.42321018566784 and sum of bs: 0.001599383541036282\n",
      "sum of abs dWs for layer 1: 0.0048231111012698585 and sum of dbs: 0.13721891725155388\n",
      "sum of Ws for layer 1: 321.32497270444776 and sum of bs: 0.0016886124670950347\n",
      "sum of abs dWs for layer 2: 0.0022046648355137964 and sum of dbs: 0.25023128787946525\n",
      "sum of Ws for layer 2: 340.8626858056805 and sum of bs: 0.007883124890205791\n",
      "sum of abs dWs for layer 3: 0.0010695595414130271 and sum of dbs: 0.09022960906756744\n",
      "sum of Ws for layer 3: 7.500588144814922 and sum of bs: 0.002385200763481849\n",
      "MSE on the training set: 3.208549844814689e-06\n",
      "Epoch: 2500\n",
      "sum of abs dWs for layer 0: 0.00018325166533889457 and sum of dbs: 0.1981982189678637\n",
      "sum of Ws for layer 0: 86.42321018566906 and sum of bs: 0.0015993820312104451\n",
      "sum of abs dWs for layer 1: 0.004821658522676211 and sum of dbs: 0.1372188502440521\n",
      "sum of Ws for layer 1: 321.32497270446214 and sum of bs: 0.0016886113391926826\n",
      "sum of abs dWs for layer 2: 0.002203907172292993 and sum of dbs: 0.25023116571901116\n",
      "sum of Ws for layer 2: 340.8626858056873 and sum of bs: 0.007883122531741789\n",
      "sum of abs dWs for layer 3: 0.001069153699101319 and sum of dbs: 0.09022956501953863\n",
      "sum of Ws for layer 3: 7.500588144818302 and sum of bs: 0.0023851998611860003\n",
      "MSE on the training set: 3.208546686046374e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2510\n",
      "sum of abs dWs for layer 0: 0.0001832040491506463 and sum of dbs: 0.1981981221350546\n",
      "sum of Ws for layer 0: 86.42321018567026 and sum of bs: 0.0015993805213853451\n",
      "sum of abs dWs for layer 1: 0.0048203860212055714 and sum of dbs: 0.13721878316872133\n",
      "sum of Ws for layer 1: 321.3249727044765 and sum of bs: 0.0016886102112908812\n",
      "sum of abs dWs for layer 2: 0.0022032435052732253 and sum of dbs: 0.25023104343274344\n",
      "sum of Ws for layer 2: 340.8626858056941 and sum of bs: 0.007883120173278937\n",
      "sum of abs dWs for layer 3: 0.0010687982328221146 and sum of dbs: 0.09022952092606566\n",
      "sum of Ws for layer 3: 7.500588144821676 and sum of bs: 0.0023851989588905926\n",
      "MSE on the training set: 3.2085435272840867e-06\n",
      "Epoch: 2520\n",
      "sum of abs dWs for layer 0: 0.00018316194723076335 and sum of dbs: 0.1981980252192535\n",
      "sum of Ws for layer 0: 86.42321018567145 and sum of bs: 0.0015993790115609832\n",
      "sum of abs dWs for layer 1: 0.004819260908395051 and sum of dbs: 0.13721871603775\n",
      "sum of Ws for layer 1: 321.32497270449085 and sum of bs: 0.0016886090833896313\n",
      "sum of abs dWs for layer 2: 0.002202656771811215 and sum of dbs: 0.2502309210432697\n",
      "sum of Ws for layer 2: 340.8626858057009 and sum of bs: 0.00788311781481724\n",
      "sum of abs dWs for layer 3: 0.001068483998074537 and sum of dbs: 0.09022947679531444\n",
      "sum of Ws for layer 3: 7.50058814482505 and sum of bs: 0.002385198056595626\n",
      "MSE on the training set: 3.208540368542811e-06\n",
      "Epoch: 2530\n",
      "sum of abs dWs for layer 0: 0.00018312467565744368 and sum of dbs: 0.19819792823005444\n",
      "sum of Ws for layer 0: 86.42321018567267 and sum of bs: 0.0015993775017373603\n",
      "sum of abs dWs for layer 1: 0.004818264903965471 and sum of dbs: 0.13721864885757035\n",
      "sum of Ws for layer 1: 321.3249727045053 and sum of bs: 0.0016886079554889332\n",
      "sum of abs dWs for layer 2: 0.002202137430031015 and sum of dbs: 0.25023079856252084\n",
      "sum of Ws for layer 2: 340.86268580570777 and sum of bs: 0.007883115456356695\n",
      "sum of abs dWs for layer 3: 0.0010682058810178852 and sum of dbs: 0.09022943263159441\n",
      "sum of Ws for layer 3: 7.500588144828423 and sum of bs: 0.002385197154301101\n",
      "MSE on the training set: 3.208537209782681e-06\n",
      "Epoch: 2540\n",
      "sum of abs dWs for layer 0: 0.00018309199552411047 and sum of dbs: 0.1981978311719666\n",
      "sum of Ws for layer 0: 86.42321018567385 and sum of bs: 0.0015993759919144764\n",
      "sum of abs dWs for layer 1: 0.004817391622284279 and sum of dbs: 0.13721858163120557\n",
      "sum of Ws for layer 1: 321.3249727045196 and sum of bs: 0.0016886068275887868\n",
      "sum of abs dWs for layer 2: 0.002201682146753943 and sum of dbs: 0.2502306759961043\n",
      "sum of Ws for layer 2: 340.86268580571453 and sum of bs: 0.007883113097897305\n",
      "sum of abs dWs for layer 3: 0.0010679620952789204 and sum of dbs: 0.09022938843693108\n",
      "sum of Ws for layer 3: 7.500588144831791 and sum of bs: 0.0023851962520070173\n",
      "MSE on the training set: 3.2085340510338676e-06\n",
      "Epoch: 2550\n",
      "sum of abs dWs for layer 0: 0.00018306296929615506 and sum of dbs: 0.19819773405868601\n",
      "sum of Ws for layer 0: 86.42321018567505 and sum of bs: 0.0015993744820923324\n",
      "sum of abs dWs for layer 1: 0.004816616004364764 and sum of dbs: 0.1372185143678381\n",
      "sum of Ws for layer 1: 321.32497270453393 and sum of bs: 0.0016886056996891932\n",
      "sum of abs dWs for layer 2: 0.0022012778417650853 and sum of dbs: 0.25023055336105227\n",
      "sum of Ws for layer 2: 340.86268580572136 and sum of bs: 0.007883110739439071\n",
      "sum of abs dWs for layer 3: 0.001067745630680764 and sum of dbs: 0.0902293442174764\n",
      "sum of Ws for layer 3: 7.5005881448351595 and sum of bs: 0.002385195349713376\n",
      "MSE on the training set: 3.208530892282447e-06\n",
      "Epoch: 2560\n",
      "sum of abs dWs for layer 0: 0.00018303739681443305 and sum of dbs: 0.19819763689392603\n",
      "sum of Ws for layer 0: 86.42321018567627 and sum of bs: 0.0015993729722709284\n",
      "sum of abs dWs for layer 1: 0.0048159327002416374 and sum of dbs: 0.1372184470699575\n",
      "sum of Ws for layer 1: 321.32497270454826 and sum of bs: 0.0016886045717901526\n",
      "sum of abs dWs for layer 2: 0.0022009217224945973 and sum of dbs: 0.25023043066198253\n",
      "sum of Ws for layer 2: 340.8626858057282 and sum of bs: 0.007883108380981992\n",
      "sum of abs dWs for layer 3: 0.0010675549905929853 and sum of dbs: 0.09022929997489838\n",
      "sum of Ws for layer 3: 7.5005881448385265 and sum of bs: 0.002385194447420177\n",
      "MSE on the training set: 3.208527733557716e-06\n",
      "Epoch: 2570\n",
      "sum of abs dWs for layer 0: 0.00018301550637156644 and sum of dbs: 0.19819753967367268\n",
      "sum of Ws for layer 0: 86.42321018567746 and sum of bs: 0.001599371462450265\n",
      "sum of abs dWs for layer 1: 0.004815347811842451 and sum of dbs: 0.13721837973487272\n",
      "sum of Ws for layer 1: 321.32497270456264 and sum of bs: 0.0016886034438916649\n",
      "sum of abs dWs for layer 2: 0.0022006169740216835 and sum of dbs: 0.2502303078939034\n",
      "sum of Ws for layer 2: 340.862685805735 and sum of bs: 0.007883106022526072\n",
      "sum of abs dWs for layer 3: 0.0010673918820072366 and sum of dbs: 0.09022925570739401\n",
      "sum of Ws for layer 3: 7.500588144841892 and sum of bs: 0.002385193545127421\n",
      "MSE on the training set: 3.2085245748041056e-06\n",
      "Epoch: 2580\n",
      "sum of abs dWs for layer 0: 0.00018299659815632302 and sum of dbs: 0.19819744240785808\n",
      "sum of Ws for layer 0: 86.42321018567866 and sum of bs: 0.0015993699526303425\n",
      "sum of abs dWs for layer 1: 0.0048148426342070364 and sum of dbs: 0.13721831236924262\n",
      "sum of Ws for layer 1: 321.324972704577 and sum of bs: 0.0016886023159937307\n",
      "sum of abs dWs for layer 2: 0.0022003538327739847 and sum of dbs: 0.25023018506916633\n",
      "sum of Ws for layer 2: 340.8626858057418 and sum of bs: 0.007883103664071306\n",
      "sum of abs dWs for layer 3: 0.0010672510722683271 and sum of dbs: 0.09022921141942462\n",
      "sum of Ws for layer 3: 7.500588144845257 and sum of bs: 0.0023851926428351076\n",
      "MSE on the training set: 3.208521416075537e-06\n",
      "Epoch: 2590\n",
      "sum of abs dWs for layer 0: 0.00018297963651134049 and sum of dbs: 0.19819734511310164\n",
      "sum of Ws for layer 0: 86.42321018567988 and sum of bs: 0.0015993684428111609\n",
      "sum of abs dWs for layer 1: 0.004814389485665918 and sum of dbs: 0.1372182449842096\n",
      "sum of Ws for layer 1: 321.32497270459135 and sum of bs: 0.0016886011880963502\n",
      "sum of abs dWs for layer 2: 0.002200117849547027 and sum of dbs: 0.25023006220843863\n",
      "sum of Ws for layer 2: 340.8626858057486 and sum of bs: 0.0078831013056177\n",
      "sum of abs dWs for layer 3: 0.0010671248175153042 and sum of dbs: 0.09022916711845534\n",
      "sum of Ws for layer 3: 7.500588144848619 and sum of bs: 0.002385191740543237\n",
      "MSE on the training set: 3.2085182573461675e-06\n",
      "Epoch: 2600\n",
      "sum of abs dWs for layer 0: 0.00018296427804661606 and sum of dbs: 0.1981972477935962\n",
      "sum of Ws for layer 0: 86.42321018568107 and sum of bs: 0.001599366932992721\n",
      "sum of abs dWs for layer 1: 0.004813979187883403 and sum of dbs: 0.13721817758258478\n",
      "sum of Ws for layer 1: 321.32497270460567 and sum of bs: 0.0016886000601995233\n",
      "sum of abs dWs for layer 2: 0.0021999042334437035 and sum of dbs: 0.25022993931693466\n",
      "sum of Ws for layer 2: 340.8626858057554 and sum of bs: 0.007883098947165249\n",
      "sum of abs dWs for layer 3: 0.0010670105501326206 and sum of dbs: 0.09022912280636956\n",
      "sum of Ws for layer 3: 7.500588144851982 and sum of bs: 0.0023851908382518097\n",
      "MSE on the training set: 3.2085150986084528e-06\n",
      "Epoch: 2610\n",
      "sum of abs dWs for layer 0: 0.0001829512550627939 and sum of dbs: 0.19819715043982702\n",
      "sum of Ws for layer 0: 86.42321018568225 and sum of bs: 0.0015993654231750223\n",
      "sum of abs dWs for layer 1: 0.004813631314216838 and sum of dbs: 0.13721811015798907\n",
      "sum of Ws for layer 1: 321.32497270462005 and sum of bs: 0.0016885989323032506\n",
      "sum of abs dWs for layer 2: 0.002199723201322194 and sum of dbs: 0.25022981638282193\n",
      "sum of Ws for layer 2: 340.8626858057622 and sum of bs: 0.007883096588713957\n",
      "sum of abs dWs for layer 3: 0.0010669137457082647 and sum of dbs: 0.09022907847889336\n",
      "sum of Ws for layer 3: 7.500588144855345 and sum of bs: 0.002385189935960825\n",
      "MSE on the training set: 3.208511939907706e-06\n",
      "Epoch: 2620\n",
      "sum of abs dWs for layer 0: 0.00018293982112397157 and sum of dbs: 0.19819705306171118\n",
      "sum of Ws for layer 0: 86.42321018568346 and sum of bs: 0.0015993639133580657\n",
      "sum of abs dWs for layer 1: 0.004813325913505005 and sum of dbs: 0.13721804271707128\n",
      "sum of Ws for layer 1: 321.32497270463443 and sum of bs: 0.0016885978044075317\n",
      "sum of abs dWs for layer 2: 0.002199564339128499 and sum of dbs: 0.25022969341843326\n",
      "sum of Ws for layer 2: 340.86268580576905 and sum of bs: 0.007883094230263826\n",
      "sum of abs dWs for layer 3: 0.001066828822963097 and sum of dbs: 0.09022903414048142\n",
      "sum of Ws for layer 3: 7.500588144858706 and sum of bs: 0.0023851890336702842\n",
      "MSE on the training set: 3.20850878116577e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2630\n",
      "sum of abs dWs for layer 0: 0.00018292997108874915 and sum of dbs: 0.19819695566025824\n",
      "sum of Ws for layer 0: 86.42321018568465 and sum of bs: 0.001599362403541851\n",
      "sum of abs dWs for layer 1: 0.004813062848303098 and sum of dbs: 0.13721797526050827\n",
      "sum of Ws for layer 1: 321.32497270464876 and sum of bs: 0.001688596676512367\n",
      "sum of abs dWs for layer 2: 0.0021994275751189294 and sum of dbs: 0.2502295704250239\n",
      "sum of Ws for layer 2: 340.8626858057758 and sum of bs: 0.007883091871814851\n",
      "sum of abs dWs for layer 3: 0.001066755743450606 and sum of dbs: 0.09022898979158711\n",
      "sum of Ws for layer 3: 7.500588144862067 and sum of bs: 0.002385188131380187\n",
      "MSE on the training set: 3.2085056224542635e-06\n",
      "Epoch: 2640\n",
      "sum of abs dWs for layer 0: 0.00018292095920214972 and sum of dbs: 0.19819685824542094\n",
      "sum of Ws for layer 0: 86.42321018568585 and sum of bs: 0.0015993608937263785\n",
      "sum of abs dWs for layer 1: 0.004812822185647278 and sum of dbs: 0.13721790779497287\n",
      "sum of Ws for layer 1: 321.3249727046631 and sum of bs: 0.001688595548617757\n",
      "sum of abs dWs for layer 2: 0.0021993025047337524 and sum of dbs: 0.25022944741497116\n",
      "sum of Ws for layer 2: 340.86268580578263 and sum of bs: 0.007883089513367037\n",
      "sum of abs dWs for layer 3: 0.0010666889309827758 and sum of dbs: 0.09022894543668111\n",
      "sum of Ws for layer 3: 7.500588144865429 and sum of bs: 0.002385187229090533\n",
      "MSE on the training set: 3.2085024637275047e-06\n",
      "Epoch: 2650\n",
      "sum of abs dWs for layer 0: 0.00031078222665650434 and sum of dbs: 0.19819483313010663\n",
      "sum of Ws for layer 0: 86.42321018570205 and sum of bs: 0.0015993593838075193\n",
      "sum of abs dWs for layer 1: 0.008230374660284447 and sum of dbs: 0.13721654791435578\n",
      "sum of Ws for layer 1: 321.3249727044813 and sum of bs: 0.0016885944205260162\n",
      "sum of abs dWs for layer 2: 0.003983188540584123 and sum of dbs: 0.25022692715780004\n",
      "sum of Ws for layer 2: 340.8626858058112 and sum of bs: 0.007883087155606943\n",
      "sum of abs dWs for layer 3: 0.0020289044856993802 and sum of dbs: 0.09022803518978391\n",
      "sum of Ws for layer 3: 7.5005881448743015 and sum of bs: 0.002385186326808539\n",
      "MSE on the training set: 3.2084993053297904e-06\n",
      "Epoch: 2660\n",
      "sum of abs dWs for layer 0: 0.00029524995422645803 and sum of dbs: 0.19819496973697875\n",
      "sum of Ws for layer 0: 86.42321018570402 and sum of bs: 0.001599357874007091\n",
      "sum of abs dWs for layer 1: 0.007815219025771765 and sum of dbs: 0.13721663734773046\n",
      "sum of Ws for layer 1: 321.32497270450546 and sum of bs: 0.0016885932926427342\n",
      "sum of abs dWs for layer 2: 0.0037664871918224167 and sum of dbs: 0.2502270951718698\n",
      "sum of Ws for layer 2: 340.8626858058235 and sum of bs: 0.007883084797182884\n",
      "sum of abs dWs for layer 3: 0.0019117286951577709 and sum of dbs: 0.090228095953335\n",
      "sum of Ws for layer 3: 7.500588144883824 and sum of bs: 0.002385185424527841\n",
      "MSE on the training set: 3.2084961466044906e-06\n",
      "Epoch: 2670\n",
      "sum of abs dWs for layer 0: 0.0002816034798399877 and sum of dbs: 0.19819507791513788\n",
      "sum of Ws for layer 0: 86.42321018570593 and sum of bs: 0.0015993563642057583\n",
      "sum of abs dWs for layer 1: 0.00745046812677862 and sum of dbs: 0.1372167077211906\n",
      "sum of Ws for layer 1: 321.32497270452836 and sum of bs: 0.0016885921647587661\n",
      "sum of abs dWs for layer 2: 0.003576095980723612 and sum of dbs: 0.25022722783263407\n",
      "sum of Ws for layer 2: 340.862685805835 and sum of bs: 0.00788308243875738\n",
      "sum of abs dWs for layer 3: 0.0018087794755153638 and sum of dbs: 0.090228143947206\n",
      "sum of Ws for layer 3: 7.500588144892598 and sum of bs: 0.0023851845222466073\n",
      "MSE on the training set: 3.208492987871043e-06\n",
      "Epoch: 2680\n",
      "sum of abs dWs for layer 0: 0.00026961371578055563 and sum of dbs: 0.19819516111697486\n",
      "sum of Ws for layer 0: 86.42321018570773 and sum of bs: 0.0015993548544037217\n",
      "sum of abs dWs for layer 1: 0.007129998770110348 and sum of dbs: 0.1372167613493777\n",
      "sum of Ws for layer 1: 321.3249727045503 and sum of bs: 0.0016885910368742625\n",
      "sum of abs dWs for layer 2: 0.0034088187382445853 and sum of dbs: 0.25022732943340814\n",
      "sum of Ws for layer 2: 340.862685805846 and sum of bs: 0.007883080080330751\n",
      "sum of abs dWs for layer 3: 0.001718328575640897 and sum of dbs: 0.09022818072215047\n",
      "sum of Ws for layer 3: 7.500588144900713 and sum of bs: 0.0023851836199649565\n",
      "MSE on the training set: 3.2084898291524703e-06\n",
      "Epoch: 2690\n",
      "sum of abs dWs for layer 0: 0.00025908016317643197 and sum of dbs: 0.19819522236661735\n",
      "sum of Ws for layer 0: 86.42321018570946 and sum of bs: 0.0015993533446011575\n",
      "sum of abs dWs for layer 1: 0.00684845190226396 and sum of dbs: 0.1372168002598059\n",
      "sum of Ws for layer 1: 321.3249727045712 and sum of bs: 0.001688589908989356\n",
      "sum of abs dWs for layer 2: 0.0032618581590468294 and sum of dbs: 0.25022740373492913\n",
      "sum of Ws for layer 2: 340.8626858058565 and sum of bs: 0.007883077721903274\n",
      "sum of abs dWs for layer 3: 0.0016388634202857454 and sum of dbs: 0.09022820763655363\n",
      "sum of Ws for layer 3: 7.500588144908252 and sum of bs: 0.0023851827176829933\n",
      "MSE on the training set: 3.208486670446356e-06\n",
      "Epoch: 2700\n",
      "sum of abs dWs for layer 0: 0.0002498250583083759 and sum of dbs: 0.19819526434176207\n",
      "sum of Ws for layer 0: 86.42321018571113 and sum of bs: 0.0015993518347982188\n",
      "sum of abs dWs for layer 1: 0.006601076148474191 and sum of dbs: 0.13721682624772613\n",
      "sum of Ws for layer 1: 321.3249727045914 and sum of bs: 0.0016885887811041625\n",
      "sum of abs dWs for layer 2: 0.003132734132430243 and sum of dbs: 0.25022745406712027\n",
      "sum of Ws for layer 2: 340.8626858058666 and sum of bs: 0.007883075363475189\n",
      "sum of abs dWs for layer 3: 0.0015690429495490348 and sum of dbs: 0.09022822589319\n",
      "sum of Ws for layer 3: 7.500588144915282 and sum of bs: 0.00238518181540081\n",
      "MSE on the training set: 3.208483511719722e-06\n",
      "Epoch: 2710\n",
      "sum of abs dWs for layer 0: 0.0002416938995543638 and sum of dbs: 0.1981952893739015\n",
      "sum of Ws for layer 0: 86.42321018571275 and sum of bs: 0.001599350324995042\n",
      "sum of abs dWs for layer 1: 0.006383741897164127 and sum of dbs: 0.1372168408762781\n",
      "sum of Ws for layer 1: 321.3249727046109 and sum of bs: 0.0016885876532187847\n",
      "sum of abs dWs for layer 2: 0.003019291094072251 and sum of dbs: 0.25022748332937295\n",
      "sum of Ws for layer 2: 340.8626858058762 and sum of bs: 0.007883073005046716\n",
      "sum of abs dWs for layer 3: 0.0015077015941339313 and sum of dbs: 0.09022823653932517\n",
      "sum of Ws for layer 3: 7.500588144921867 and sum of bs: 0.0023851809131184864\n",
      "MSE on the training set: 3.2084803530425033e-06\n",
      "Epoch: 2720\n",
      "sum of abs dWs for layer 0: 0.00023454968680982756 and sum of dbs: 0.1981952995266294\n",
      "sum of Ws for layer 0: 86.4232101857143 and sum of bs: 0.0015993488151917461\n",
      "sum of abs dWs for layer 1: 0.006192787327797208 and sum of dbs: 0.13721684552898994\n",
      "sum of Ws for layer 1: 321.32497270462966 and sum of bs: 0.001688586525333312\n",
      "sum of abs dWs for layer 2: 0.002919617656058947 and sum of dbs: 0.25022749408792594\n",
      "sum of Ws for layer 2: 340.86268580588546 and sum of bs: 0.007883070646618038\n",
      "sum of abs dWs for layer 3: 0.0014538058172303366 and sum of dbs: 0.09022824050188938\n",
      "sum of Ws for layer 3: 7.50058814492806 and sum of bs: 0.002385180010836094\n",
      "MSE on the training set: 3.2084771943324028e-06\n",
      "Epoch: 2730\n",
      "sum of abs dWs for layer 0: 0.00022827332779188232 and sum of dbs: 0.1981952965971025\n",
      "sum of Ws for layer 0: 86.42321018571582 and sum of bs: 0.001599347305388436\n",
      "sum of abs dWs for layer 1: 0.006025029270753859 and sum of dbs: 0.13721684141075802\n",
      "sum of Ws for layer 1: 321.32497270464796 and sum of bs: 0.0016885853974478232\n",
      "sum of abs dWs for layer 2: 0.002832052275486012 and sum of dbs: 0.25022748857768273\n",
      "sum of Ws for layer 2: 340.86268580589444 and sum of bs: 0.007883068288189323\n",
      "sum of abs dWs for layer 3: 0.001406457179676965 and sum of dbs: 0.09022823858813395\n",
      "sum of Ws for layer 3: 7.500588144933909 and sum of bs: 0.002385179108553695\n",
      "MSE on the training set: 3.2084740356484014e-06\n",
      "Epoch: 2740\n",
      "sum of abs dWs for layer 0: 0.00022275880545597813 and sum of dbs: 0.19819528218156357\n",
      "sum of Ws for layer 0: 86.42321018571732 and sum of bs: 0.0015993457955852035\n",
      "sum of abs dWs for layer 1: 0.005877634039199637 and sum of dbs: 0.13721682959177686\n",
      "sum of Ws for layer 1: 321.3249727046658 and sum of bs: 0.0016885842695623884\n",
      "sum of abs dWs for layer 2: 0.0027551158315792554 and sum of dbs: 0.25022746878369484\n",
      "sum of Ws for layer 2: 340.8626858059032 and sum of bs: 0.007883065929760718\n",
      "sum of abs dWs for layer 3: 0.0013648558826807383 and sum of dbs: 0.09022823151506322\n",
      "sum of Ws for layer 3: 7.500588144939455 and sum of bs: 0.002385178206271344\n",
      "MSE on the training set: 3.2084708769484576e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2750\n",
      "sum of abs dWs for layer 0: 0.00021791384389267788 and sum of dbs: 0.19819525767185867\n",
      "sum of Ws for layer 0: 86.42321018571874 and sum of bs: 0.0015993442857821286\n",
      "sum of abs dWs for layer 1: 0.005748135227561942 and sum of dbs: 0.1372168110052041\n",
      "sum of Ws for layer 1: 321.3249727046832 and sum of bs: 0.0016885831416770677\n",
      "sum of abs dWs for layer 2: 0.002687520916153499 and sum of dbs: 0.2502274364368308\n",
      "sum of Ws for layer 2: 340.8626858059117 and sum of bs: 0.00788306357133235\n",
      "sum of abs dWs for layer 3: 0.0013283455875526535 and sum of dbs: 0.0902282199078701\n",
      "sum of Ws for layer 3: 7.500588144944735 and sum of bs: 0.0023851773039890896\n",
      "MSE on the training set: 3.2084677182799074e-06\n",
      "Epoch: 2760\n",
      "sum of abs dWs for layer 0: 0.00021365730284978048 and sum of dbs: 0.1981952242919721\n",
      "sum of Ws for layer 0: 86.42321018572018 and sum of bs: 0.0015993427759792842\n",
      "sum of abs dWs for layer 1: 0.005634364070747792 and sum of dbs: 0.13721678647165497\n",
      "sum of Ws for layer 1: 321.32497270470026 and sum of bs: 0.001688582013791915\n",
      "sum of abs dWs for layer 2: 0.002628135482606246 and sum of dbs: 0.2502273930592099\n",
      "sum of Ws for layer 2: 340.86268580592 and sum of bs: 0.00788306121290433\n",
      "sum of abs dWs for layer 3: 0.0012965189593202526 and sum of dbs: 0.0902282043163469\n",
      "sum of Ws for layer 3: 7.500588144949781 and sum of bs: 0.002385176401706973\n",
      "MSE on the training set: 3.208464559602672e-06\n",
      "Epoch: 2770\n",
      "sum of abs dWs for layer 0: 0.0002099172954385879 and sum of dbs: 0.19819518312496442\n",
      "sum of Ws for layer 0: 86.42321018572156 and sum of bs: 0.001599341266176731\n",
      "sum of abs dWs for layer 1: 0.0055343991331083635 and sum of dbs: 0.13721675671726308\n",
      "sum of Ws for layer 1: 321.3249727047169 and sum of bs: 0.0016885808859069772\n",
      "sum of abs dWs for layer 2: 0.0025759565847942654 and sum of dbs: 0.25022733999770147\n",
      "sum of Ws for layer 2: 340.8626858059281 and sum of bs: 0.007883058854476759\n",
      "sum of abs dWs for layer 3: 0.0012685545795580182 and sum of dbs: 0.09022818522698543\n",
      "sum of Ws for layer 3: 7.500588144954624 and sum of bs: 0.002385175499425032\n",
      "MSE on the training set: 3.2084614009202365e-06\n",
      "Epoch: 2780\n",
      "sum of abs dWs for layer 0: 0.00020663139634804407 and sum of dbs: 0.19819513511232173\n",
      "sum of Ws for layer 0: 86.42321018572291 and sum of bs: 0.0015993397563745246\n",
      "sum of abs dWs for layer 1: 0.005446571873665307 and sum of dbs: 0.13721672237324395\n",
      "sum of Ws for layer 1: 321.32497270473334 and sum of bs: 0.001688579758022296\n",
      "sum of abs dWs for layer 2: 0.002530113281864755 and sum of dbs: 0.25022727842311676\n",
      "sum of Ws for layer 2: 340.86268580593605 and sum of bs: 0.007883056496049716\n",
      "sum of abs dWs for layer 3: 0.0012439856789254273 and sum of dbs: 0.09022816306268483\n",
      "sum of Ws for layer 3: 7.500588144959283 and sum of bs: 0.0023851745971433\n",
      "MSE on the training set: 3.2084582422412818e-06\n",
      "Epoch: 2790\n",
      "sum of abs dWs for layer 0: 0.0002037447040503136 and sum of dbs: 0.19819508108135875\n",
      "sum of Ws for layer 0: 86.42321018572427 and sum of bs: 0.001599338246572713\n",
      "sum of abs dWs for layer 1: 0.005369414851619263 and sum of dbs: 0.1372166839942674\n",
      "sum of Ws for layer 1: 321.3249727047495 and sum of bs: 0.0016885786301379067\n",
      "sum of abs dWs for layer 2: 0.0024898396027318126 and sum of dbs: 0.25022720936428544\n",
      "sum of Ws for layer 2: 340.86268580594384 and sum of bs: 0.007883054137623288\n",
      "sum of abs dWs for layer 3: 0.0012224017452456867 and sum of dbs: 0.09022813819506034\n",
      "sum of Ws for layer 3: 7.500588144963787 and sum of bs: 0.0023851736948618035\n",
      "MSE on the training set: 3.2084550835529622e-06\n",
      "Epoch: 2800\n",
      "sum of abs dWs for layer 0: 0.00020120807916135968 and sum of dbs: 0.19819502177257262\n",
      "sum of Ws for layer 0: 86.42321018572561 and sum of bs: 0.0015993367367713382\n",
      "sum of abs dWs for layer 1: 0.005301614640187012 and sum of dbs: 0.13721664207679674\n",
      "sum of Ws for layer 1: 321.3249727047654 and sum of bs: 0.0016885775022538424\n",
      "sum of abs dWs for layer 2: 0.002454449968185871 and sum of dbs: 0.2502271337420733\n",
      "sum of Ws for layer 2: 340.86268580595157 and sum of bs: 0.007883051779197535\n",
      "sum of abs dWs for layer 3: 0.0012034353513175828 and sum of dbs: 0.09022811095673042\n",
      "sum of Ws for layer 3: 7.500588144968151 and sum of bs: 0.0023851727925805694\n",
      "MSE on the training set: 3.20845192491803e-06\n",
      "Epoch: 2810\n",
      "sum of abs dWs for layer 0: 0.00019897968810724534 and sum of dbs: 0.19819495781732943\n",
      "sum of Ws for layer 0: 86.4232101857269 and sum of bs: 0.0015993352269704376\n",
      "sum of abs dWs for layer 1: 0.005242053086646059 and sum of dbs: 0.1372165970441288\n",
      "sum of Ws for layer 1: 321.32497270478115 and sum of bs: 0.0016885763743701302\n",
      "sum of abs dWs for layer 2: 0.002423360727662486 and sum of dbs: 0.25022705234163284\n",
      "sum of Ws for layer 2: 340.86268580595913 and sum of bs: 0.007883049420772519\n",
      "sum of abs dWs for layer 3: 0.001186773697259241 and sum of dbs: 0.09022808163129356\n",
      "sum of Ws for layer 3: 7.500588144972392 and sum of bs: 0.00238517189029962\n",
      "MSE on the training set: 3.208448766233988e-06\n",
      "Epoch: 2820\n",
      "sum of abs dWs for layer 0: 0.0001970217199703149 and sum of dbs: 0.1981948897850852\n",
      "sum of Ws for layer 0: 86.42321018572821 and sum of bs: 0.001599333717170044\n",
      "sum of abs dWs for layer 1: 0.00518971955821927 and sum of dbs: 0.13721654927805307\n",
      "sum of Ws for layer 1: 321.32497270479666 and sum of bs: 0.0016885752464867953\n",
      "sum of abs dWs for layer 2: 0.0023960443535912326 and sum of dbs: 0.2502269658711267\n",
      "sum of Ws for layer 2: 340.86268580596663 and sum of bs: 0.007883047062348289\n",
      "sum of abs dWs for layer 3: 0.0011721340615860494 and sum of dbs: 0.09022805047453945\n",
      "sum of Ws for layer 3: 7.5005881449765255 and sum of bs: 0.0023851709880189738\n",
      "MSE on the training set: 3.208445607586165e-06\n",
      "Epoch: 2830\n",
      "sum of abs dWs for layer 0: 0.00019530145940748695 and sum of dbs: 0.1981948181704353\n",
      "sum of Ws for layer 0: 86.42321018572949 and sum of bs: 0.0015993322073701857\n",
      "sum of abs dWs for layer 1: 0.005143739619686462 and sum of dbs: 0.13721649911016912\n",
      "sum of Ws for layer 1: 321.3249727048121 and sum of bs: 0.001688574118603859\n",
      "sum of abs dWs for layer 2: 0.002372044410465737 and sum of dbs: 0.2502268749456226\n",
      "sum of Ws for layer 2: 340.862685805974 and sum of bs: 0.007883044703924893\n",
      "sum of abs dWs for layer 3: 0.0011592718236476173 and sum of dbs: 0.09022801770863173\n",
      "sum of Ws for layer 3: 7.500588144980566 and sum of bs: 0.0023851700857386473\n",
      "MSE on the training set: 3.2084424489170164e-06\n",
      "Epoch: 2840\n",
      "sum of abs dWs for layer 0: 0.00019379008741423568 and sum of dbs: 0.19819474340548907\n",
      "sum of Ws for layer 0: 86.42321018573075 and sum of bs: 0.0015993306975708884\n",
      "sum of abs dWs for layer 1: 0.005103342979514442 and sum of dbs: 0.1372164468301827\n",
      "sum of Ws for layer 1: 321.3249727048275 and sum of bs: 0.00168857299072134\n",
      "sum of abs dWs for layer 2: 0.002350958823448772 and sum of dbs: 0.2502267801024819\n",
      "sum of Ws for layer 2: 340.86268580598136 and sum of bs: 0.007883042345502366\n",
      "sum of abs dWs for layer 3: 0.001147971496714911 and sum of dbs: 0.0902279835276663\n",
      "sum of Ws for layer 3: 7.500588144984522 and sum of bs: 0.002385169183458657\n",
      "MSE on the training set: 3.2084392902502932e-06\n",
      "Epoch: 2850\n",
      "sum of abs dWs for layer 0: 0.0001924620381756033 and sum of dbs: 0.19819466587739815\n",
      "sum of Ws for layer 0: 86.42321018573205 and sum of bs: 0.001599329187772174\n",
      "sum of abs dWs for layer 1: 0.005067846299316138 and sum of dbs: 0.13721639269765817\n",
      "sum of Ws for layer 1: 321.3249727048426 and sum of bs: 0.0016885718628392556\n",
      "sum of abs dWs for layer 2: 0.0023324309052615675 and sum of dbs: 0.2502266818231579\n",
      "sum of Ws for layer 2: 340.8626858059886 and sum of bs: 0.00788303998708075\n",
      "sum of abs dWs for layer 3: 0.0011380419189058392 and sum of dbs: 0.09022794810554496\n",
      "sum of Ws for layer 3: 7.5005881449884075 and sum of bs: 0.0023851682811790154\n",
      "MSE on the training set: 3.2084361316277526e-06\n",
      "Epoch: 2860\n",
      "sum of abs dWs for layer 0: 0.0001912951810718871 and sum of dbs: 0.1981945859190528\n",
      "sum of Ws for layer 0: 86.4232101857333 and sum of bs: 0.0015993276779740617\n",
      "sum of abs dWs for layer 1: 0.005036658058708665 and sum of dbs: 0.13721633693578048\n",
      "sum of Ws for layer 1: 321.3249727048577 and sum of bs: 0.0016885707349576203\n",
      "sum of abs dWs for layer 2: 0.00231615189560046 and sum of dbs: 0.250226580521625\n",
      "sum of Ws for layer 2: 340.8626858059958 and sum of bs: 0.00788303762866007\n",
      "sum of abs dWs for layer 3: 0.001129317614171495 and sum of dbs: 0.09022791159179613\n",
      "sum of Ws for layer 3: 7.500588144992227 and sum of bs: 0.002385167378899734\n",
      "MSE on the training set: 3.2084329729778494e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2870\n",
      "sum of abs dWs for layer 0: 0.00019026999323459984 and sum of dbs: 0.19819450382533996\n",
      "sum of Ws for layer 0: 86.42321018573453 and sum of bs: 0.0015993261681765691\n",
      "sum of abs dWs for layer 1: 0.0050092564387959485 and sum of dbs: 0.1372162797422554\n",
      "sum of Ws for layer 1: 321.3249727048726 and sum of bs: 0.0016885696070764468\n",
      "sum of abs dWs for layer 2: 0.0023018494167161493 and sum of dbs: 0.25022647656459823\n",
      "sum of Ws for layer 2: 340.862685806003 and sum of bs: 0.007883035270240356\n",
      "sum of abs dWs for layer 3: 0.0011216526052303549 and sum of dbs: 0.0902278741188779\n",
      "sum of Ws for layer 3: 7.500588144995992 and sum of bs: 0.0023851664766208233\n",
      "MSE on the training set: 3.2084298143148307e-06\n",
      "Epoch: 2880\n",
      "sum of abs dWs for layer 0: 0.00018936933638904913 and sum of dbs: 0.19819441985358832\n",
      "sum of Ws for layer 0: 86.4232101857358 and sum of bs: 0.0015993246583797108\n",
      "sum of abs dWs for layer 1: 0.004985183357550444 and sum of dbs: 0.13721622128960778\n",
      "sum of Ws for layer 1: 321.32497270488756 and sum of bs: 0.0016885684791957466\n",
      "sum of abs dWs for layer 2: 0.0022892843601199407 and sum of dbs: 0.2502263702720857\n",
      "sum of Ws for layer 2: 340.86268580601006 and sum of bs: 0.007883032911821632\n",
      "sum of abs dWs for layer 3: 0.0011149187450218952 and sum of dbs: 0.0902278358023778\n",
      "sum of Ws for layer 3: 7.500588144999705 and sum of bs: 0.0023851655743422928\n",
      "MSE on the training set: 3.208426655675609e-06\n",
      "Epoch: 2890\n",
      "sum of abs dWs for layer 0: 0.00018857808320291928 and sum of dbs: 0.19819433423336993\n",
      "sum of Ws for layer 0: 86.42321018573703 and sum of bs: 0.0015993231485835\n",
      "sum of abs dWs for layer 1: 0.0049640344825987144 and sum of dbs: 0.1372161617317535\n",
      "sum of Ws for layer 1: 321.3249727049024 and sum of bs: 0.0016885673513155294\n",
      "sum of abs dWs for layer 2: 0.002278245673402542 and sum of dbs: 0.250226261929578\n",
      "sum of Ws for layer 2: 340.86268580601717 and sum of bs: 0.007883030553403916\n",
      "sum of abs dWs for layer 3: 0.0011090029227601747 and sum of dbs: 0.09022779674541559\n",
      "sum of Ws for layer 3: 7.500588145003375 and sum of bs: 0.0023851646720641486\n",
      "MSE on the training set: 3.2084234970407127e-06\n",
      "Epoch: 2900\n",
      "sum of abs dWs for layer 0: 0.000187882652787744 and sum of dbs: 0.19819424716828976\n",
      "sum of Ws for layer 0: 86.42321018573827 and sum of bs: 0.0015993216387879489\n",
      "sum of abs dWs for layer 1: 0.004945446815856975 and sum of dbs: 0.1372161012051984\n",
      "sum of Ws for layer 1: 321.3249727049172 and sum of bs: 0.001688566223435804\n",
      "sum of abs dWs for layer 2: 0.002268543879733192 and sum of dbs: 0.2502261517902737\n",
      "sum of Ws for layer 2: 340.8626858060242 and sum of bs: 0.007883028194987228\n",
      "sum of abs dWs for layer 3: 0.0011038035908271222 and sum of dbs: 0.09022775703944702\n",
      "sum of Ws for layer 3: 7.500588145007009 and sum of bs: 0.002385163769786399\n",
      "MSE on the training set: 3.208420338400442e-06\n",
      "Epoch: 2910\n",
      "sum of abs dWs for layer 0: 0.00018727193683592885 and sum of dbs: 0.19819415882657437\n",
      "sum of Ws for layer 0: 86.42321018573952 and sum of bs: 0.0015993201289930669\n",
      "sum of abs dWs for layer 1: 0.0049291234478493874 and sum of dbs: 0.1372160398227295\n",
      "sum of Ws for layer 1: 321.3249727049319 and sum of bs: 0.0016885650955565782\n",
      "sum of abs dWs for layer 2: 0.0022600239990444027 and sum of dbs: 0.25022604006337545\n",
      "sum of Ws for layer 2: 340.8626858060312 and sum of bs: 0.007883025836571585\n",
      "sum of abs dWs for layer 3: 0.0010992376896786912 and sum of dbs: 0.09022771676003656\n",
      "sum of Ws for layer 3: 7.5005881450106076 and sum of bs: 0.0023851628675090495\n",
      "MSE on the training set: 3.208417179767684e-06\n",
      "Epoch: 2920\n",
      "sum of abs dWs for layer 0: 0.00018673511276751857 and sum of dbs: 0.19819406937078932\n",
      "sum of Ws for layer 0: 86.42321018574074 and sum of bs: 0.0015993186191988634\n",
      "sum of abs dWs for layer 1: 0.004914775106200727 and sum of dbs: 0.13721597769333815\n",
      "sum of Ws for layer 1: 321.3249727049466 and sum of bs: 0.001688563967677859\n",
      "sum of abs dWs for layer 2: 0.002252535037680202 and sum of dbs: 0.2502259269510463\n",
      "sum of Ws for layer 2: 340.8626858060382 and sum of bs: 0.007883023478157\n",
      "sum of abs dWs for layer 3: 0.0010952242962459612 and sum of dbs: 0.0902276759802058\n",
      "sum of Ws for layer 3: 7.500588145014177 and sum of bs: 0.002385161965232106\n",
      "MSE on the training set: 3.2084140211387868e-06\n",
      "Epoch: 2930\n",
      "sum of abs dWs for layer 0: 0.00018626350378843365 and sum of dbs: 0.19819397893163263\n",
      "sum of Ws for layer 0: 86.42321018574198 and sum of bs: 0.0015993171094053455\n",
      "sum of abs dWs for layer 1: 0.004902169872376851 and sum of dbs: 0.13721591490465038\n",
      "sum of Ws for layer 1: 321.32497270496117 and sum of bs: 0.001688562839799652\n",
      "sum of abs dWs for layer 2: 0.0022459559393696034 and sum of dbs: 0.2502258126158193\n",
      "sum of Ws for layer 2: 340.8626858060452 and sum of bs: 0.007883021119743486\n",
      "sum of abs dWs for layer 3: 0.001091698532004128 and sum of dbs: 0.09022763475866201\n",
      "sum of Ws for layer 3: 7.500588145017722 and sum of bs: 0.0023851610629555733\n",
      "MSE on the training set: 3.208410862501156e-06\n",
      "Epoch: 2940\n",
      "sum of abs dWs for layer 0: 0.00018584933626994744 and sum of dbs: 0.1981938876268163\n",
      "sum of Ws for layer 0: 86.42321018574319 and sum of bs: 0.0015993155996125213\n",
      "sum of abs dWs for layer 1: 0.00489109996818197 and sum of dbs: 0.13721585153558558\n",
      "sum of Ws for layer 1: 321.32497270497583 and sum of bs: 0.001688561711921962\n",
      "sum of abs dWs for layer 2: 0.002240178248573358 and sum of dbs: 0.2502256972040784\n",
      "sum of Ws for layer 2: 340.8626858060521 and sum of bs: 0.007883018761331054\n",
      "sum of abs dWs for layer 3: 0.0010886022716251357 and sum of dbs: 0.09022759314827936\n",
      "sum of Ws for layer 3: 7.5005881450212435 and sum of bs: 0.002385160160679454\n",
      "MSE on the training set: 3.2084077038836165e-06\n",
      "Epoch: 2950\n",
      "sum of abs dWs for layer 0: 0.00018548543168678462 and sum of dbs: 0.19819379556400557\n",
      "sum of Ws for layer 0: 86.42321018574442 and sum of bs: 0.0015993140898203965\n",
      "sum of abs dWs for layer 1: 0.004881373521727556 and sum of dbs: 0.13721578765832765\n",
      "sum of Ws for layer 1: 321.3249727049904 and sum of bs: 0.0016885605840447945\n",
      "sum of abs dWs for layer 2: 0.0022351018124810995 and sum of dbs: 0.25022558084971364\n",
      "sum of Ws for layer 2: 340.86268580605906 and sum of bs: 0.007883016402919713\n",
      "sum of abs dWs for layer 3: 0.001085881839526623 and sum of dbs: 0.09022755119741915\n",
      "sum of Ws for layer 3: 7.500588145024746 and sum of bs: 0.002385159258403753\n",
      "MSE on the training set: 3.2084045452686224e-06\n",
      "Epoch: 2960\n",
      "sum of abs dWs for layer 0: 0.00018516562802414605 and sum of dbs: 0.198193702836446\n",
      "sum of Ws for layer 0: 86.42321018574563 and sum of bs: 0.001599312580028976\n",
      "sum of abs dWs for layer 1: 0.0048728258309945725 and sum of dbs: 0.13721572333539256\n",
      "sum of Ws for layer 1: 321.32497270500494 and sum of bs: 0.0016885594561681528\n",
      "sum of abs dWs for layer 2: 0.002230640660355744 and sum of dbs: 0.25022546366868315\n",
      "sum of Ws for layer 2: 340.862685806066 and sum of bs: 0.007883014044509471\n",
      "sum of abs dWs for layer 3: 0.0010834911608113623 and sum of dbs: 0.0902275089479657\n",
      "sum of Ws for layer 3: 7.500588145028229 and sum of bs: 0.0023851583561284727\n",
      "MSE on the training set: 3.2084013866502704e-06\n",
      "Epoch: 2970\n",
      "sum of abs dWs for layer 0: 0.00018488494389817817 and sum of dbs: 0.19819360951912846\n",
      "sum of Ws for layer 0: 86.42321018574685 and sum of bs: 0.001599311070238265\n",
      "sum of abs dWs for layer 1: 0.004865323750584535 and sum of dbs: 0.1372156586170577\n",
      "sum of Ws for layer 1: 321.32497270501943 and sum of bs: 0.0016885583282920406\n",
      "sum of abs dWs for layer 2: 0.0022267252932876786 and sum of dbs: 0.2502253457542438\n",
      "sum of Ws for layer 2: 340.8626858060728 and sum of bs: 0.007883011686100339\n",
      "sum of abs dWs for layer 3: 0.0010813929884537238 and sum of dbs: 0.0902274664336036\n",
      "sum of Ws for layer 3: 7.500588145031697 and sum of bs: 0.0023851574538536167\n",
      "MSE on the training set: 3.2083982280252695e-06\n",
      "Epoch: 2980\n",
      "sum of abs dWs for layer 0: 0.00018463789152694746 and sum of dbs: 0.19819351569523597\n",
      "sum of Ws for layer 0: 86.42321018574805 and sum of bs: 0.001599309560448267\n",
      "sum of abs dWs for layer 1: 0.004858720599781822 and sum of dbs: 0.1372155935590928\n",
      "sum of Ws for layer 1: 321.32497270503393 and sum of bs: 0.001688557200416461\n",
      "sum of abs dWs for layer 2: 0.0022232791472416255 and sum of dbs: 0.25022522720984025\n",
      "sum of Ws for layer 2: 340.86268580607975 and sum of bs: 0.00788300932769232\n",
      "sum of abs dWs for layer 3: 0.0010795462889564983 and sum of dbs: 0.09022742369169717\n",
      "sum of Ws for layer 3: 7.5005881450351515 and sum of bs: 0.002385156551579187\n",
      "MSE on the training set: 3.208395069426401e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2990\n",
      "sum of abs dWs for layer 0: 0.00018442060154450485 and sum of dbs: 0.1981934214217095\n",
      "sum of Ws for layer 0: 86.42321018574927 and sum of bs: 0.0015993080506589864\n",
      "sum of abs dWs for layer 1: 0.00485291295586956 and sum of dbs: 0.1372155281996736\n",
      "sum of Ws for layer 1: 321.32497270504837 and sum of bs: 0.0016885560725414172\n",
      "sum of abs dWs for layer 2: 0.002220248237893794 and sum of dbs: 0.25022510810628296\n",
      "sum of Ws for layer 2: 340.86268580608663 and sum of bs: 0.007883006969285417\n",
      "sum of abs dWs for layer 3: 0.0010779221301345566 and sum of dbs: 0.09022738074782334\n",
      "sum of Ws for layer 3: 7.500588145038595 and sum of bs: 0.0023851556493051864\n",
      "MSE on the training set: 3.208391910817781e-06\n",
      "Epoch: 3000\n",
      "sum of abs dWs for layer 0: 0.0001842297627775733 and sum of dbs: 0.1981933267501848\n",
      "sum of Ws for layer 0: 86.42321018575049 and sum of bs: 0.0015993065408704261\n",
      "sum of abs dWs for layer 1: 0.004847812315828139 and sum of dbs: 0.1372154625734192\n",
      "sum of Ws for layer 1: 321.3249727050628 and sum of bs: 0.0016885549446669108\n",
      "sum of abs dWs for layer 2: 0.0022175863686541564 and sum of dbs: 0.2502249885077849\n",
      "sum of Ws for layer 2: 340.8626858060935 and sum of bs: 0.007883004610879641\n",
      "sum of abs dWs for layer 3: 0.0010764957535209215 and sum of dbs: 0.09022733762517597\n",
      "sum of Ws for layer 3: 7.500588145042028 and sum of bs: 0.002385154747031616\n",
      "MSE on the training set: 3.208388752215695e-06\n",
      "Epoch: 3010\n",
      "sum of abs dWs for layer 0: 0.00018406247061647762 and sum of dbs: 0.19819323172395045\n",
      "sum of Ws for layer 0: 86.42321018575167 and sum of bs: 0.0015993050310825882\n",
      "sum of abs dWs for layer 1: 0.004843341043531063 and sum of dbs: 0.1372153967093522\n",
      "sum of Ws for layer 1: 321.3249727050773 and sum of bs: 0.0016885538167929445\n",
      "sum of abs dWs for layer 2: 0.002215253015210621 and sum of dbs: 0.250224868468179\n",
      "sum of Ws for layer 2: 340.86268580610033 and sum of bs: 0.007883002252474992\n",
      "sum of abs dWs for layer 3: 0.0010752454406534038 and sum of dbs: 0.09022729434319952\n",
      "sum of Ws for layer 3: 7.500588145045451 and sum of bs: 0.002385153844758477\n",
      "MSE on the training set: 3.208385593613221e-06\n",
      "Epoch: 3020\n",
      "sum of abs dWs for layer 0: 0.00018391567194890856 and sum of dbs: 0.19819313638889716\n",
      "sum of Ws for layer 0: 86.4232101857529 and sum of bs: 0.001599303521295476\n",
      "sum of abs dWs for layer 1: 0.004839417533540799 and sum of dbs: 0.13721533063823987\n",
      "sum of Ws for layer 1: 321.32497270509174 and sum of bs: 0.00168855268891952\n",
      "sum of abs dWs for layer 2: 0.002213205581368174 and sum of dbs: 0.25022474804453354\n",
      "sum of Ws for layer 2: 340.86268580610727 and sum of bs: 0.007882999894071476\n",
      "sum of abs dWs for layer 3: 0.0010741483626477998 and sum of dbs: 0.09022725092250729\n",
      "sum of Ws for layer 3: 7.500588145048866 and sum of bs: 0.0023851529424857715\n",
      "MSE on the training set: 3.2083824350074894e-06\n",
      "Epoch: 3030\n",
      "sum of abs dWs for layer 0: 0.00018378654910173624 and sum of dbs: 0.19819304078712896\n",
      "sum of Ws for layer 0: 86.4232101857541 and sum of bs: 0.001599302011509091\n",
      "sum of abs dWs for layer 1: 0.004835966473441358 and sum of dbs: 0.1372152643883107\n",
      "sum of Ws for layer 1: 321.3249727051061 and sum of bs: 0.0016885515610466388\n",
      "sum of abs dWs for layer 2: 0.002211404755731298 and sum of dbs: 0.2502246272892084\n",
      "sum of Ws for layer 2: 340.8626858061141 and sum of bs: 0.007882997535669099\n",
      "sum of abs dWs for layer 3: 0.0010731834510840823 and sum of dbs: 0.09022720738201168\n",
      "sum of Ws for layer 3: 7.500588145052275 and sum of bs: 0.002385152040213501\n",
      "MSE on the training set: 3.208379276422515e-06\n",
      "Epoch: 3040\n",
      "sum of abs dWs for layer 0: 0.00018367303347337381 and sum of dbs: 0.19819294494971257\n",
      "sum of Ws for layer 0: 86.42321018575531 and sum of bs: 0.001599300501723436\n",
      "sum of abs dWs for layer 1: 0.004832932572394102 and sum of dbs: 0.13721519798039317\n",
      "sum of Ws for layer 1: 321.32497270512056 and sum of bs: 0.0016885504331743025\n",
      "sum of abs dWs for layer 2: 0.002209821677744854 and sum of dbs: 0.25022450624083753\n",
      "sum of Ws for layer 2: 340.8626858061209 and sum of bs: 0.007882995177267859\n",
      "sum of abs dWs for layer 3: 0.001072335238519995 and sum of dbs: 0.09022716373566735\n",
      "sum of Ws for layer 3: 7.5005881450556755 and sum of bs: 0.0023851511379416675\n",
      "MSE on the training set: 3.208376117835154e-06\n",
      "Epoch: 3050\n",
      "sum of abs dWs for layer 0: 0.0001835732957218312 and sum of dbs: 0.1981928489052174\n",
      "sum of Ws for layer 0: 86.42321018575649 and sum of bs: 0.0015992989919385117\n",
      "sum of abs dWs for layer 1: 0.004830266934656918 and sum of dbs: 0.13721513143364156\n",
      "sum of Ws for layer 1: 321.324972705135 and sum of bs: 0.0016885493053025121\n",
      "sum of abs dWs for layer 2: 0.0022084308249046185 and sum of dbs: 0.2502243849349489\n",
      "sum of Ws for layer 2: 340.86268580612773 and sum of bs: 0.007882992818867762\n",
      "sum of abs dWs for layer 3: 0.0010715900465105455 and sum of dbs: 0.09022711999630711\n",
      "sum of Ws for layer 3: 7.500588145059073 and sum of bs: 0.0023851502356702706\n",
      "MSE on the training set: 3.2083729592229707e-06\n",
      "Epoch: 3060\n",
      "sum of abs dWs for layer 0: 0.00018348554838071112 and sum of dbs: 0.19819275268015035\n",
      "sum of Ws for layer 0: 86.42321018575771 and sum of bs: 0.0015992974821543201\n",
      "sum of abs dWs for layer 1: 0.004827921783774088 and sum of dbs: 0.13721506476582734\n",
      "sum of Ws for layer 1: 321.3249727051493 and sum of bs: 0.0016885481774312687\n",
      "sum of abs dWs for layer 2: 0.0022072072589958895 and sum of dbs: 0.2502242634045062\n",
      "sum of Ws for layer 2: 340.86268580613455 and sum of bs: 0.007882990460468807\n",
      "sum of abs dWs for layer 3: 0.0010709345097357232 and sum of dbs: 0.09022707617583747\n",
      "sum of Ws for layer 3: 7.500588145062465 and sum of bs: 0.0023851493333993117\n",
      "MSE on the training set: 3.208369800654665e-06\n",
      "Epoch: 3070\n",
      "sum of abs dWs for layer 0: 0.00018340836443113363 and sum of dbs: 0.1981926562950549\n",
      "sum of Ws for layer 0: 86.4232101857589 and sum of bs: 0.0015992959723708628\n",
      "sum of abs dWs for layer 1: 0.004825858977519357 and sum of dbs: 0.13721499799072379\n",
      "sum of Ws for layer 1: 321.32497270516376 and sum of bs: 0.0016885470495605738\n",
      "sum of abs dWs for layer 2: 0.002206131070626196 and sum of dbs: 0.2502241416750567\n",
      "sum of Ws for layer 2: 340.86268580614137 and sum of bs: 0.007882988102071\n",
      "sum of abs dWs for layer 3: 0.0010703579580256212 and sum of dbs: 0.09022703228348614\n",
      "sum of Ws for layer 3: 7.500588145065852 and sum of bs: 0.002385148431128791\n",
      "MSE on the training set: 3.2083666420771808e-06\n",
      "Epoch: 3080\n",
      "sum of abs dWs for layer 0: 0.0001833409173107152 and sum of dbs: 0.19819255976343605\n",
      "sum of Ws for layer 0: 86.42321018576011 and sum of bs: 0.0015992944625881401\n",
      "sum of abs dWs for layer 1: 0.004824056423040701 and sum of dbs: 0.1372149311173853\n",
      "sum of Ws for layer 1: 321.32497270517814 and sum of bs: 0.0016885459216904278\n",
      "sum of abs dWs for layer 2: 0.0022051907278298924 and sum of dbs: 0.25022401976339487\n",
      "sum of Ws for layer 2: 340.8626858061482 and sum of bs: 0.00788298574367434\n",
      "sum of abs dWs for layer 3: 0.0010698542109734848 and sum of dbs: 0.09022698832531939\n",
      "sum of Ws for layer 3: 7.500588145069236 and sum of bs: 0.00238514752885871\n",
      "MSE on the training set: 3.208363483481983e-06\n",
      "Epoch: 3090\n",
      "sum of abs dWs for layer 0: 0.00018328158705346995 and sum of dbs: 0.19819246311050667\n",
      "sum of Ws for layer 0: 86.42321018576132 and sum of bs: 0.0015992929528061532\n",
      "sum of abs dWs for layer 1: 0.004822470820935118 and sum of dbs: 0.1372148641627158\n",
      "sum of Ws for layer 1: 321.3249727051925 and sum of bs: 0.0016885447938208313\n",
      "sum of abs dWs for layer 2: 0.002204363629280697 and sum of dbs: 0.25022389770087505\n",
      "sum of Ws for layer 2: 340.862685806155 and sum of bs: 0.00788298338527883\n",
      "sum of abs dWs for layer 3: 0.0010694111556881828 and sum of dbs: 0.09022694431266237\n",
      "sum of Ws for layer 3: 7.500588145072616 and sum of bs: 0.0023851466265890687\n",
      "MSE on the training set: 3.208360324909472e-06\n",
      "Epoch: 3100\n",
      "sum of abs dWs for layer 0: 0.0001832291155588403 and sum of dbs: 0.1981923663532669\n",
      "sum of Ws for layer 0: 86.42321018576251 and sum of bs: 0.0015992914430249037\n",
      "sum of abs dWs for layer 1: 0.004821068543920783 and sum of dbs: 0.13721479713811274\n",
      "sum of Ws for layer 1: 321.32497270520685 and sum of bs: 0.0016885436659517853\n",
      "sum of abs dWs for layer 2: 0.0022036322223108113 and sum of dbs: 0.250223775508638\n",
      "sum of Ws for layer 2: 340.86268580616183 and sum of bs: 0.00788298102688447\n",
      "sum of abs dWs for layer 3: 0.0010690193850542372 and sum of dbs: 0.09022690025315122\n",
      "sum of Ws for layer 3: 7.500588145075995 and sum of bs: 0.002385145724319868\n",
      "MSE on the training set: 3.208357166325693e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3110\n",
      "sum of abs dWs for layer 0: 0.0001831832092155946 and sum of dbs: 0.19819226949751212\n",
      "sum of Ws for layer 0: 86.4232101857637 and sum of bs: 0.0015992899332443913\n",
      "sum of abs dWs for layer 1: 0.004819841744189679 and sum of dbs: 0.1372147300474618\n",
      "sum of Ws for layer 1: 321.3249727052212 and sum of bs: 0.0016885425380832903\n",
      "sum of abs dWs for layer 2: 0.002202992410534582 and sum of dbs: 0.2502236531938909\n",
      "sum of Ws for layer 2: 340.86268580616866 and sum of bs: 0.007882978668491261\n",
      "sum of abs dWs for layer 3: 0.0010686767036640695 and sum of dbs: 0.09022685614938916\n",
      "sum of Ws for layer 3: 7.500588145079369 and sum of bs: 0.002385144822051108\n",
      "MSE on the training set: 3.208354007756595e-06\n",
      "Epoch: 3120\n",
      "sum of abs dWs for layer 0: 0.0001831425638534751 and sum of dbs: 0.19819217256194294\n",
      "sum of Ws for layer 0: 86.42321018576493 and sum of bs: 0.0015992884234646172\n",
      "sum of abs dWs for layer 1: 0.004818755563111668 and sum of dbs: 0.13721466290330073\n",
      "sum of Ws for layer 1: 321.3249727052356 and sum of bs: 0.0016885414102153467\n",
      "sum of abs dWs for layer 2: 0.002202425998525355 and sum of dbs: 0.2502235307798893\n",
      "sum of Ws for layer 2: 340.8626858061755 and sum of bs: 0.007882976310099207\n",
      "sum of abs dWs for layer 3: 0.0010683733599196575 and sum of dbs: 0.09022681200977618\n",
      "sum of Ws for layer 3: 7.500588145082741 and sum of bs: 0.002385143919782789\n",
      "MSE on the training set: 3.2083508491886685e-06\n",
      "Epoch: 3130\n",
      "sum of abs dWs for layer 0: 0.00018310678875790387 and sum of dbs: 0.19819207555385374\n",
      "sum of Ws for layer 0: 86.42321018576611 and sum of bs: 0.0015992869136855822\n",
      "sum of abs dWs for layer 1: 0.0048177995574135235 and sum of dbs: 0.1372145957105199\n",
      "sum of Ws for layer 1: 321.32497270524993 and sum of bs: 0.001688540282347955\n",
      "sum of abs dWs for layer 2: 0.0022019275351357667 and sum of dbs: 0.2502234082757042\n",
      "sum of Ws for layer 2: 340.8626858061823 and sum of bs: 0.007882973951708306\n",
      "sum of abs dWs for layer 3: 0.0010681064323483282 and sum of dbs: 0.09022676783758873\n",
      "sum of Ws for layer 3: 7.500588145086112 and sum of bs: 0.002385143017514912\n",
      "MSE on the training set: 3.2083476906383404e-06\n",
      "Epoch: 3140\n",
      "sum of abs dWs for layer 0: 0.000183075319147661 and sum of dbs: 0.19819197848012846\n",
      "sum of Ws for layer 0: 86.42321018576732 and sum of bs: 0.0015992854039072867\n",
      "sum of abs dWs for layer 1: 0.00481695863130128 and sum of dbs: 0.13721452847373472\n",
      "sum of Ws for layer 1: 321.3249727052643 and sum of bs: 0.0016885391544811154\n",
      "sum of abs dWs for layer 2: 0.002201489140701415 and sum of dbs: 0.25022328568989666\n",
      "sum of Ws for layer 2: 340.8626858061891 and sum of bs: 0.00788297159331856\n",
      "sum of abs dWs for layer 3: 0.0010678716979452382 and sum of dbs: 0.09022672363591903\n",
      "sum of Ws for layer 3: 7.500588145089481 and sum of bs: 0.0023851421152474767\n",
      "MSE on the training set: 3.20834453205865e-06\n",
      "Epoch: 3150\n",
      "sum of abs dWs for layer 0: 0.00018304735067722997 and sum of dbs: 0.1981918813539243\n",
      "sum of Ws for layer 0: 86.42321018576853 and sum of bs: 0.0015992838941297307\n",
      "sum of abs dWs for layer 1: 0.004816211285741318 and sum of dbs: 0.13721446120176625\n",
      "sum of Ws for layer 1: 321.3249727052787 and sum of bs: 0.0016885380266148287\n",
      "sum of abs dWs for layer 2: 0.0022010995932103938 and sum of dbs: 0.25022316303882836\n",
      "sum of Ws for layer 2: 340.86268580619594 and sum of bs: 0.007882969234929968\n",
      "sum of abs dWs for layer 3: 0.0010676631424230752 and sum of dbs: 0.09022667941067701\n",
      "sum of Ws for layer 3: 7.500588145092849 and sum of bs: 0.0023851412129804834\n",
      "MSE on the training set: 3.2083413735164626e-06\n",
      "Epoch: 3160\n",
      "sum of abs dWs for layer 0: 0.00018302287304622881 and sum of dbs: 0.1981917841752432\n",
      "sum of Ws for layer 0: 86.42321018576972 and sum of bs: 0.0015992823843529152\n",
      "sum of abs dWs for layer 1: 0.004815557245423964 and sum of dbs: 0.13721439389461595\n",
      "sum of Ws for layer 1: 321.324972705293 and sum of bs: 0.001688536898749095\n",
      "sum of abs dWs for layer 2: 0.0022007587489526604 and sum of dbs: 0.2502230403225019\n",
      "sum of Ws for layer 2: 340.86268580620276 and sum of bs: 0.007882966876542535\n",
      "sum of abs dWs for layer 3: 0.001067480688755765 and sum of dbs: 0.09022663516186356\n",
      "sum of Ws for layer 3: 7.5005881450962155 and sum of bs: 0.0023851403107139325\n",
      "MSE on the training set: 3.2083382149597978e-06\n",
      "Epoch: 3170\n",
      "sum of abs dWs for layer 0: 0.00018300237599910474 and sum of dbs: 0.19819168693648426\n",
      "sum of Ws for layer 0: 86.42321018577091 and sum of bs: 0.00159928087457684\n",
      "sum of abs dWs for layer 1: 0.00481500960052257 and sum of dbs: 0.13721432654718785\n",
      "sum of Ws for layer 1: 321.3249727053074 and sum of bs: 0.0016885357708839141\n",
      "sum of abs dWs for layer 2: 0.0022004734407067 and sum of dbs: 0.25022291753146503\n",
      "sum of Ws for layer 2: 340.8626858062095 and sum of bs: 0.007882964518156257\n",
      "sum of abs dWs for layer 3: 0.001067327998890768 and sum of dbs: 0.09022659088606452\n",
      "sum of Ws for layer 3: 7.500588145099582 and sum of bs: 0.0023851394084478243\n",
      "MSE on the training set: 3.2083350564023334e-06\n",
      "Epoch: 3180\n",
      "sum of abs dWs for layer 0: 0.00018298392493651196 and sum of dbs: 0.19819158966736478\n",
      "sum of Ws for layer 0: 86.42321018577212 and sum of bs: 0.001599279364801506\n",
      "sum of abs dWs for layer 1: 0.004814516641919082 and sum of dbs: 0.13721425917940558\n",
      "sum of Ws for layer 1: 321.3249727053218 and sum of bs: 0.001688534643019287\n",
      "sum of abs dWs for layer 2: 0.002200216677476501 and sum of dbs: 0.25022279470267333\n",
      "sum of Ws for layer 2: 340.8626858062163 and sum of bs: 0.007882962159771136\n",
      "sum of abs dWs for layer 3: 0.0010671906073515176 and sum of dbs: 0.09022654659662833\n",
      "sum of Ws for layer 3: 7.500588145102945 and sum of bs: 0.002385138506182159\n",
      "MSE on the training set: 3.2083318978529324e-06\n",
      "Epoch: 3190\n",
      "sum of abs dWs for layer 0: 0.00018296736018291659 and sum of dbs: 0.1981914923688813\n",
      "sum of Ws for layer 0: 86.42321018577333 and sum of bs: 0.0015992778550269128\n",
      "sum of abs dWs for layer 1: 0.004814074101709377 and sum of dbs: 0.13721419179193742\n",
      "sum of Ws for layer 1: 321.32497270533605 and sum of bs: 0.0016885335151552135\n",
      "sum of abs dWs for layer 2: 0.0021999862315197766 and sum of dbs: 0.250222671837366\n",
      "sum of Ws for layer 2: 340.8626858062231 and sum of bs: 0.007882959801387174\n",
      "sum of abs dWs for layer 3: 0.0010670673202139587 and sum of dbs: 0.09022650229400264\n",
      "sum of Ws for layer 3: 7.50058814510631 and sum of bs: 0.0023851376039169363\n",
      "MSE on the training set: 3.2083287393166374e-06\n",
      "Epoch: 3200\n",
      "sum of abs dWs for layer 0: 0.00018295273610667754 and sum of dbs: 0.19819139504202243\n",
      "sum of Ws for layer 0: 86.4232101857745 and sum of bs: 0.0015992763452530613\n",
      "sum of abs dWs for layer 1: 0.004813683433076767 and sum of dbs: 0.1372141243854462\n",
      "sum of Ws for layer 1: 321.32497270535043 and sum of bs: 0.0016885323872916935\n",
      "sum of abs dWs for layer 2: 0.0021997828613512563 and sum of dbs: 0.25022254893677276\n",
      "sum of Ws for layer 2: 340.86268580622993 and sum of bs: 0.007882957443004369\n",
      "sum of abs dWs for layer 3: 0.0010669585439868233 and sum of dbs: 0.09022645797863152\n",
      "sum of Ws for layer 3: 7.500588145109673 and sum of bs: 0.0023851367016521573\n",
      "MSE on the training set: 3.2083255807660916e-06\n",
      "Epoch: 3210\n",
      "sum of abs dWs for layer 0: 0.00018294019259415778 and sum of dbs: 0.19819129768347024\n",
      "sum of Ws for layer 0: 86.42321018577573 and sum of bs: 0.0015992748354799512\n",
      "sum of abs dWs for layer 1: 0.004813348374997713 and sum of dbs: 0.13721405695770736\n",
      "sum of Ws for layer 1: 321.32497270536476 and sum of bs: 0.0016885312594287277\n",
      "sum of abs dWs for layer 2: 0.0021996085186359346 and sum of dbs: 0.2502224259967673\n",
      "sum of Ws for layer 2: 340.86268580623675 and sum of bs: 0.007882955084622723\n",
      "sum of abs dWs for layer 3: 0.001066865324642001 and sum of dbs: 0.09022641364902462\n",
      "sum of Ws for layer 3: 7.500588145113034 and sum of bs: 0.0023851357993878216\n",
      "MSE on the training set: 3.208322422221631e-06\n",
      "Epoch: 3220\n",
      "sum of abs dWs for layer 0: 0.00018292936280360467 and sum of dbs: 0.19819120029923581\n",
      "sum of Ws for layer 0: 86.42321018577691 and sum of bs: 0.0015992733257075835\n",
      "sum of abs dWs for layer 1: 0.00481305912229712 and sum of dbs: 0.13721398951275102\n",
      "sum of Ws for layer 1: 321.3249727053791 and sum of bs: 0.0016885301315663162\n",
      "sum of abs dWs for layer 2: 0.0021994580853032814 and sum of dbs: 0.25022230302482495\n",
      "sum of Ws for layer 2: 340.8626858062436 and sum of bs: 0.007882952726242235\n",
      "sum of abs dWs for layer 3: 0.0010667849192219526 and sum of dbs: 0.09022636930788197\n",
      "sum of Ws for layer 3: 7.500588145116396 and sum of bs: 0.002385134897123929\n",
      "MSE on the training set: 3.2083192637019425e-06\n",
      "Epoch: 3230\n",
      "sum of abs dWs for layer 0: 0.00018291980420378796 and sum of dbs: 0.1981911028961854\n",
      "sum of Ws for layer 0: 86.4232101857781 and sum of bs: 0.001599271815935957\n",
      "sum of abs dWs for layer 1: 0.004812803846744703 and sum of dbs: 0.13721392205518063\n",
      "sum of Ws for layer 1: 321.3249727053935 and sum of bs: 0.0016885290037044584\n",
      "sum of abs dWs for layer 2: 0.0021993253872781586 and sum of dbs: 0.2502221800294843\n",
      "sum of Ws for layer 2: 340.8626858062504 and sum of bs: 0.007882950367862905\n",
      "sum of abs dWs for layer 3: 0.0010667140188036122 and sum of dbs: 0.09022632495828783\n",
      "sum of Ws for layer 3: 7.500588145119756 and sum of bs: 0.00238513399486048\n",
      "MSE on the training set: 3.2083161051678588e-06\n",
      "Epoch: 3240\n",
      "sum of abs dWs for layer 0: 0.00018291094224458883 and sum of dbs: 0.19819100548220853\n",
      "sum of Ws for layer 0: 86.4232101857793 and sum of bs: 0.0015992703061650734\n",
      "sum of abs dWs for layer 1: 0.004812567191420176 and sum of dbs: 0.13721385459028568\n",
      "sum of Ws for layer 1: 321.3249727054078 and sum of bs: 0.0016885278758431556\n",
      "sum of abs dWs for layer 2: 0.0021992024085926054 and sum of dbs: 0.2502220570205566\n",
      "sum of Ws for layer 2: 340.86268580625716 and sum of bs: 0.007882948009484737\n",
      "sum of abs dWs for layer 3: 0.0010666483273396117 and sum of dbs: 0.09022628060378599\n",
      "sum of Ws for layer 3: 7.500588145123116 and sum of bs: 0.002385133092597474\n",
      "MSE on the training set: 3.2083129466393278e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3250\n",
      "sum of abs dWs for layer 0: 0.00018290298575036478 and sum of dbs: 0.19819090805520617\n",
      "sum of Ws for layer 0: 86.42321018578052 and sum of bs: 0.0015992687963949312\n",
      "sum of abs dWs for layer 1: 0.004812354737901026 and sum of dbs: 0.137213787116659\n",
      "sum of Ws for layer 1: 321.32497270542217 and sum of bs: 0.0016885267479824068\n",
      "sum of abs dWs for layer 2: 0.0021990920626933598 and sum of dbs: 0.25022193399543186\n",
      "sum of Ws for layer 2: 340.862685806264 and sum of bs: 0.007882945651107727\n",
      "sum of abs dWs for layer 3: 0.0010665894062528946 and sum of dbs: 0.09022623624343365\n",
      "sum of Ws for layer 3: 7.500588145126477 and sum of bs: 0.002385132190334912\n",
      "MSE on the training set: 3.2083097880986457e-06\n",
      "Epoch: 3260\n",
      "sum of abs dWs for layer 0: 0.000182896118922244 and sum of dbs: 0.1981908106109183\n",
      "sum of Ws for layer 0: 86.4232101857817 and sum of bs: 0.0015992672866255318\n",
      "sum of abs dWs for layer 1: 0.00481217140962736 and sum of dbs: 0.13721371963144435\n",
      "sum of Ws for layer 1: 321.3249727054365 and sum of bs: 0.0016885256201222124\n",
      "sum of abs dWs for layer 2: 0.002198996919531874 and sum of dbs: 0.250221810948812\n",
      "sum of Ws for layer 2: 340.86268580627075 and sum of bs: 0.007882943292731875\n",
      "sum of abs dWs for layer 3: 0.0010665386328729523 and sum of dbs: 0.09022619187531725\n",
      "sum of Ws for layer 3: 7.500588145129837 and sum of bs: 0.002385131288072794\n",
      "MSE on the training set: 3.20830662958876e-06\n",
      "Epoch: 3270\n",
      "sum of abs dWs for layer 0: 0.00018289046177561358 and sum of dbs: 0.19819071314951492\n",
      "sum of Ws for layer 0: 86.4232101857829 and sum of bs: 0.0015992657768568747\n",
      "sum of abs dWs for layer 1: 0.004812020414436513 and sum of dbs: 0.13721365213475575\n",
      "sum of Ws for layer 1: 321.3249727054508 and sum of bs: 0.001688524492262573\n",
      "sum of abs dWs for layer 2: 0.0021989186534946926 and sum of dbs: 0.25022168788090865\n",
      "sum of Ws for layer 2: 340.86268580627757 and sum of bs: 0.007882940934357183\n",
      "sum of abs dWs for layer 3: 0.00106649690456213 and sum of dbs: 0.09022614749951316\n",
      "sum of Ws for layer 3: 7.500588145133196 and sum of bs: 0.0023851303858111192\n",
      "MSE on the training set: 3.208303471062872e-06\n",
      "Epoch: 3280\n",
      "sum of abs dWs for layer 0: 0.00018288494430384515 and sum of dbs: 0.19819061568517835\n",
      "sum of Ws for layer 0: 86.4232101857841 and sum of bs: 0.0015992642670889605\n",
      "sum of abs dWs for layer 1: 0.00481187315256705 and sum of dbs: 0.13721358463610167\n",
      "sum of Ws for layer 1: 321.3249727054652 and sum of bs: 0.0016885233644034882\n",
      "sum of abs dWs for layer 2: 0.002198842336175456 and sum of dbs: 0.25022156480935864\n",
      "sum of Ws for layer 2: 340.8626858062844 and sum of bs: 0.00788293857598365\n",
      "sum of abs dWs for layer 3: 0.0010664562206436075 and sum of dbs: 0.09022610312239185\n",
      "sum of Ws for layer 3: 7.500588145136556 and sum of bs: 0.0023851294835498882\n",
      "MSE on the training set: 3.2083003125386453e-06\n",
      "Epoch: 3290\n",
      "sum of abs dWs for layer 0: 0.00018288007859423814 and sum of dbs: 0.19819051821150024\n",
      "sum of Ws for layer 0: 86.42321018578531 and sum of bs: 0.0015992627573217886\n",
      "sum of abs dWs for layer 1: 0.0048117433113853076 and sum of dbs: 0.1372135171311856\n",
      "sum of Ws for layer 1: 321.3249727054795 and sum of bs: 0.001688522236544958\n",
      "sum of abs dWs for layer 2: 0.0021987751120656268 and sum of dbs: 0.2502214417261925\n",
      "sum of Ws for layer 2: 340.8626858062912 and sum of bs: 0.00788293621761128\n",
      "sum of abs dWs for layer 3: 0.0010664204101153278 and sum of dbs: 0.0902260587410747\n",
      "sum of Ws for layer 3: 7.500588145139916 and sum of bs: 0.0023851285812891013\n",
      "MSE on the training set: 3.2082971540149136e-06\n",
      "Epoch: 3300\n",
      "sum of abs dWs for layer 0: 0.00018287591466158767 and sum of dbs: 0.19819042072688864\n",
      "sum of Ws for layer 0: 86.4232101857865 and sum of bs: 0.001599261247555359\n",
      "sum of abs dWs for layer 1: 0.004811632227711877 and sum of dbs: 0.13721344961894028\n",
      "sum of Ws for layer 1: 321.32497270549385 and sum of bs: 0.0016885211086869823\n",
      "sum of abs dWs for layer 2: 0.002198717678945761 and sum of dbs: 0.2502213186294307\n",
      "sum of Ws for layer 2: 340.8626858062979 and sum of bs: 0.007882933859240068\n",
      "sum of abs dWs for layer 3: 0.0010663898469378728 and sum of dbs: 0.09022601435484678\n",
      "sum of Ws for layer 3: 7.500588145143274 and sum of bs: 0.002385127679028758\n",
      "MSE on the training set: 3.208293995528972e-06\n",
      "Epoch: 3310\n",
      "sum of abs dWs for layer 0: 0.0001828724478695021 and sum of dbs: 0.19819032323236693\n",
      "sum of Ws for layer 0: 86.4232101857877 and sum of bs: 0.0015992597377896725\n",
      "sum of abs dWs for layer 1: 0.00481153977762917 and sum of dbs: 0.13721338210005185\n",
      "sum of Ws for layer 1: 321.32497270550823 and sum of bs: 0.0016885199808295618\n",
      "sum of abs dWs for layer 2: 0.002198669972148614 and sum of dbs: 0.25022119552034583\n",
      "sum of Ws for layer 2: 340.8626858063048 and sum of bs: 0.007882931500870015\n",
      "sum of abs dWs for layer 3: 0.0010663644964523504 and sum of dbs: 0.09022596996416768\n",
      "sum of Ws for layer 3: 7.500588145146632 and sum of bs: 0.0023851267767688584\n",
      "MSE on the training set: 3.2082908369973987e-06\n",
      "Epoch: 3320\n",
      "sum of abs dWs for layer 0: 0.00018286946512143955 and sum of dbs: 0.19819022573046804\n",
      "sum of Ws for layer 0: 86.42321018578889 and sum of bs: 0.001599258228024729\n",
      "sum of abs dWs for layer 1: 0.004811460265364716 and sum of dbs: 0.13721331457621846\n",
      "sum of Ws for layer 1: 321.32497270552255 and sum of bs: 0.001688518852972696\n",
      "sum of abs dWs for layer 2: 0.0021986290185927403 and sum of dbs: 0.2502210724020878\n",
      "sum of Ws for layer 2: 340.86268580631156 and sum of bs: 0.007882929142501124\n",
      "sum of abs dWs for layer 3: 0.0010663427652796633 and sum of dbs: 0.09022592557017517\n",
      "sum of Ws for layer 3: 7.500588145149991 and sum of bs: 0.002385125874509403\n",
      "MSE on the training set: 3.2082876785128147e-06\n",
      "Epoch: 3330\n",
      "sum of abs dWs for layer 0: 0.00018286661539909486 and sum of dbs: 0.19819012822579807\n",
      "sum of Ws for layer 0: 86.42321018579007 and sum of bs: 0.0015992567182605275\n",
      "sum of abs dWs for layer 1: 0.004811384308696111 and sum of dbs: 0.1372132470505283\n",
      "sum of Ws for layer 1: 321.32497270553694 and sum of bs: 0.001688517725116385\n",
      "sum of abs dWs for layer 2: 0.002198589920979663 and sum of dbs: 0.25022094928038463\n",
      "sum of Ws for layer 2: 340.8626858063184 and sum of bs: 0.007882926784133392\n",
      "sum of abs dWs for layer 3: 0.0010663220287764614 and sum of dbs: 0.09022588117493827\n",
      "sum of Ws for layer 3: 7.50058814515335 and sum of bs: 0.0023851249722503915\n",
      "MSE on the training set: 3.2082845199918033e-06\n",
      "Epoch: 3340\n",
      "sum of abs dWs for layer 0: 0.00018286477846998856 and sum of dbs: 0.19819003070718136\n",
      "sum of Ws for layer 0: 86.42321018579129 and sum of bs: 0.0015992552084970698\n",
      "sum of abs dWs for layer 1: 0.004811335422555594 and sum of dbs: 0.13721317951548864\n",
      "sum of Ws for layer 1: 321.32497270555126 and sum of bs: 0.0016885165972606287\n",
      "sum of abs dWs for layer 2: 0.0021985649535706865 and sum of dbs: 0.25022082614133845\n",
      "sum of Ws for layer 2: 340.86268580632515 and sum of bs: 0.007882924425766822\n",
      "sum of abs dWs for layer 3: 0.0010663088651678525 and sum of dbs: 0.09022583677343698\n",
      "sum of Ws for layer 3: 7.500588145156706 and sum of bs: 0.002385124069991824\n",
      "MSE on the training set: 3.2082813614889755e-06\n",
      "Epoch: 3350\n",
      "sum of abs dWs for layer 0: 0.00018286278111985516 and sum of dbs: 0.1981899331902504\n",
      "sum of Ws for layer 0: 86.42321018579247 and sum of bs: 0.0015992536987343544\n",
      "sum of abs dWs for layer 1: 0.0048112822485919145 and sum of dbs: 0.13721311198158026\n",
      "sum of Ws for layer 1: 321.3249727055656 and sum of bs: 0.0016885154694054277\n",
      "sum of abs dWs for layer 2: 0.002198537748020219 and sum of dbs: 0.25022070300438976\n",
      "sum of Ws for layer 2: 340.86268580633197 and sum of bs: 0.007882922067401411\n",
      "sum of abs dWs for layer 3: 0.0010662945020592787 and sum of dbs: 0.09022579237269325\n",
      "sum of Ws for layer 3: 7.500588145160065 and sum of bs: 0.0023851231677337005\n",
      "MSE on the training set: 3.2082782030080164e-06\n",
      "Epoch: 3360\n",
      "sum of abs dWs for layer 0: 0.00018286104247304053 and sum of dbs: 0.19818983566986237\n",
      "sum of Ws for layer 0: 86.42321018579366 and sum of bs: 0.0015992521889723822\n",
      "sum of abs dWs for layer 1: 0.004811235989403509 and sum of dbs: 0.1372130444453552\n",
      "sum of Ws for layer 1: 321.32497270557985 and sum of bs: 0.0016885143415507817\n",
      "sum of abs dWs for layer 2: 0.002198514151818477 and sum of dbs: 0.25022057986314267\n",
      "sum of Ws for layer 2: 340.8626858063388 and sum of bs: 0.007882919709037163\n",
      "sum of abs dWs for layer 3: 0.0010662820733356314 and sum of dbs: 0.09022574797039694\n",
      "sum of Ws for layer 3: 7.500588145163424 and sum of bs: 0.0023851222654760213\n",
      "MSE on the training set: 3.2082750445066963e-06\n",
      "Epoch: 3370\n",
      "sum of abs dWs for layer 0: 0.00018285935138338425 and sum of dbs: 0.19818973814846214\n",
      "sum of Ws for layer 0: 86.42321018579487 and sum of bs: 0.0015992506792111532\n",
      "sum of abs dWs for layer 1: 0.00481119100135263 and sum of dbs: 0.13721297690845255\n",
      "sum of Ws for layer 1: 321.32497270559423 and sum of bs: 0.0016885132136966903\n",
      "sum of abs dWs for layer 2: 0.0021984912191267032 and sum of dbs: 0.2502204567206378\n",
      "sum of Ws for layer 2: 340.86268580634555 and sum of bs: 0.007882917350674071\n",
      "sum of abs dWs for layer 3: 0.0010662700002052182 and sum of dbs: 0.09022570356764625\n",
      "sum of Ws for layer 3: 7.500588145166781 and sum of bs: 0.002385121363218786\n",
      "MSE on the training set: 3.2082718860040634e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3380\n",
      "sum of abs dWs for layer 0: 0.00018285770187237286 and sum of dbs: 0.198189640626284\n",
      "sum of Ws for layer 0: 86.42321018579608 and sum of bs: 0.0015992491694506667\n",
      "sum of abs dWs for layer 1: 0.004811147124643752 and sum of dbs: 0.13721290937102945\n",
      "sum of Ws for layer 1: 321.3249727056086 and sum of bs: 0.0016885120858431543\n",
      "sum of abs dWs for layer 2: 0.002198468866530675 and sum of dbs: 0.2502203335771665\n",
      "sum of Ws for layer 2: 340.8626858063524 and sum of bs: 0.007882914992312142\n",
      "sum of abs dWs for layer 3: 0.0010662582379711721 and sum of dbs: 0.09022565916454645\n",
      "sum of Ws for layer 3: 7.500588145170139 and sum of bs: 0.0023851204609619944\n",
      "MSE on the training set: 3.20826872753641e-06\n",
      "Epoch: 3390\n",
      "sum of abs dWs for layer 0: 0.00018285664310059514 and sum of dbs: 0.19818954309661896\n",
      "sum of Ws for layer 0: 86.42321018579726 and sum of bs: 0.0015992476596909237\n",
      "sum of abs dWs for layer 1: 0.004811119037560972 and sum of dbs: 0.1372128418285878\n",
      "sum of Ws for layer 1: 321.32497270562294 and sum of bs: 0.0016885109579901731\n",
      "sum of abs dWs for layer 2: 0.0021984547557507435 and sum of dbs: 0.2502202104243856\n",
      "sum of Ws for layer 2: 340.8626858063592 and sum of bs: 0.007882912633951373\n",
      "sum of abs dWs for layer 3: 0.0010662508928294813 and sum of dbs: 0.09022561475808395\n",
      "sum of Ws for layer 3: 7.500588145173498 and sum of bs: 0.002385119558705647\n",
      "MSE on the training set: 3.2082655690455418e-06\n",
      "Epoch: 3400\n",
      "sum of abs dWs for layer 0: 0.00018285536996252402 and sum of dbs: 0.19818944556827003\n",
      "sum of Ws for layer 0: 86.42321018579848 and sum of bs: 0.0015992461499319231\n",
      "sum of abs dWs for layer 1: 0.004811085220769597 and sum of dbs: 0.1372127742870296\n",
      "sum of Ws for layer 1: 321.32497270563726 and sum of bs: 0.0016885098301377469\n",
      "sum of abs dWs for layer 2: 0.002198437654208573 and sum of dbs: 0.25022008727324224\n",
      "sum of Ws for layer 2: 340.862685806366 and sum of bs: 0.007882910275591765\n",
      "sum of abs dWs for layer 3: 0.001066241944830614 and sum of dbs: 0.09022557035221293\n",
      "sum of Ws for layer 3: 7.500588145176854 and sum of bs: 0.0023851186564497434\n",
      "MSE on the training set: 3.2082624105574113e-06\n",
      "Epoch: 3410\n",
      "sum of abs dWs for layer 0: 0.0001828542557139283 and sum of dbs: 0.19818934803887928\n",
      "sum of Ws for layer 0: 86.42321018579966 and sum of bs: 0.001599244640173666\n",
      "sum of abs dWs for layer 1: 0.0048110556508685525 and sum of dbs: 0.13721270674477398\n",
      "sum of Ws for layer 1: 321.3249727056516 and sum of bs: 0.0016885087022858757\n",
      "sum of abs dWs for layer 2: 0.002198422769436688 and sum of dbs: 0.2502199641208042\n",
      "sum of Ws for layer 2: 340.8626858063728 and sum of bs: 0.007882907917233317\n",
      "sum of abs dWs for layer 3: 0.001066234184881183 and sum of dbs: 0.09022552594587421\n",
      "sum of Ws for layer 3: 7.500588145180211 and sum of bs: 0.0023851177541942847\n",
      "MSE on the training set: 3.2082592520970486e-06\n",
      "Epoch: 3420\n",
      "sum of abs dWs for layer 0: 0.00018285333489488474 and sum of dbs: 0.19818925050585434\n",
      "sum of Ws for layer 0: 86.42321018580087 and sum of bs: 0.0015992431304161516\n",
      "sum of abs dWs for layer 1: 0.004811031251070124 and sum of dbs: 0.13721263920008292\n",
      "sum of Ws for layer 1: 321.3249727056659 and sum of bs: 0.00168850757443456\n",
      "sum of abs dWs for layer 2: 0.0021984105833480544 and sum of dbs: 0.25021984096384775\n",
      "sum of Ws for layer 2: 340.86268580637955 and sum of bs: 0.007882905558876031\n",
      "sum of abs dWs for layer 3: 0.0010662278712517525 and sum of dbs: 0.0902254815379034\n",
      "sum of Ws for layer 3: 7.5005881451835705 and sum of bs: 0.002385116851939269\n",
      "MSE on the training set: 3.208256093618044e-06\n",
      "Epoch: 3430\n",
      "sum of abs dWs for layer 0: 0.00018285274019925042 and sum of dbs: 0.19818915296890963\n",
      "sum of Ws for layer 0: 86.42321018580206 and sum of bs: 0.0015992416206593804\n",
      "sum of abs dWs for layer 1: 0.004811015568092855 and sum of dbs: 0.13721257165276485\n",
      "sum of Ws for layer 1: 321.32497270568024 and sum of bs: 0.0016885064465837988\n",
      "sum of abs dWs for layer 2: 0.002198402947234597 and sum of dbs: 0.25021971780201757\n",
      "sum of Ws for layer 2: 340.86268580638637 and sum of bs: 0.007882903200519905\n",
      "sum of abs dWs for layer 3: 0.0010662239961229667 and sum of dbs: 0.09022543712817216\n",
      "sum of Ws for layer 3: 7.500588145186927 and sum of bs: 0.0023851159496846982\n",
      "MSE on the training set: 3.208252935136347e-06\n",
      "Epoch: 3440\n",
      "sum of abs dWs for layer 0: 0.0001828517665532914 and sum of dbs: 0.198189055437025\n",
      "sum of Ws for layer 0: 86.42321018580326 and sum of bs: 0.001599240110903352\n",
      "sum of abs dWs for layer 1: 0.004810989756303484 and sum of dbs: 0.13721250410884045\n",
      "sum of Ws for layer 1: 321.3249727056946 and sum of bs: 0.0016885053187335928\n",
      "sum of abs dWs for layer 2: 0.002198390024113428 and sum of dbs: 0.2502195946464811\n",
      "sum of Ws for layer 2: 340.8626858063932 and sum of bs: 0.007882900842164938\n",
      "sum of abs dWs for layer 3: 0.001066217287492827 and sum of dbs: 0.09022539272071417\n",
      "sum of Ws for layer 3: 7.500588145190284 and sum of bs: 0.0023851150474305713\n",
      "MSE on the training set: 3.208249776689979e-06\n",
      "Epoch: 3450\n",
      "sum of abs dWs for layer 0: 0.00018285103249671142 and sum of dbs: 0.19818895790166055\n",
      "sum of Ws for layer 0: 86.42321018580445 and sum of bs: 0.001599238601148067\n",
      "sum of abs dWs for layer 1: 0.004810970348403655 and sum of dbs: 0.13721243656258403\n",
      "sum of Ws for layer 1: 321.32497270570894 and sum of bs: 0.0016885041908839419\n",
      "sum of abs dWs for layer 2: 0.002198380443680859 and sum of dbs: 0.25021947148661805\n",
      "sum of Ws for layer 2: 340.86268580639995 and sum of bs: 0.007882898483811133\n",
      "sum of abs dWs for layer 3: 0.0010662123703318264 and sum of dbs: 0.09022534831169336\n",
      "sum of Ws for layer 3: 7.5005881451936425 and sum of bs: 0.002385114145176888\n",
      "MSE on the training set: 3.2082466182072935e-06\n",
      "Epoch: 3460\n",
      "sum of abs dWs for layer 0: 0.00018285033051820243 and sum of dbs: 0.1981888603656349\n",
      "sum of Ws for layer 0: 86.42321018580567 and sum of bs: 0.0015992370913935252\n",
      "sum of abs dWs for layer 1: 0.004810951797907558 and sum of dbs: 0.13721236901588535\n",
      "sum of Ws for layer 1: 321.3249727057233 and sum of bs: 0.001688503063034846\n",
      "sum of abs dWs for layer 2: 0.0021983713107896234 and sum of dbs: 0.2502193483259336\n",
      "sum of Ws for layer 2: 340.8626858064068 and sum of bs: 0.007882896125458488\n",
      "sum of abs dWs for layer 3: 0.0010662076930241552 and sum of dbs: 0.09022530390237585\n",
      "sum of Ws for layer 3: 7.500588145196999 and sum of bs: 0.00238511324292365\n",
      "MSE on the training set: 3.2082434597460957e-06\n",
      "Epoch: 3470\n",
      "sum of abs dWs for layer 0: 0.00018284986516621513 and sum of dbs: 0.19818876282680262\n",
      "sum of Ws for layer 0: 86.42321018580685 and sum of bs: 0.0015992355816397262\n",
      "sum of abs dWs for layer 1: 0.004810939572098089 and sum of dbs: 0.1372123014673061\n",
      "sum of Ws for layer 1: 321.32497270573765 and sum of bs: 0.0016885019351863052\n",
      "sum of abs dWs for layer 2: 0.002198365479241937 and sum of dbs: 0.2502192251617598\n",
      "sum of Ws for layer 2: 340.8626858064136 and sum of bs: 0.007882893767107005\n",
      "sum of abs dWs for layer 3: 0.0010662047850295093 and sum of dbs: 0.09022525949179794\n",
      "sum of Ws for layer 3: 7.500588145200357 and sum of bs: 0.002385112340670855\n",
      "MSE on the training set: 3.20824030128772e-06\n",
      "Epoch: 3480\n",
      "sum of abs dWs for layer 0: 0.0001828493380194227 and sum of dbs: 0.19818866528850138\n",
      "sum of Ws for layer 0: 86.42321018580805 and sum of bs: 0.0015992340718866706\n",
      "sum of abs dWs for layer 1: 0.004810925694601156 and sum of dbs: 0.137212233919084\n",
      "sum of Ws for layer 1: 321.324972705752 and sum of bs: 0.0016885008073383194\n",
      "sum of abs dWs for layer 2: 0.002198358785550211 and sum of dbs: 0.2502191019982475\n",
      "sum of Ws for layer 2: 340.86268580642036 and sum of bs: 0.00788289140875668\n",
      "sum of abs dWs for layer 3: 0.001066201414979449 and sum of dbs: 0.0902252150814589\n",
      "sum of Ws for layer 3: 7.500588145203714 and sum of bs: 0.0023851114384185043\n",
      "MSE on the training set: 3.2082371428277954e-06\n",
      "Epoch: 3490\n",
      "sum of abs dWs for layer 0: 0.00018284871187645215 and sum of dbs: 0.19818856775146493\n",
      "sum of Ws for layer 0: 86.42321018580925 and sum of bs: 0.0015992325621343576\n",
      "sum of abs dWs for layer 1: 0.004810909171082132 and sum of dbs: 0.1372121663717108\n",
      "sum of Ws for layer 1: 321.3249727057663 and sum of bs: 0.001688499679490889\n",
      "sum of abs dWs for layer 2: 0.0021983507106971764 and sum of dbs: 0.2502189788363088\n",
      "sum of Ws for layer 2: 340.8626858064272 and sum of bs: 0.007882889050407517\n",
      "sum of abs dWs for layer 3: 0.0010661973047121007 and sum of dbs: 0.09022517067168824\n",
      "sum of Ws for layer 3: 7.500588145207072 and sum of bs: 0.002385110536166598\n",
      "MSE on the training set: 3.208233984391445e-06\n",
      "Epoch: 3500\n",
      "sum of abs dWs for layer 0: 0.00018284830884932713 and sum of dbs: 0.1981884702115052\n",
      "sum of Ws for layer 0: 86.42321018581043 and sum of bs: 0.0015992310523827882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of abs dWs for layer 1: 0.004810898611129251 and sum of dbs: 0.1372120988223788\n",
      "sum of Ws for layer 1: 321.3249727057807 and sum of bs: 0.001688498551644013\n",
      "sum of abs dWs for layer 2: 0.0021983457486847674 and sum of dbs: 0.25021885567073576\n",
      "sum of Ws for layer 2: 340.862685806434 and sum of bs: 0.007882886692059516\n",
      "sum of abs dWs for layer 3: 0.0010661948627289104 and sum of dbs: 0.09022512626060479\n",
      "sum of Ws for layer 3: 7.500588145210429 and sum of bs: 0.002385109633915135\n",
      "MSE on the training set: 3.2082308259373677e-06\n",
      "Epoch: 3510\n",
      "sum of abs dWs for layer 0: 0.0001828477666097147 and sum of dbs: 0.1981883726733579\n",
      "sum of Ws for layer 0: 86.42321018581164 and sum of bs: 0.0015992295426319617\n",
      "sum of abs dWs for layer 1: 0.004810884330227278 and sum of dbs: 0.13721203127426299\n",
      "sum of Ws for layer 1: 321.32497270579495 and sum of bs: 0.0016884974237976926\n",
      "sum of abs dWs for layer 2: 0.002198338844423011 and sum of dbs: 0.2502187325074175\n",
      "sum of Ws for layer 2: 340.8626858064408 and sum of bs: 0.007882884333712675\n",
      "sum of abs dWs for layer 3: 0.0010661913798269596 and sum of dbs: 0.09022508185033574\n",
      "sum of Ws for layer 3: 7.500588145213786 and sum of bs: 0.0023851087316641172\n",
      "MSE on the training set: 3.2082276674910882e-06\n",
      "Epoch: 3520\n",
      "sum of abs dWs for layer 0: 0.00018284732207081525 and sum of dbs: 0.19818827513390475\n",
      "sum of Ws for layer 0: 86.42321018581283 and sum of bs: 0.0015992280328818783\n",
      "sum of abs dWs for layer 1: 0.004810872660724959 and sum of dbs: 0.13721196372527275\n",
      "sum of Ws for layer 1: 321.32497270580933 and sum of bs: 0.0016884962959519273\n",
      "sum of abs dWs for layer 2: 0.00219833330325601 and sum of dbs: 0.25021860934247625\n",
      "sum of Ws for layer 2: 340.86268580644764 and sum of bs: 0.007882881975366993\n",
      "sum of abs dWs for layer 3: 0.0010661886274536856 and sum of dbs: 0.09022503743948043\n",
      "sum of Ws for layer 3: 7.500588145217144 and sum of bs: 0.0023851078294135426\n",
      "MSE on the training set: 3.208224509044646e-06\n",
      "Epoch: 3530\n",
      "sum of abs dWs for layer 0: 0.0001828470122697423 and sum of dbs: 0.1981881775931952\n",
      "sum of Ws for layer 0: 86.42321018581404 and sum of bs: 0.0015992265231325378\n",
      "sum of abs dWs for layer 1: 0.004810864592571405 and sum of dbs: 0.13721189617544127\n",
      "sum of Ws for layer 1: 321.32497270582365 and sum of bs: 0.0016884951681067168\n",
      "sum of abs dWs for layer 2: 0.0021983296419047244 and sum of dbs: 0.25021848617597353\n",
      "sum of Ws for layer 2: 340.86268580645435 and sum of bs: 0.007882879617022474\n",
      "sum of abs dWs for layer 3: 0.0010661868825448755 and sum of dbs: 0.09022499302806107\n",
      "sum of Ws for layer 3: 7.500588145220502 and sum of bs: 0.0023851069271634126\n",
      "MSE on the training set: 3.208221350600623e-06\n",
      "Epoch: 3540\n",
      "sum of abs dWs for layer 0: 0.00018284666548705892 and sum of dbs: 0.19818808005272415\n",
      "sum of Ws for layer 0: 86.42321018581524 and sum of bs: 0.0015992250133839406\n",
      "sum of abs dWs for layer 1: 0.004810855535959934 and sum of dbs: 0.13721182862577067\n",
      "sum of Ws for layer 1: 321.32497270583804 and sum of bs: 0.001688494040262062\n",
      "sum of abs dWs for layer 2: 0.002198325464596989 and sum of dbs: 0.25021836300976824\n",
      "sum of Ws for layer 2: 340.8626858064612 and sum of bs: 0.007882877258679113\n",
      "sum of abs dWs for layer 3: 0.0010661848611154164 and sum of dbs: 0.09022494861674912\n",
      "sum of Ws for layer 3: 7.500588145223859 and sum of bs: 0.0023851060249137266\n",
      "MSE on the training set: 3.2082181921742427e-06\n",
      "Epoch: 3550\n",
      "sum of abs dWs for layer 0: 0.0001828462634740317 and sum of dbs: 0.19818798251292857\n",
      "sum of Ws for layer 0: 86.42321018581644 and sum of bs: 0.001599223503636086\n",
      "sum of abs dWs for layer 1: 0.0048108450031130595 and sum of dbs: 0.13721176107655403\n",
      "sum of Ws for layer 1: 321.32497270585236 and sum of bs: 0.0016884929124179617\n",
      "sum of abs dWs for layer 2: 0.0021983205167328797 and sum of dbs: 0.25021823984440394\n",
      "sum of Ws for layer 2: 340.862685806468 and sum of bs: 0.007882874900336914\n",
      "sum of abs dWs for layer 3: 0.0010661824267151523 and sum of dbs: 0.09022490420574088\n",
      "sum of Ws for layer 3: 7.500588145227216 and sum of bs: 0.0023851051226644842\n",
      "MSE on the training set: 3.2082150337236586e-06\n",
      "Epoch: 3560\n",
      "sum of abs dWs for layer 0: 0.0001828459201922956 and sum of dbs: 0.19818788497219791\n",
      "sum of Ws for layer 0: 86.42321018581762 and sum of bs: 0.001599221993888975\n",
      "sum of abs dWs for layer 1: 0.0048108360400673025 and sum of dbs: 0.13721169352671153\n",
      "sum of Ws for layer 1: 321.3249727058667 and sum of bs: 0.0016884917845744166\n",
      "sum of abs dWs for layer 2: 0.002198316388271503 and sum of dbs: 0.2502181166778777\n",
      "sum of Ws for layer 2: 340.8626858064748 and sum of bs: 0.007882872541995878\n",
      "sum of abs dWs for layer 3: 0.001066180431463081 and sum of dbs: 0.09022485979431291\n",
      "sum of Ws for layer 3: 7.500588145230574 and sum of bs: 0.002385104220415687\n",
      "MSE on the training set: 3.208211875298508e-06\n",
      "Epoch: 3570\n",
      "sum of abs dWs for layer 0: 0.00018284555951805244 and sum of dbs: 0.19818778743163057\n",
      "sum of Ws for layer 0: 86.42321018581883 and sum of bs: 0.0015992204841426072\n",
      "sum of abs dWs for layer 1: 0.004810826612152831 and sum of dbs: 0.1372116259769796\n",
      "sum of Ws for layer 1: 321.324972705881 and sum of bs: 0.0016884906567314266\n",
      "sum of abs dWs for layer 2: 0.002198312017152771 and sum of dbs: 0.2502179935115554\n",
      "sum of Ws for layer 2: 340.8626858064816 and sum of bs: 0.007882870183656\n",
      "sum of abs dWs for layer 3: 0.001066178306163324 and sum of dbs: 0.0902248153829586\n",
      "sum of Ws for layer 3: 7.500588145233932 and sum of bs: 0.002385103318167333\n",
      "MSE on the training set: 3.2082087168725235e-06\n",
      "Epoch: 3580\n",
      "sum of abs dWs for layer 0: 0.00018284528602712427 and sum of dbs: 0.19818768989004015\n",
      "sum of Ws for layer 0: 86.42321018582004 and sum of bs: 0.0015992189743969823\n",
      "sum of abs dWs for layer 1: 0.004810819514523196 and sum of dbs: 0.13721155842656277\n",
      "sum of Ws for layer 1: 321.32497270589533 and sum of bs: 0.0016884895288889917\n",
      "sum of abs dWs for layer 2: 0.00219830886239549 and sum of dbs: 0.25021787034396187\n",
      "sum of Ws for layer 2: 340.8626858064884 and sum of bs: 0.007882867825317283\n",
      "sum of abs dWs for layer 3: 0.0010661768327540461 and sum of dbs: 0.09022477097114504\n",
      "sum of Ws for layer 3: 7.500588145237289 and sum of bs: 0.0023851024159194235\n",
      "MSE on the training set: 3.208205558439703e-06\n",
      "Epoch: 3590\n",
      "sum of abs dWs for layer 0: 0.00018284496730355332 and sum of dbs: 0.19818759234906982\n",
      "sum of Ws for layer 0: 86.42321018582123 and sum of bs: 0.0015992174646521006\n",
      "sum of abs dWs for layer 1: 0.0048108112078869515 and sum of dbs: 0.13721149087656276\n",
      "sum of Ws for layer 1: 321.32497270590966 and sum of bs: 0.001688488401047112\n",
      "sum of abs dWs for layer 2: 0.0021983050765571064 and sum of dbs: 0.25021774717714035\n",
      "sum of Ws for layer 2: 340.8626858064952 and sum of bs: 0.007882865466979727\n",
      "sum of abs dWs for layer 3: 0.0010661750211277874 and sum of dbs: 0.0902247265596103\n",
      "sum of Ws for layer 3: 7.500588145240646 and sum of bs: 0.0023851015136719584\n",
      "MSE on the training set: 3.2082024000259402e-06\n",
      "Epoch: 3600\n",
      "sum of abs dWs for layer 0: 0.00018284466470604974 and sum of dbs: 0.19818749480801667\n",
      "sum of Ws for layer 0: 86.42321018582241 and sum of bs: 0.001599215954907962\n",
      "sum of abs dWs for layer 1: 0.004810803332271576 and sum of dbs: 0.13721142332650832\n",
      "sum of Ws for layer 1: 321.324972705924 and sum of bs: 0.001688487273205787\n",
      "sum of abs dWs for layer 2: 0.002198301515706869 and sum of dbs: 0.2502176240102168\n",
      "sum of Ws for layer 2: 340.862685806502 and sum of bs: 0.00788286310864333\n",
      "sum of abs dWs for layer 3: 0.0010661733300775936 and sum of dbs: 0.09022468214803865\n",
      "sum of Ws for layer 3: 7.500588145244003 and sum of bs: 0.0023851006114249365\n",
      "MSE on the training set: 3.2081992415968462e-06\n",
      "Epoch: 3610\n",
      "sum of abs dWs for layer 0: 0.00018284440203287012 and sum of dbs: 0.19818739726623996\n",
      "sum of Ws for layer 0: 86.42321018582363 and sum of bs: 0.0015992144451645665\n",
      "sum of abs dWs for layer 1: 0.004810796523786625 and sum of dbs: 0.13721135577596982\n",
      "sum of Ws for layer 1: 321.32497270593836 and sum of bs: 0.0016884861453650177\n",
      "sum of abs dWs for layer 2: 0.0021982985118720894 and sum of dbs: 0.25021750084239436\n",
      "sum of Ws for layer 2: 340.8626858065088 and sum of bs: 0.007882860750308097\n",
      "sum of abs dWs for layer 3: 0.001066171937555263 and sum of dbs: 0.09022463773614231\n",
      "sum of Ws for layer 3: 7.50058814524736 and sum of bs: 0.0023850997091783596\n",
      "MSE on the training set: 3.2081960831873467e-06\n",
      "Epoch: 3620\n",
      "sum of abs dWs for layer 0: 0.00018284407556594016 and sum of dbs: 0.19818729972552043\n",
      "sum of Ws for layer 0: 86.42321018582481 and sum of bs: 0.001599212935421914\n",
      "sum of abs dWs for layer 1: 0.004810788010184835 and sum of dbs: 0.13721128822614112\n",
      "sum of Ws for layer 1: 321.32497270595275 and sum of bs: 0.0016884850175248032\n",
      "sum of abs dWs for layer 2: 0.002198294618003545 and sum of dbs: 0.25021737767588736\n",
      "sum of Ws for layer 2: 340.86268580651563 and sum of bs: 0.007882858391974024\n",
      "sum of abs dWs for layer 3: 0.0010661700680286508 and sum of dbs: 0.09022459332472109\n",
      "sum of Ws for layer 3: 7.500588145250718 and sum of bs: 0.0023850988069322268\n",
      "MSE on the training set: 3.20819292476881e-06\n",
      "Epoch: 3630\n",
      "sum of abs dWs for layer 0: 0.00018284378497346432 and sum of dbs: 0.19818720218455485\n",
      "sum of Ws for layer 0: 86.42321018582601 and sum of bs: 0.0015992114256800046\n",
      "sum of abs dWs for layer 1: 0.0048107804554488025 and sum of dbs: 0.13721122067614855\n",
      "sum of Ws for layer 1: 321.32497270596707 and sum of bs: 0.0016884838896851434\n",
      "sum of abs dWs for layer 2: 0.00219829122464207 and sum of dbs: 0.25021725450907545\n",
      "sum of Ws for layer 2: 340.86268580652245 and sum of bs: 0.007882856033641108\n",
      "sum of abs dWs for layer 3: 0.0010661684667430996 and sum of dbs: 0.09022454891318968\n",
      "sum of Ws for layer 3: 7.500588145254075 and sum of bs: 0.0023850979046865376\n",
      "MSE on the training set: 3.2081897663691886e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3640\n",
      "sum of abs dWs for layer 0: 0.00018284356631456704 and sum of dbs: 0.1981871046426701\n",
      "sum of Ws for layer 0: 86.42321018582722 and sum of bs: 0.001599209915938838\n",
      "sum of abs dWs for layer 1: 0.004810774823400895 and sum of dbs: 0.13721115312554072\n",
      "sum of Ws for layer 1: 321.3249727059814 and sum of bs: 0.0016884827618460392\n",
      "sum of abs dWs for layer 2: 0.0021982888348725266 and sum of dbs: 0.2502171313411213\n",
      "sum of Ws for layer 2: 340.8626858065292 and sum of bs: 0.007882853675309357\n",
      "sum of abs dWs for layer 3: 0.0010661674033176957 and sum of dbs: 0.09022450450124565\n",
      "sum of Ws for layer 3: 7.5005881452574314 and sum of bs: 0.002385097002441293\n",
      "MSE on the training set: 3.2081866079692724e-06\n",
      "Epoch: 3650\n",
      "sum of abs dWs for layer 0: 0.0001828432749439885 and sum of dbs: 0.19818700710176126\n",
      "sum of Ws for layer 0: 86.4232101858284 and sum of bs: 0.0015992084061984151\n",
      "sum of abs dWs for layer 1: 0.0048107672478674875 and sum of dbs: 0.1372110855755883\n",
      "sum of Ws for layer 1: 321.3249727059957 and sum of bs: 0.0016884816340074897\n",
      "sum of abs dWs for layer 2: 0.002198285430654369 and sum of dbs: 0.2502170081743818\n",
      "sum of Ws for layer 2: 340.862685806536 and sum of bs: 0.007882851316978764\n",
      "sum of abs dWs for layer 3: 0.0010661657962108759 and sum of dbs: 0.09022446008974032\n",
      "sum of Ws for layer 3: 7.5005881452607905 and sum of bs: 0.002385096100196492\n",
      "MSE on the training set: 3.2081834495636136e-06\n",
      "Epoch: 3660\n",
      "sum of abs dWs for layer 0: 0.00018284285631966015 and sum of dbs: 0.19818690956212703\n",
      "sum of Ws for layer 0: 86.42321018582959 and sum of bs: 0.001599206896458735\n",
      "sum of abs dWs for layer 1: 0.00481075627101813 and sum of dbs: 0.13721101802649155\n",
      "sum of Ws for layer 1: 321.32497270601004 and sum of bs: 0.0016884805061694957\n",
      "sum of abs dWs for layer 2: 0.0021982802510259196 and sum of dbs: 0.25021688500922823\n",
      "sum of Ws for layer 2: 340.8626858065428 and sum of bs: 0.007882848958649333\n",
      "sum of abs dWs for layer 3: 0.0010661632375980588 and sum of dbs: 0.0902244156788078\n",
      "sum of Ws for layer 3: 7.500588145264148 and sum of bs: 0.0023850951979521354\n",
      "MSE on the training set: 3.208180291163641e-06\n",
      "Epoch: 3670\n",
      "sum of abs dWs for layer 0: 0.00018284257685736532 and sum of dbs: 0.19818681202089733\n",
      "sum of Ws for layer 0: 86.4232101858308 and sum of bs: 0.0015992053867197984\n",
      "sum of abs dWs for layer 1: 0.004810749013782041 and sum of dbs: 0.13721095047632614\n",
      "sum of Ws for layer 1: 321.32497270602437 and sum of bs: 0.0016884793783320565\n",
      "sum of abs dWs for layer 2: 0.002198277012948802 and sum of dbs: 0.2502167618420916\n",
      "sum of Ws for layer 2: 340.8626858065496 and sum of bs: 0.007882846600321063\n",
      "sum of abs dWs for layer 3: 0.001066161719533804 and sum of dbs: 0.09022437126715892\n",
      "sum of Ws for layer 3: 7.500588145267504 and sum of bs: 0.002385094295708223\n",
      "MSE on the training set: 3.2081771327717094e-06\n",
      "Epoch: 3680\n",
      "sum of abs dWs for layer 0: 0.00018284237802477878 and sum of dbs: 0.19818671447908648\n",
      "sum of Ws for layer 0: 86.423210185832 and sum of bs: 0.0015992038769816045\n",
      "sum of abs dWs for layer 1: 0.00481074391166666 and sum of dbs: 0.13721088292577213\n",
      "sum of Ws for layer 1: 321.3249727060387 and sum of bs: 0.0016884782504951725\n",
      "sum of abs dWs for layer 2: 0.0021982748997935488 and sum of dbs: 0.2502166386742331\n",
      "sum of Ws for layer 2: 340.8626858065564 and sum of bs: 0.007882844241993953\n",
      "sum of abs dWs for layer 3: 0.001066160804355144 and sum of dbs: 0.0902243268552493\n",
      "sum of Ws for layer 3: 7.5005881452708625 and sum of bs: 0.0023850933934647543\n",
      "MSE on the training set: 3.208173974387356e-06\n",
      "Epoch: 3690\n",
      "sum of abs dWs for layer 0: 0.000182842167235761 and sum of dbs: 0.1981866169371819\n",
      "sum of Ws for layer 0: 86.4232101858332 and sum of bs: 0.001599202367244154\n",
      "sum of abs dWs for layer 1: 0.004810738489958937 and sum of dbs: 0.1372108153751564\n",
      "sum of Ws for layer 1: 321.324972706053 and sum of bs: 0.0016884771226588439\n",
      "sum of abs dWs for layer 2: 0.002198272619815157 and sum of dbs: 0.25021651550625895\n",
      "sum of Ws for layer 2: 340.8626858065632 and sum of bs: 0.007882841883668003\n",
      "sum of abs dWs for layer 3: 0.0010661597997694196 and sum of dbs: 0.09022428244329786\n",
      "sum of Ws for layer 3: 7.50058814527422 and sum of bs: 0.00238509249122173\n",
      "MSE on the training set: 3.2081708159927003e-06\n",
      "Epoch: 3700\n",
      "sum of abs dWs for layer 0: 0.00018284191849790262 and sum of dbs: 0.19818651939551188\n",
      "sum of Ws for layer 0: 86.4232101858344 and sum of bs: 0.0015992008575074464\n",
      "sum of abs dWs for layer 1: 0.004810732053935902 and sum of dbs: 0.13721074782469894\n",
      "sum of Ws for layer 1: 321.3249727060674 and sum of bs: 0.0016884759948230699\n",
      "sum of abs dWs for layer 2: 0.0021982698103958196 and sum of dbs: 0.2502163923385774\n",
      "sum of Ws for layer 2: 340.86268580657 and sum of bs: 0.007882839525343216\n",
      "sum of abs dWs for layer 3: 0.0010661585114379148 and sum of dbs: 0.09022423803145208\n",
      "sum of Ws for layer 3: 7.500588145277576 and sum of bs: 0.00238509158897915\n",
      "MSE on the training set: 3.2081676575929836e-06\n",
      "Epoch: 3710\n",
      "sum of abs dWs for layer 0: 0.00018284170705684387 and sum of dbs: 0.19818642185356714\n",
      "sum of Ws for layer 0: 86.42321018583559 and sum of bs: 0.001599199347771482\n",
      "sum of abs dWs for layer 1: 0.004810726614807498 and sum of dbs: 0.13721068027405842\n",
      "sum of Ws for layer 1: 321.3249727060818 and sum of bs: 0.0016884748669878512\n",
      "sum of abs dWs for layer 2: 0.0021982675213299946 and sum of dbs: 0.2502162691705553\n",
      "sum of Ws for layer 2: 340.8626858065768 and sum of bs: 0.007882837167019588\n",
      "sum of abs dWs for layer 3: 0.0010661575019842657 and sum of dbs: 0.09022419361948321\n",
      "sum of Ws for layer 3: 7.5005881452809335 and sum of bs: 0.002385090686737014\n",
      "MSE on the training set: 3.208164499207783e-06\n",
      "Epoch: 3720\n",
      "sum of abs dWs for layer 0: 0.00018284147177372083 and sum of dbs: 0.1981863243122366\n",
      "sum of Ws for layer 0: 86.4232101858368 and sum of bs: 0.0015991978380362603\n",
      "sum of abs dWs for layer 1: 0.004810720538412252 and sum of dbs: 0.13721061272383067\n",
      "sum of Ws for layer 1: 321.3249727060961 and sum of bs: 0.0016884737391531877\n",
      "sum of abs dWs for layer 2: 0.0021982648996220393 and sum of dbs: 0.2502161460032977\n",
      "sum of Ws for layer 2: 340.8626858065836 and sum of bs: 0.007882834808697121\n",
      "sum of abs dWs for layer 3: 0.0010661563142519335 and sum of dbs: 0.09022414920779051\n",
      "sum of Ws for layer 3: 7.500588145284291 and sum of bs: 0.002385089784495322\n",
      "MSE on the training set: 3.2081613408377774e-06\n",
      "Epoch: 3730\n",
      "sum of abs dWs for layer 0: 0.000182841065861675 and sum of dbs: 0.19818622677290337\n",
      "sum of Ws for layer 0: 86.42321018583797 and sum of bs: 0.0015991963283017822\n",
      "sum of abs dWs for layer 1: 0.0048107099013495955 and sum of dbs: 0.13721054517494308\n",
      "sum of Ws for layer 1: 321.32497270611043 and sum of bs: 0.0016884726113190792\n",
      "sum of abs dWs for layer 2: 0.0021982598973423403 and sum of dbs: 0.2502160228385249\n",
      "sum of Ws for layer 2: 340.86268580659043 and sum of bs: 0.007882832450375814\n",
      "sum of abs dWs for layer 3: 0.0010661538506848982 and sum of dbs: 0.09022410479699525\n",
      "sum of Ws for layer 3: 7.500588145287649 and sum of bs: 0.002385088882254074\n",
      "MSE on the training set: 3.208158182450473e-06\n",
      "Epoch: 3740\n",
      "sum of abs dWs for layer 0: 0.0001828408898767645 and sum of dbs: 0.19818612923070617\n",
      "sum of Ws for layer 0: 86.42321018583918 and sum of bs: 0.0015991948185680469\n",
      "sum of abs dWs for layer 1: 0.004810705409909362 and sum of dbs: 0.13721047762413646\n",
      "sum of Ws for layer 1: 321.32497270612475 and sum of bs: 0.0016884714834855257\n",
      "sum of abs dWs for layer 2: 0.0021982581029500793 and sum of dbs: 0.2502158996701915\n",
      "sum of Ws for layer 2: 340.86268580659726 and sum of bs: 0.007882830092055667\n",
      "sum of abs dWs for layer 3: 0.0010661531063397064 and sum of dbs: 0.09022406038491385\n",
      "sum of Ws for layer 3: 7.500588145291006 and sum of bs: 0.0023850879800132703\n",
      "MSE on the training set: 3.208155024091529e-06\n",
      "Epoch: 3750\n",
      "sum of abs dWs for layer 0: 0.00018284070405216003 and sum of dbs: 0.1981860316885191\n",
      "sum of Ws for layer 0: 86.42321018584039 and sum of bs: 0.001599193308835055\n",
      "sum of abs dWs for layer 1: 0.004810700655475685 and sum of dbs: 0.13721041007333762\n",
      "sum of Ws for layer 1: 321.3249727061391 and sum of bs: 0.0016884703556525273\n",
      "sum of abs dWs for layer 2: 0.0021982561712695615 and sum of dbs: 0.25021577650187155\n",
      "sum of Ws for layer 2: 340.86268580660396 and sum of bs: 0.007882827733736684\n",
      "sum of abs dWs for layer 3: 0.0010661522884196353 and sum of dbs: 0.09022401597283727\n",
      "sum of Ws for layer 3: 7.500588145294364 and sum of bs: 0.002385087077772911\n",
      "MSE on the training set: 3.208151865703635e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3760\n",
      "sum of abs dWs for layer 0: 0.00018284048725953484 and sum of dbs: 0.19818593414735444\n",
      "sum of Ws for layer 0: 86.42321018584158 and sum of bs: 0.001599191799102806\n",
      "sum of abs dWs for layer 1: 0.004810695073310207 and sum of dbs: 0.1372103425232254\n",
      "sum of Ws for layer 1: 321.3249727061534 and sum of bs: 0.001688469227820084\n",
      "sum of abs dWs for layer 2: 0.002198253807532858 and sum of dbs: 0.25021565333482404\n",
      "sum of Ws for layer 2: 340.8626858066108 and sum of bs: 0.007882825375418859\n",
      "sum of abs dWs for layer 3: 0.0010661512389457395 and sum of dbs: 0.09022397156122028\n",
      "sum of Ws for layer 3: 7.50058814529772 and sum of bs: 0.0023850861755329955\n",
      "MSE on the training set: 3.2081487073459317e-06\n",
      "Epoch: 3770\n",
      "sum of abs dWs for layer 0: 0.00018284023643598606 and sum of dbs: 0.19818583660606248\n",
      "sum of Ws for layer 0: 86.42321018584278 and sum of bs: 0.0015991902893713003\n",
      "sum of abs dWs for layer 1: 0.004810688581534904 and sum of dbs: 0.1372102749730288\n",
      "sum of Ws for layer 1: 321.3249727061678 and sum of bs: 0.001688468099988196\n",
      "sum of abs dWs for layer 2: 0.002198250968996898 and sum of dbs: 0.25021553016761916\n",
      "sum of Ws for layer 2: 340.8626858066176 and sum of bs: 0.007882823017102197\n",
      "sum of abs dWs for layer 3: 0.001066149935005224 and sum of dbs: 0.09022392714954638\n",
      "sum of Ws for layer 3: 7.500588145301077 and sum of bs: 0.002385085273293524\n",
      "MSE on the training set: 3.208145548975512e-06\n",
      "Epoch: 3780\n",
      "sum of abs dWs for layer 0: 0.0001828400712906783 and sum of dbs: 0.19818573906376924\n",
      "sum of Ws for layer 0: 86.42321018584397 and sum of bs: 0.0015991887796405378\n",
      "sum of abs dWs for layer 1: 0.004810684379826 and sum of dbs: 0.13721020742216203\n",
      "sum of Ws for layer 1: 321.3249727061821 and sum of bs: 0.0016884669721568628\n",
      "sum of abs dWs for layer 2: 0.0021982493258343922 and sum of dbs: 0.2502154069991699\n",
      "sum of Ws for layer 2: 340.8626858066244 and sum of bs: 0.007882820658786692\n",
      "sum of abs dWs for layer 3: 0.0010661492717147349 and sum of dbs: 0.090223882737423\n",
      "sum of Ws for layer 3: 7.500588145304436 and sum of bs: 0.002385084371054497\n",
      "MSE on the training set: 3.2081423906233634e-06\n",
      "Epoch: 3790\n",
      "sum of abs dWs for layer 0: 0.00018283999062492545 and sum of dbs: 0.1981856415205251\n",
      "sum of Ws for layer 0: 86.42321018584516 and sum of bs: 0.001599187269910518\n",
      "sum of abs dWs for layer 1: 0.004810682436134419 and sum of dbs: 0.13721013987065875\n",
      "sum of Ws for layer 1: 321.3249727061965 and sum of bs: 0.0016884658443260848\n",
      "sum of abs dWs for layer 2: 0.002198248861299344 and sum of dbs: 0.25021528382953906\n",
      "sum of Ws for layer 2: 340.86268580663125 and sum of bs: 0.00788281830047235\n",
      "sum of abs dWs for layer 3: 0.0010661492400874549 and sum of dbs: 0.0902238383248728\n",
      "sum of Ws for layer 3: 7.500588145307792 and sum of bs: 0.0023850834688159137\n",
      "MSE on the training set: 3.208139232251024e-06\n",
      "Epoch: 3800\n",
      "sum of abs dWs for layer 0: 0.0001828396328642517 and sum of dbs: 0.19818554398079283\n",
      "sum of Ws for layer 0: 86.42321018584639 and sum of bs: 0.0015991857601812417\n",
      "sum of abs dWs for layer 1: 0.004810673086085531 and sum of dbs: 0.13721007232151106\n",
      "sum of Ws for layer 1: 321.32497270621076 and sum of bs: 0.001688464716495862\n",
      "sum of abs dWs for layer 2: 0.0021982445308143866 and sum of dbs: 0.25021516066427657\n",
      "sum of Ws for layer 2: 340.862685806638 and sum of bs: 0.007882815942159168\n",
      "sum of abs dWs for layer 3: 0.001066147136557073 and sum of dbs: 0.09022379391390038\n",
      "sum of Ws for layer 3: 7.500588145311149 and sum of bs: 0.002385082566577775\n",
      "MSE on the training set: 3.208136073897146e-06\n",
      "Epoch: 3810\n",
      "sum of abs dWs for layer 0: 0.0001828394202449453 and sum of dbs: 0.19818544643936625\n",
      "sum of Ws for layer 0: 86.42321018584757 and sum of bs: 0.0015991842504527083\n",
      "sum of abs dWs for layer 1: 0.004810667615459365 and sum of dbs: 0.1372100047712285\n",
      "sum of Ws for layer 1: 321.32497270622514 and sum of bs: 0.0016884635886661942\n",
      "sum of abs dWs for layer 2: 0.002198242225300813 and sum of dbs: 0.2502150374969078\n",
      "sum of Ws for layer 2: 340.86268580664483 and sum of bs: 0.007882813583847147\n",
      "sum of abs dWs for layer 3: 0.0010661461182868404 and sum of dbs: 0.09022374950216719\n",
      "sum of Ws for layer 3: 7.5005881453145085 and sum of bs: 0.00238508166434008\n",
      "MSE on the training set: 3.2081329155620783e-06\n",
      "Epoch: 3820\n",
      "sum of abs dWs for layer 0: 0.00018283929978221215 and sum of dbs: 0.19818534889670303\n",
      "sum of Ws for layer 0: 86.42321018584875 and sum of bs: 0.001599182740724918\n",
      "sum of abs dWs for layer 1: 0.004810664608051688 and sum of dbs: 0.13720993722011787\n",
      "sum of Ws for layer 1: 321.32497270623946 and sum of bs: 0.0016884624608370818\n",
      "sum of abs dWs for layer 2: 0.002198241205523056 and sum of dbs: 0.25021491432800225\n",
      "sum of Ws for layer 2: 340.8626858066516 and sum of bs: 0.007882811225536286\n",
      "sum of abs dWs for layer 3: 0.0010661457890852764 and sum of dbs: 0.09022370508987881\n",
      "sum of Ws for layer 3: 7.500588145317865 and sum of bs: 0.002385080762102829\n",
      "MSE on the training set: 3.2081297571873845e-06\n",
      "Epoch: 3830\n",
      "sum of abs dWs for layer 0: 0.00018283896389937167 and sum of dbs: 0.19818525135698126\n",
      "sum of Ws for layer 0: 86.42321018584997 and sum of bs: 0.0015991812309978704\n",
      "sum of abs dWs for layer 1: 0.004810655842763531 and sum of dbs: 0.13720986967098042\n",
      "sum of Ws for layer 1: 321.3249727062538 and sum of bs: 0.001688461333008524\n",
      "sum of abs dWs for layer 2: 0.002198237180269909 and sum of dbs: 0.2502147911627555\n",
      "sum of Ws for layer 2: 340.8626858066584 and sum of bs: 0.007882808867226587\n",
      "sum of abs dWs for layer 3: 0.00106614384914004 and sum of dbs: 0.090223660678912\n",
      "sum of Ws for layer 3: 7.500588145321222 and sum of bs: 0.002385079859866023\n",
      "MSE on the training set: 3.208126598837878e-06\n",
      "Epoch: 3840\n",
      "sum of abs dWs for layer 0: 0.00018283880332480316 and sum of dbs: 0.19818515381520238\n",
      "sum of Ws for layer 0: 86.42321018585115 and sum of bs: 0.0015991797212715666\n",
      "sum of abs dWs for layer 1: 0.0048106517632231175 and sum of dbs: 0.13720980212046485\n",
      "sum of Ws for layer 1: 321.3249727062681 and sum of bs: 0.0016884602051805214\n",
      "sum of abs dWs for layer 2: 0.0021982356008630856 and sum of dbs: 0.2502146679949514\n",
      "sum of Ws for layer 2: 340.86268580666524 and sum of bs: 0.00788280650891805\n",
      "sum of abs dWs for layer 3: 0.0010661432200123843 and sum of dbs: 0.09022361626702141\n",
      "sum of Ws for layer 3: 7.500588145324579 and sum of bs: 0.00238507895762966\n",
      "MSE on the training set: 3.2081234405220325e-06\n",
      "Epoch: 3850\n",
      "sum of abs dWs for layer 0: 0.00018283868501129148 and sum of dbs: 0.19818505627299943\n",
      "sum of Ws for layer 0: 86.42321018585235 and sum of bs: 0.001599178211546006\n",
      "sum of abs dWs for layer 1: 0.0048106488132540134 and sum of dbs: 0.137209734569666\n",
      "sum of Ws for layer 1: 321.32497270628244 and sum of bs: 0.0016884590773530743\n",
      "sum of abs dWs for layer 2: 0.0021982346110694957 and sum of dbs: 0.25021454482662103\n",
      "sum of Ws for layer 2: 340.862685806672 and sum of bs: 0.007882804150610672\n",
      "sum of abs dWs for layer 3: 0.0010661429068808226 and sum of dbs: 0.0902235718549407\n",
      "sum of Ws for layer 3: 7.500588145327937 and sum of bs: 0.002385078055393742\n",
      "MSE on the training set: 3.208120282160768e-06\n",
      "Epoch: 3860\n",
      "sum of abs dWs for layer 0: 0.0001828382764395548 and sum of dbs: 0.19818495873427286\n",
      "sum of Ws for layer 0: 86.42321018585356 and sum of bs: 0.0015991767018211876\n",
      "sum of abs dWs for layer 1: 0.004810638105095635 and sum of dbs: 0.13720966702119897\n",
      "sum of Ws for layer 1: 321.32497270629676 and sum of bs: 0.001688457949526182\n",
      "sum of abs dWs for layer 2: 0.0021982295716795677 and sum of dbs: 0.25021442166261465\n",
      "sum of Ws for layer 2: 340.86268580667877 and sum of bs: 0.007882801792304453\n",
      "sum of abs dWs for layer 3: 0.0010661404234227137 and sum of dbs: 0.0902235274444218\n",
      "sum of Ws for layer 3: 7.500588145331293 and sum of bs: 0.0023850771531582675\n",
      "MSE on the training set: 3.2081171238318395e-06\n",
      "Epoch: 3870\n",
      "sum of abs dWs for layer 0: 0.00018283822193870426 and sum of dbs: 0.1981848611907624\n",
      "sum of Ws for layer 0: 86.42321018585476 and sum of bs: 0.0015991751920971134\n",
      "sum of abs dWs for layer 1: 0.004810636860754127 and sum of dbs: 0.13720959946952563\n",
      "sum of Ws for layer 1: 321.32497270631114 and sum of bs: 0.001688456821699845\n",
      "sum of abs dWs for layer 2: 0.0021982294721845635 and sum of dbs: 0.2502142984926601\n",
      "sum of Ws for layer 2: 340.8626858066856 and sum of bs: 0.007882799433999397\n",
      "sum of abs dWs for layer 3: 0.0010661405874337034 and sum of dbs: 0.09022348303175433\n",
      "sum of Ws for layer 3: 7.500588145334651 and sum of bs: 0.002385076250923237\n",
      "MSE on the training set: 3.2081139654776126e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3880\n",
      "sum of abs dWs for layer 0: 0.00018283804582158288 and sum of dbs: 0.19818476364883503\n",
      "sum of Ws for layer 0: 86.42321018585595 and sum of bs: 0.0015991736823737816\n",
      "sum of abs dWs for layer 1: 0.004810632365782048 and sum of dbs: 0.13720953191891472\n",
      "sum of Ws for layer 1: 321.3249727063255 and sum of bs: 0.0016884556938740625\n",
      "sum of abs dWs for layer 2: 0.0021982276759338845 and sum of dbs: 0.25021417532467494\n",
      "sum of Ws for layer 2: 340.8626858066924 and sum of bs: 0.0078827970756955\n",
      "sum of abs dWs for layer 3: 0.001066139842089716 and sum of dbs: 0.09022343861979823\n",
      "sum of Ws for layer 3: 7.500588145338009 and sum of bs: 0.0023850753486886513\n",
      "MSE on the training set: 3.2081108071582776e-06\n",
      "Epoch: 3890\n",
      "sum of abs dWs for layer 0: 0.00018283797358903178 and sum of dbs: 0.19818466610599622\n",
      "sum of Ws for layer 0: 86.42321018585716 and sum of bs: 0.0015991721726511929\n",
      "sum of abs dWs for layer 1: 0.004810630647496758 and sum of dbs: 0.13720946436769377\n",
      "sum of Ws for layer 1: 321.32497270633985 and sum of bs: 0.0016884545660488357\n",
      "sum of abs dWs for layer 2: 0.002198227329049762 and sum of dbs: 0.25021405215555753\n",
      "sum of Ws for layer 2: 340.86268580669923 and sum of bs: 0.007882794717392765\n",
      "sum of abs dWs for layer 3: 0.0010661398735174563 and sum of dbs: 0.09022339420743307\n",
      "sum of Ws for layer 3: 7.500588145341365 and sum of bs: 0.0023850744464545093\n",
      "MSE on the training set: 3.2081076488241894e-06\n",
      "Epoch: 3900\n",
      "sum of abs dWs for layer 0: 0.00018283779760145817 and sum of dbs: 0.19818456856409983\n",
      "sum of Ws for layer 0: 86.42321018585834 and sum of bs: 0.0015991706629293477\n",
      "sum of abs dWs for layer 1: 0.004810626155980727 and sum of dbs: 0.1372093968171058\n",
      "sum of Ws for layer 1: 321.3249727063542 and sum of bs: 0.0016884534382241637\n",
      "sum of abs dWs for layer 2: 0.0021982255345999757 and sum of dbs: 0.25021392898761297\n",
      "sum of Ws for layer 2: 340.86268580670605 and sum of bs: 0.007882792359091192\n",
      "sum of abs dWs for layer 3: 0.0010661391291410202 and sum of dbs: 0.0902233497954915\n",
      "sum of Ws for layer 3: 7.500588145344723 and sum of bs: 0.0023850735442208114\n",
      "MSE on the training set: 3.2081044905063633e-06\n",
      "Epoch: 3910\n",
      "sum of abs dWs for layer 0: 0.00018283779935693675 and sum of dbs: 0.19818447102036876\n",
      "sum of Ws for layer 0: 86.42321018585955 and sum of bs: 0.0015991691532082452\n",
      "sum of abs dWs for layer 1: 0.0048106264152915825 and sum of dbs: 0.13720932926528875\n",
      "sum of Ws for layer 1: 321.3249727063685 and sum of bs: 0.0016884523104000468\n",
      "sum of abs dWs for layer 2: 0.002198226219975216 and sum of dbs: 0.25021380581738756\n",
      "sum of Ws for layer 2: 340.8626858067128 and sum of bs: 0.007882790000790776\n",
      "sum of abs dWs for layer 3: 0.001066139713790549 and sum of dbs: 0.0902233053827261\n",
      "sum of Ws for layer 3: 7.500588145348081 and sum of bs: 0.002385072641987558\n",
      "MSE on the training set: 3.208101332178767e-06\n",
      "Epoch: 3920\n",
      "sum of abs dWs for layer 0: 0.00018283761227856664 and sum of dbs: 0.19818437347875154\n",
      "sum of Ws for layer 0: 86.42321018586075 and sum of bs: 0.001599167643487886\n",
      "sum of abs dWs for layer 1: 0.004810621627337183 and sum of dbs: 0.13720926171489006\n",
      "sum of Ws for layer 1: 321.3249727063828 and sum of bs: 0.001688451182576485\n",
      "sum of abs dWs for layer 2: 0.0021982242707912374 and sum of dbs: 0.25021368264979194\n",
      "sum of Ws for layer 2: 340.86268580671964 and sum of bs: 0.007882787642491522\n",
      "sum of abs dWs for layer 3: 0.00106613888648518 and sum of dbs: 0.09022326097091055\n",
      "sum of Ws for layer 3: 7.500588145351437 and sum of bs: 0.002385071739754748\n",
      "MSE on the training set: 3.20809817385888e-06\n",
      "Epoch: 3930\n",
      "sum of abs dWs for layer 0: 0.00018283754412535232 and sum of dbs: 0.19818427593570004\n",
      "sum of Ws for layer 0: 86.42321018586195 and sum of bs: 0.0015991661337682703\n",
      "sum of abs dWs for layer 1: 0.00481062001808896 and sum of dbs: 0.1372091941635308\n",
      "sum of Ws for layer 1: 321.3249727063972 and sum of bs: 0.0016884500547534784\n",
      "sum of abs dWs for layer 2: 0.0021982239808187418 and sum of dbs: 0.2502135594804137\n",
      "sum of Ws for layer 2: 340.86268580672646 and sum of bs: 0.00788278528419343\n",
      "sum of abs dWs for layer 3: 0.001066138948413335 and sum of dbs: 0.090223216558451\n",
      "sum of Ws for layer 3: 7.5005881453547945 and sum of bs: 0.0023850708375223826\n",
      "MSE on the training set: 3.2080950155404892e-06\n",
      "Epoch: 3940\n",
      "sum of abs dWs for layer 0: 0.0001828373937865219 and sum of dbs: 0.19818417839388652\n",
      "sum of Ws for layer 0: 86.42321018586313 and sum of bs: 0.0015991646240493974\n",
      "sum of abs dWs for layer 1: 0.004810616212127881 and sum of dbs: 0.1372091266130026\n",
      "sum of Ws for layer 1: 321.3249727064115 and sum of bs: 0.0016884489269310273\n",
      "sum of abs dWs for layer 2: 0.002198222544209274 and sum of dbs: 0.2502134363125757\n",
      "sum of Ws for layer 2: 340.8626858067332 and sum of bs: 0.0078827829258965\n",
      "sum of abs dWs for layer 3: 0.00106613839581194 and sum of dbs: 0.09022317214654783\n",
      "sum of Ws for layer 3: 7.500588145358152 and sum of bs: 0.0023850699352904615\n",
      "MSE on the training set: 3.2080918572353373e-06\n",
      "Epoch: 3950\n",
      "sum of abs dWs for layer 0: 0.000182837391423048 and sum of dbs: 0.19818408085052205\n",
      "sum of Ws for layer 0: 86.42321018586433 and sum of bs: 0.0015991631143312678\n",
      "sum of abs dWs for layer 1: 0.004810616361343227 and sum of dbs: 0.1372090590614356\n",
      "sum of Ws for layer 1: 321.32497270642585 and sum of bs: 0.001688447799109131\n",
      "sum of abs dWs for layer 2: 0.0021982231721179293 and sum of dbs: 0.25021331314281\n",
      "sum of Ws for layer 2: 340.86268580674 and sum of bs: 0.00788278056760073\n",
      "sum of abs dWs for layer 3: 0.0010661389496638427 and sum of dbs: 0.09022312773394829\n",
      "sum of Ws for layer 3: 7.500588145361508 and sum of bs: 0.002385069033058984\n",
      "MSE on the training set: 3.2080886989263446e-06\n",
      "Epoch: 3960\n",
      "sum of abs dWs for layer 0: 0.0001828371464301715 and sum of dbs: 0.1981839833098281\n",
      "sum of Ws for layer 0: 86.42321018586553 and sum of bs: 0.001599161604613881\n",
      "sum of abs dWs for layer 1: 0.004810610025415042 and sum of dbs: 0.1372089915116601\n",
      "sum of Ws for layer 1: 321.3249727064402 and sum of bs: 0.0016884466712877894\n",
      "sum of abs dWs for layer 2: 0.002198220414923869 and sum of dbs: 0.2502131899763662\n",
      "sum of Ws for layer 2: 340.8626858067468 and sum of bs: 0.007882778209306118\n",
      "sum of abs dWs for layer 3: 0.0010661376893166719 and sum of dbs: 0.09022308332254861\n",
      "sum of Ws for layer 3: 7.5005881453648655 and sum of bs: 0.002385068130827951\n",
      "MSE on the training set: 3.208085540630497e-06\n",
      "Epoch: 3970\n",
      "sum of abs dWs for layer 0: 0.00018283711956319906 and sum of dbs: 0.19818388576666862\n",
      "sum of Ws for layer 0: 86.42321018586672 and sum of bs: 0.0015991600948972373\n",
      "sum of abs dWs for layer 1: 0.004810609519686842 and sum of dbs: 0.1372089239602327\n",
      "sum of Ws for layer 1: 321.32497270645456 and sum of bs: 0.001688445543467003\n",
      "sum of abs dWs for layer 2: 0.0021982207009638086 and sum of dbs: 0.2502130668068573\n",
      "sum of Ws for layer 2: 340.86268580675363 and sum of bs: 0.007882775851012668\n",
      "sum of abs dWs for layer 3: 0.0010661380599498508 and sum of dbs: 0.09022303891004174\n",
      "sum of Ws for layer 3: 7.500588145368225 and sum of bs: 0.002385067228597362\n",
      "MSE on the training set: 3.208082382318016e-06\n",
      "Epoch: 3980\n",
      "sum of abs dWs for layer 0: 0.00018283694897361185 and sum of dbs: 0.19818378822536903\n",
      "sum of Ws for layer 0: 86.42321018586793 and sum of bs: 0.0015991585851813368\n",
      "sum of abs dWs for layer 1: 0.004810605172448585 and sum of dbs: 0.13720885641005331\n",
      "sum of Ws for layer 1: 321.3249727064689 and sum of bs: 0.001688444415646772\n",
      "sum of abs dWs for layer 2: 0.002198218981816136 and sum of dbs: 0.2502129436396622\n",
      "sum of Ws for layer 2: 340.86268580676045 and sum of bs: 0.00788277349272038\n",
      "sum of abs dWs for layer 3: 0.0010661373559244423 and sum of dbs: 0.09022299449837061\n",
      "sum of Ws for layer 3: 7.50058814537158 and sum of bs: 0.002385066326367217\n",
      "MSE on the training set: 3.208079224031234e-06\n",
      "Epoch: 3990\n",
      "sum of abs dWs for layer 0: 0.0001828368836737266 and sum of dbs: 0.19818369068230687\n",
      "sum of Ws for layer 0: 86.42321018586912 and sum of bs: 0.0015991570754661795\n",
      "sum of abs dWs for layer 1: 0.004810603639462088 and sum of dbs: 0.13720878885869325\n",
      "sum of Ws for layer 1: 321.3249727064832 and sum of bs: 0.001688443287827096\n",
      "sum of abs dWs for layer 2: 0.0021982187316475035 and sum of dbs: 0.2502128204702761\n",
      "sum of Ws for layer 2: 340.8626858067672 and sum of bs: 0.00788277113442925\n",
      "sum of abs dWs for layer 3: 0.0010661374391846882 and sum of dbs: 0.09022295008590805\n",
      "sum of Ws for layer 3: 7.500588145374938 and sum of bs: 0.0023850654241375164\n",
      "MSE on the training set: 3.2080760657188923e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000\n",
      "sum of abs dWs for layer 0: 0.00018283688408386614 and sum of dbs: 0.1981835931389473\n",
      "sum of Ws for layer 0: 86.42321018587032 and sum of bs: 0.0015991555657517652\n",
      "sum of abs dWs for layer 1: 0.004810603862808066 and sum of dbs: 0.13720872130713485\n",
      "sum of Ws for layer 1: 321.32497270649753 and sum of bs: 0.0016884421600079753\n",
      "sum of abs dWs for layer 2: 0.0021982193982419667 and sum of dbs: 0.2502126973005211\n",
      "sum of Ws for layer 2: 340.86268580677404 and sum of bs: 0.007882768776139283\n",
      "sum of abs dWs for layer 3: 0.0010661380137662515 and sum of dbs: 0.0902229056733122\n",
      "sum of Ws for layer 3: 7.500588145378295 and sum of bs: 0.0023850645219082598\n",
      "MSE on the training set: 3.20807290741332e-06\n",
      "Epoch: 4010\n",
      "sum of abs dWs for layer 0: 0.00018283665987580506 and sum of dbs: 0.19818349559855292\n",
      "sum of Ws for layer 0: 86.42321018587153 and sum of bs: 0.001599154056038094\n",
      "sum of abs dWs for layer 1: 0.0048105980824214595 and sum of dbs: 0.13720865375756555\n",
      "sum of Ws for layer 1: 321.32497270651186 and sum of bs: 0.0016884410321894095\n",
      "sum of abs dWs for layer 2: 0.002198216931023159 and sum of dbs: 0.25021257413445447\n",
      "sum of Ws for layer 2: 340.86268580678086 and sum of bs: 0.007882766417850474\n",
      "sum of abs dWs for layer 3: 0.0010661369088231392 and sum of dbs: 0.09022286126204856\n",
      "sum of Ws for layer 3: 7.500588145381652 and sum of bs: 0.0023850636196794473\n",
      "MSE on the training set: 3.2080697491366612e-06\n",
      "Epoch: 4020\n",
      "sum of abs dWs for layer 0: 0.00018283664889909783 and sum of dbs: 0.19818339805519086\n",
      "sum of Ws for layer 0: 86.42321018587272 and sum of bs: 0.001599152546325166\n",
      "sum of abs dWs for layer 1: 0.004810598001423318 and sum of dbs: 0.1372085862060076\n",
      "sum of Ws for layer 1: 321.32497270652624 and sum of bs: 0.0016884399043713984\n",
      "sum of abs dWs for layer 2: 0.0021982174387633995 and sum of dbs: 0.25021245096469813\n",
      "sum of Ws for layer 2: 340.8626858067876 and sum of bs: 0.007882764059562829\n",
      "sum of abs dWs for layer 3: 0.0010661373982716943 and sum of dbs: 0.09022281684945216\n",
      "sum of Ws for layer 3: 7.5005881453850085 and sum of bs: 0.002385062717451079\n",
      "MSE on the training set: 3.2080665908452245e-06\n",
      "Epoch: 4030\n",
      "sum of abs dWs for layer 0: 0.00018283656276917197 and sum of dbs: 0.19818330051273156\n",
      "sum of Ws for layer 0: 86.42321018587391 and sum of bs: 0.0015991510366129814\n",
      "sum of abs dWs for layer 1: 0.004810595911674959 and sum of dbs: 0.137208518655056\n",
      "sum of Ws for layer 1: 321.32497270654056 and sum of bs: 0.0016884387765539431\n",
      "sum of abs dWs for layer 2: 0.0021982168979723963 and sum of dbs: 0.25021232779606556\n",
      "sum of Ws for layer 2: 340.8626858067944 and sum of bs: 0.007882761701276345\n",
      "sum of abs dWs for layer 3: 0.0010661373257713762 and sum of dbs: 0.0902227724372616\n",
      "sum of Ws for layer 3: 7.500588145388367 and sum of bs: 0.0023850618152231546\n",
      "MSE on the training set: 3.2080634325566503e-06\n",
      "Epoch: 4040\n",
      "sum of abs dWs for layer 0: 0.00018283636726346294 and sum of dbs: 0.19818320297223\n",
      "sum of Ws for layer 0: 86.42321018587513 and sum of bs: 0.0015991495269015393\n",
      "sum of abs dWs for layer 1: 0.004810590898461691 and sum of dbs: 0.137208451105418\n",
      "sum of Ws for layer 1: 321.3249727065549 and sum of bs: 0.0016884376487370425\n",
      "sum of abs dWs for layer 2: 0.002198214831201703 and sum of dbs: 0.25021220462986843\n",
      "sum of Ws for layer 2: 340.8626858068012 and sum of bs: 0.007882759342991019\n",
      "sum of abs dWs for layer 3: 0.0010661364354436005 and sum of dbs: 0.0902227280259507\n",
      "sum of Ws for layer 3: 7.500588145391722 and sum of bs: 0.0023850609129956743\n",
      "MSE on the training set: 3.208060274286723e-06\n",
      "Epoch: 4050\n",
      "sum of abs dWs for layer 0: 0.00018283616072923978 and sum of dbs: 0.19818310543146966\n",
      "sum of Ws for layer 0: 86.4232101858763 and sum of bs: 0.001599148017190841\n",
      "sum of abs dWs for layer 1: 0.004810585590482542 and sum of dbs: 0.13720838355560755\n",
      "sum of Ws for layer 1: 321.32497270656927 and sum of bs: 0.001688436520920697\n",
      "sum of abs dWs for layer 2: 0.00219821261056888 and sum of dbs: 0.25021208146335033\n",
      "sum of Ws for layer 2: 340.86268580680803 and sum of bs: 0.007882756984706854\n",
      "sum of abs dWs for layer 3: 0.0010661354626548942 and sum of dbs: 0.09022268361452385\n",
      "sum of Ws for layer 3: 7.500588145395081 and sum of bs: 0.0023850600107686386\n",
      "MSE on the training set: 3.208057116005348e-06\n",
      "Epoch: 4060\n",
      "sum of abs dWs for layer 0: 0.00018283620184037216 and sum of dbs: 0.19818300788758533\n",
      "sum of Ws for layer 0: 86.42321018587751 and sum of bs: 0.0015991465074808853\n",
      "sum of abs dWs for layer 1: 0.004810586901705834 and sum of dbs: 0.1372083160037037\n",
      "sum of Ws for layer 1: 321.3249727065836 and sum of bs: 0.0016884353931049065\n",
      "sum of abs dWs for layer 2: 0.0021982138450075935 and sum of dbs: 0.2502119582929483\n",
      "sum of Ws for layer 2: 340.86268580681485 and sum of bs: 0.00788275462642385\n",
      "sum of abs dWs for layer 3: 0.0010661363415662849 and sum of dbs: 0.09022263920169406\n",
      "sum of Ws for layer 3: 7.500588145398438 and sum of bs: 0.0023850591085420466\n",
      "MSE on the training set: 3.208053957730033e-06\n",
      "Epoch: 4070\n",
      "sum of abs dWs for layer 0: 0.00018283601721362058 and sum of dbs: 0.19818291034733057\n",
      "sum of Ws for layer 0: 86.4232101858787 and sum of bs: 0.001599144997771673\n",
      "sum of abs dWs for layer 1: 0.004810582179275365 and sum of dbs: 0.13720824845423432\n",
      "sum of Ws for layer 1: 321.3249727065979 and sum of bs: 0.0016884342652896715\n",
      "sum of abs dWs for layer 2: 0.002198211930014465 and sum of dbs: 0.25021183512706074\n",
      "sum of Ws for layer 2: 340.8626858068217 and sum of bs: 0.007882752268142008\n",
      "sum of abs dWs for layer 3: 0.0010661355325802836 and sum of dbs: 0.0902225947904949\n",
      "sum of Ws for layer 3: 7.500588145401795 and sum of bs: 0.002385058206315899\n",
      "MSE on the training set: 3.2080507994443194e-06\n",
      "Epoch: 4080\n",
      "sum of abs dWs for layer 0: 0.00018283593128163775 and sum of dbs: 0.19818281280486347\n",
      "sum of Ws for layer 0: 86.42321018587991 and sum of bs: 0.0015991434880632035\n",
      "sum of abs dWs for layer 1: 0.004810580094816602 and sum of dbs: 0.13720818090328274\n",
      "sum of Ws for layer 1: 321.32497270661224 and sum of bs: 0.001688433137474991\n",
      "sum of abs dWs for layer 2: 0.0021982113919813764 and sum of dbs: 0.25021171195842296\n",
      "sum of Ws for layer 2: 340.8626858068284 and sum of bs: 0.007882749909861327\n",
      "sum of abs dWs for layer 3: 0.001066135461557729 and sum of dbs: 0.09022255037830226\n",
      "sum of Ws for layer 3: 7.500588145405152 and sum of bs: 0.002385057304090195\n",
      "MSE on the training set: 3.208047641181987e-06\n",
      "Epoch: 4090\n",
      "sum of abs dWs for layer 0: 0.00018283588934063605 and sum of dbs: 0.19818271526258532\n",
      "sum of Ws for layer 0: 86.4232101858811 and sum of bs: 0.0015991419783554775\n",
      "sum of abs dWs for layer 1: 0.0048105791861792445 and sum of dbs: 0.137208113352459\n",
      "sum of Ws for layer 1: 321.32497270662657 and sum of bs: 0.001688432009660866\n",
      "sum of abs dWs for layer 2: 0.0021982114677014932 and sum of dbs: 0.250211588790021\n",
      "sum of Ws for layer 2: 340.8626858068352 and sum of bs: 0.007882747551581805\n",
      "sum of abs dWs for layer 3: 0.0010661357194702782 and sum of dbs: 0.09022250596619483\n",
      "sum of Ws for layer 3: 7.500588145408509 and sum of bs: 0.0023850564018649354\n",
      "MSE on the training set: 3.208044482935211e-06\n",
      "Epoch: 4100\n",
      "sum of abs dWs for layer 0: 0.0001828357733206207 and sum of dbs: 0.19818261772126833\n",
      "sum of Ws for layer 0: 86.4232101858823 and sum of bs: 0.0015991404686484943\n",
      "sum of abs dWs for layer 1: 0.004810576297509911 and sum of dbs: 0.13720804580228063\n",
      "sum of Ws for layer 1: 321.3249727066409 and sum of bs: 0.001688430881847296\n",
      "sum of abs dWs for layer 2: 0.0021982105098902766 and sum of dbs: 0.25021146562281543\n",
      "sum of Ws for layer 2: 340.862685806842 and sum of bs: 0.007882745193303444\n",
      "sum of abs dWs for layer 3: 0.0010661354234699338 and sum of dbs: 0.09022246155451943\n",
      "sum of Ws for layer 3: 7.500588145411866 and sum of bs: 0.0023850554996401206\n",
      "MSE on the training set: 3.208041324654599e-06\n",
      "Epoch: 4110\n",
      "sum of abs dWs for layer 0: 0.00018283569098936664 and sum of dbs: 0.19818252017906773\n",
      "sum of Ws for layer 0: 86.42321018588349 and sum of bs: 0.0015991389589422545\n",
      "sum of abs dWs for layer 1: 0.004810574309292324 and sum of dbs: 0.13720797825151093\n",
      "sum of Ws for layer 1: 321.3249727066553 and sum of bs: 0.0016884297540342812\n",
      "sum of abs dWs for layer 2: 0.002198210022092293 and sum of dbs: 0.25021134245451176\n",
      "sum of Ws for layer 2: 340.86268580684884 and sum of bs: 0.007882742835026244\n",
      "sum of abs dWs for layer 3: 0.0010661353793701188 and sum of dbs: 0.0902224171424474\n",
      "sum of Ws for layer 3: 7.5005881454152235 and sum of bs: 0.002385054597415749\n",
      "MSE on the training set: 3.208038166397609e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4120\n",
      "sum of abs dWs for layer 0: 0.00018283548494341068 and sum of dbs: 0.19818242263881206\n",
      "sum of Ws for layer 0: 86.4232101858847 and sum of bs: 0.0015991374492367579\n",
      "sum of abs dWs for layer 1: 0.00481056901435683 and sum of dbs: 0.13720791070204624\n",
      "sum of Ws for layer 1: 321.32497270666966 and sum of bs: 0.0016884286262218214\n",
      "sum of abs dWs for layer 2: 0.002198207808265782 and sum of dbs: 0.2502112192886277\n",
      "sum of Ws for layer 2: 340.86268580685567 and sum of bs: 0.007882740476750204\n",
      "sum of abs dWs for layer 3: 0.0010661344102276082 and sum of dbs: 0.09022237273124929\n",
      "sum of Ws for layer 3: 7.500588145418581 and sum of bs: 0.002385053695191822\n",
      "MSE on the training set: 3.2080350081498448e-06\n",
      "Epoch: 4130\n",
      "sum of abs dWs for layer 0: 0.00018283538385225972 and sum of dbs: 0.19818232509721587\n",
      "sum of Ws for layer 0: 86.42321018588589 and sum of bs: 0.0015991359395320039\n",
      "sum of abs dWs for layer 1: 0.004810566524718056 and sum of dbs: 0.13720784315168388\n",
      "sum of Ws for layer 1: 321.3249727066839 and sum of bs: 0.0016884274984099166\n",
      "sum of abs dWs for layer 2: 0.0021982070587313906 and sum of dbs: 0.2502110961210776\n",
      "sum of Ws for layer 2: 340.86268580686243 and sum of bs: 0.007882738118475325\n",
      "sum of abs dWs for layer 3: 0.0010661342258539903 and sum of dbs: 0.09022232831944937\n",
      "sum of Ws for layer 3: 7.500588145421938 and sum of bs: 0.002385052792968339\n",
      "MSE on the training set: 3.2080318498964385e-06\n",
      "Epoch: 4140\n",
      "sum of abs dWs for layer 0: 0.00018283525555028979 and sum of dbs: 0.19818222755590267\n",
      "sum of Ws for layer 0: 86.42321018588709 and sum of bs: 0.0015991344298279932\n",
      "sum of abs dWs for layer 1: 0.004810563307761854 and sum of dbs: 0.13720777560151237\n",
      "sum of Ws for layer 1: 321.3249727066983 and sum of bs: 0.0016884263705985673\n",
      "sum of abs dWs for layer 2: 0.0021982059295618615 and sum of dbs: 0.25021097295388034\n",
      "sum of Ws for layer 2: 340.86268580686925 and sum of bs: 0.007882735760201608\n",
      "sum of abs dWs for layer 3: 0.0010661338380178566 and sum of dbs: 0.09022228390777685\n",
      "sum of Ws for layer 3: 7.500588145425295 and sum of bs: 0.0023850518907452996\n",
      "MSE on the training set: 3.2080286916380886e-06\n",
      "Epoch: 4150\n",
      "sum of abs dWs for layer 0: 0.0001828353093161271 and sum of dbs: 0.19818213001280421\n",
      "sum of Ws for layer 0: 86.42321018588828 and sum of bs: 0.0015991329201247258\n",
      "sum of abs dWs for layer 1: 0.004810564957229128 and sum of dbs: 0.13720770805014493\n",
      "sum of Ws for layer 1: 321.32497270671263 and sum of bs: 0.0016884252427877724\n",
      "sum of abs dWs for layer 2: 0.0021982073405492227 and sum of dbs: 0.2502108497844638\n",
      "sum of Ws for layer 2: 340.862685806876 and sum of bs: 0.00788273340192905\n",
      "sum of abs dWs for layer 3: 0.001066134811544969 and sum of dbs: 0.0902222394953027\n",
      "sum of Ws for layer 3: 7.500588145428653 and sum of bs: 0.002385050988522705\n",
      "MSE on the training set: 3.2080255333924124e-06\n",
      "Epoch: 4160\n",
      "sum of abs dWs for layer 0: 0.0001828351920207127 and sum of dbs: 0.19818203247166644\n",
      "sum of Ws for layer 0: 86.42321018588946 and sum of bs: 0.0015991314104222012\n",
      "sum of abs dWs for layer 1: 0.0048105620344681905 and sum of dbs: 0.13720764050009307\n",
      "sum of Ws for layer 1: 321.32497270672695 and sum of bs: 0.0016884241149775331\n",
      "sum of abs dWs for layer 2: 0.0021982063649401834 and sum of dbs: 0.25021072661748645\n",
      "sum of Ws for layer 2: 340.86268580688284 and sum of bs: 0.007882731043657654\n",
      "sum of abs dWs for layer 3: 0.0010661345060052091 and sum of dbs: 0.09022219508370954\n",
      "sum of Ws for layer 3: 7.500588145432009 and sum of bs: 0.0023850500863005545\n",
      "MSE on the training set: 3.2080223751440693e-06\n",
      "Epoch: 4170\n",
      "sum of abs dWs for layer 0: 0.00018283501950546192 and sum of dbs: 0.19818193493085998\n",
      "sum of Ws for layer 0: 86.42321018589068 and sum of bs: 0.0015991299007204198\n",
      "sum of abs dWs for layer 1: 0.004810557635757356 and sum of dbs: 0.13720757295026453\n",
      "sum of Ws for layer 1: 321.32497270674133 and sum of bs: 0.0016884229871678487\n",
      "sum of abs dWs for layer 2: 0.0021982046189138725 and sum of dbs: 0.25021060345092216\n",
      "sum of Ws for layer 2: 340.86268580688966 and sum of bs: 0.007882728685387419\n",
      "sum of abs dWs for layer 3: 0.0010661337875754841 and sum of dbs: 0.09022215067226555\n",
      "sum of Ws for layer 3: 7.5005881454353664 and sum of bs: 0.002385049184078848\n",
      "MSE on the training set: 3.208019216908654e-06\n",
      "Epoch: 4180\n",
      "sum of abs dWs for layer 0: 0.00018283488540369558 and sum of dbs: 0.19818183738983436\n",
      "sum of Ws for layer 0: 86.42321018589188 and sum of bs: 0.0015991283910193816\n",
      "sum of abs dWs for layer 1: 0.004810554263793463 and sum of dbs: 0.13720750540028998\n",
      "sum of Ws for layer 1: 321.3249727067556 and sum of bs: 0.0016884218593587194\n",
      "sum of abs dWs for layer 2: 0.002198203408831093 and sum of dbs: 0.25021048028408616\n",
      "sum of Ws for layer 2: 340.8626858068964 and sum of bs: 0.007882726327118342\n",
      "sum of abs dWs for layer 3: 0.0010661333563721349 and sum of dbs: 0.09022210626072336\n",
      "sum of Ws for layer 3: 7.500588145438723 and sum of bs: 0.0023850482818575857\n",
      "MSE on the training set: 3.208016058658929e-06\n",
      "Epoch: 4190\n",
      "sum of abs dWs for layer 0: 0.0001828347876418117 and sum of dbs: 0.19818173984847567\n",
      "sum of Ws for layer 0: 86.42321018589307 and sum of bs: 0.0015991268813190865\n",
      "sum of abs dWs for layer 1: 0.004810551863131721 and sum of dbs: 0.13720743785009323\n",
      "sum of Ws for layer 1: 321.3249727067699 and sum of bs: 0.0016884207315501455\n",
      "sum of abs dWs for layer 2: 0.0021982027057411676 and sum of dbs: 0.2502103571168369\n",
      "sum of Ws for layer 2: 340.8626858069032 and sum of bs: 0.007882723968850428\n",
      "sum of abs dWs for layer 3: 0.0010661331968882673 and sum of dbs: 0.09022206184903189\n",
      "sum of Ws for layer 3: 7.50058814544208 and sum of bs: 0.002385047379636767\n",
      "MSE on the training set: 3.2080129004272437e-06\n",
      "Epoch: 4200\n",
      "sum of abs dWs for layer 0: 0.0001828347374324683 and sum of dbs: 0.19818164230698873\n",
      "sum of Ws for layer 0: 86.42321018589428 and sum of bs: 0.0015991253716195347\n",
      "sum of abs dWs for layer 1: 0.0048105507334860265 and sum of dbs: 0.1372073702998115\n",
      "sum of Ws for layer 1: 321.3249727067843 and sum of bs: 0.0016884196037421262\n",
      "sum of abs dWs for layer 2: 0.0021982026660882876 and sum of dbs: 0.25021023394942904\n",
      "sum of Ws for layer 2: 340.86268580691 and sum of bs: 0.007882721610583674\n",
      "sum of abs dWs for layer 3: 0.0010661333929584801 and sum of dbs: 0.09022201743728309\n",
      "sum of Ws for layer 3: 7.500588145445437 and sum of bs: 0.002385046477416393\n",
      "MSE on the training set: 3.208009742188478e-06\n",
      "Epoch: 4210\n",
      "sum of abs dWs for layer 0: 0.0001828345575216911 and sum of dbs: 0.1981815447667902\n",
      "sum of Ws for layer 0: 86.42321018589547 and sum of bs: 0.0015991238619207258\n",
      "sum of abs dWs for layer 1: 0.004810546137103738 and sum of dbs: 0.13720730275039467\n",
      "sum of Ws for layer 1: 321.32497270679863 and sum of bs: 0.0016884184759346622\n",
      "sum of abs dWs for layer 2: 0.002198200816884821 and sum of dbs: 0.2502101107836244\n",
      "sum of Ws for layer 2: 340.86268580691683 and sum of bs: 0.007882719252318081\n",
      "sum of abs dWs for layer 3: 0.0010661326192302895 and sum of dbs: 0.0902219730261133\n",
      "sum of Ws for layer 3: 7.500588145448795 and sum of bs: 0.0023850455751964626\n",
      "MSE on the training set: 3.2080065839642867e-06\n",
      "Epoch: 4220\n",
      "sum of abs dWs for layer 0: 0.00018283445291696795 and sum of dbs: 0.19818144722560024\n",
      "sum of Ws for layer 0: 86.42321018589666 and sum of bs: 0.0015991223522226598\n",
      "sum of abs dWs for layer 1: 0.004810543553545335 and sum of dbs: 0.13720723520031425\n",
      "sum of Ws for layer 1: 321.324972706813 and sum of bs: 0.0016884173481277534\n",
      "sum of abs dWs for layer 2: 0.002198200018327978 and sum of dbs: 0.2502099876165877\n",
      "sum of Ws for layer 2: 340.86268580692365 and sum of bs: 0.00788271689405365\n",
      "sum of abs dWs for layer 3: 0.0010661324085797694 and sum of dbs: 0.09022192861449849\n",
      "sum of Ws for layer 3: 7.500588145452151 and sum of bs: 0.0023850446729769765\n",
      "MSE on the training set: 3.2080034257440054e-06\n",
      "Epoch: 4230\n",
      "sum of abs dWs for layer 0: 0.00018283434082698381 and sum of dbs: 0.19818134968469514\n",
      "sum of Ws for layer 0: 86.42321018589787 and sum of bs: 0.0015991208425253374\n",
      "sum of abs dWs for layer 1: 0.004810540769912565 and sum of dbs: 0.13720716765042584\n",
      "sum of Ws for layer 1: 321.32497270682734 and sum of bs: 0.0016884162203213999\n",
      "sum of abs dWs for layer 2: 0.002198199115332386 and sum of dbs: 0.2502098644499063\n",
      "sum of Ws for layer 2: 340.8626858069305 and sum of bs: 0.007882714535790376\n",
      "sum of abs dWs for layer 3: 0.0010661321419590624 and sum of dbs: 0.09022188420301196\n",
      "sum of Ws for layer 3: 7.500588145455508 and sum of bs: 0.002385043770757935\n",
      "MSE on the training set: 3.208000267533415e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4240\n",
      "sum of abs dWs for layer 0: 0.00018283424044944735 and sum of dbs: 0.1981812521440474\n",
      "sum of Ws for layer 0: 86.42321018589907 and sum of bs: 0.0015991193328287576\n",
      "sum of abs dWs for layer 1: 0.004810538299347109 and sum of dbs: 0.13720710010071108\n",
      "sum of Ws for layer 1: 321.32497270684166 and sum of bs: 0.0016884150925156008\n",
      "sum of abs dWs for layer 2: 0.0021981983757490818 and sum of dbs: 0.2502097412835457\n",
      "sum of Ws for layer 2: 340.8626858069373 and sum of bs: 0.007882712177528268\n",
      "sum of abs dWs for layer 3: 0.001066131962911131 and sum of dbs: 0.09022183979164128\n",
      "sum of Ws for layer 3: 7.500588145458866 and sum of bs: 0.002385042868539337\n",
      "MSE on the training set: 3.2079971093043555e-06\n",
      "Epoch: 4250\n",
      "sum of abs dWs for layer 0: 0.00018283406674451404 and sum of dbs: 0.198181154603997\n",
      "sum of Ws for layer 0: 86.42321018590027 and sum of bs: 0.0015991178231329211\n",
      "sum of abs dWs for layer 1: 0.004810533868839593 and sum of dbs: 0.13720703255139777\n",
      "sum of Ws for layer 1: 321.324972706856 and sum of bs: 0.0016884139647103573\n",
      "sum of abs dWs for layer 2: 0.002198196613122011 and sum of dbs: 0.25020961811792886\n",
      "sum of Ws for layer 2: 340.862685806944 and sum of bs: 0.007882709819267317\n",
      "sum of abs dWs for layer 3: 0.0010661312355791067 and sum of dbs: 0.0902217953805392\n",
      "sum of Ws for layer 3: 7.500588145462222 and sum of bs: 0.0023850419663211833\n",
      "MSE on the training set: 3.2079939510775753e-06\n",
      "Epoch: 4260\n",
      "sum of abs dWs for layer 0: 0.0001828339883658065 and sum of dbs: 0.19818105706247466\n",
      "sum of Ws for layer 0: 86.42321018590147 and sum of bs: 0.0015991163134378278\n",
      "sum of abs dWs for layer 1: 0.004810531986264262 and sum of dbs: 0.13720696500109875\n",
      "sum of Ws for layer 1: 321.32497270687037 and sum of bs: 0.0016884128369056687\n",
      "sum of abs dWs for layer 2: 0.002198196180458785 and sum of dbs: 0.2502094949504825\n",
      "sum of Ws for layer 2: 340.8626858069508 and sum of bs: 0.007882707461007527\n",
      "sum of abs dWs for layer 3: 0.0010661312210238942 and sum of dbs: 0.09022175096877627\n",
      "sum of Ws for layer 3: 7.50058814546558 and sum of bs: 0.002385041064103474\n",
      "MSE on the training set: 3.207990792868474e-06\n",
      "Epoch: 4270\n",
      "sum of abs dWs for layer 0: 0.00018283383462677175 and sum of dbs: 0.1981809595222832\n",
      "sum of Ws for layer 0: 86.42321018590266 and sum of bs: 0.0015991148037434776\n",
      "sum of abs dWs for layer 1: 0.004810528089415768 and sum of dbs: 0.13720689745169304\n",
      "sum of Ws for layer 1: 321.32497270688464 and sum of bs: 0.0016884117091015354\n",
      "sum of abs dWs for layer 2: 0.002198194696389324 and sum of dbs: 0.2502093717846922\n",
      "sum of Ws for layer 2: 340.86268580695764 and sum of bs: 0.007882705102748896\n",
      "sum of abs dWs for layer 3: 0.0010661306429835136 and sum of dbs: 0.09022170655761147\n",
      "sum of Ws for layer 3: 7.500588145468938 and sum of bs: 0.002385040161886208\n",
      "MSE on the training set: 3.2079876346544514e-06\n",
      "Epoch: 4280\n",
      "sum of abs dWs for layer 0: 0.00018283375157850083 and sum of dbs: 0.19818086198112414\n",
      "sum of Ws for layer 0: 86.42321018590384 and sum of bs: 0.0015991132940498705\n",
      "sum of abs dWs for layer 1: 0.004810526082021866 and sum of dbs: 0.13720682990163963\n",
      "sum of Ws for layer 1: 321.324972706899 and sum of bs: 0.0016884105812979572\n",
      "sum of abs dWs for layer 2: 0.0021981941985751304 and sum of dbs: 0.2502092486176995\n",
      "sum of Ws for layer 2: 340.86268580696446 and sum of bs: 0.007882702744491429\n",
      "sum of abs dWs for layer 3: 0.0010661305935091977 and sum of dbs: 0.0902216621460123\n",
      "sum of Ws for layer 3: 7.5005881454722925 and sum of bs: 0.002385039259669387\n",
      "MSE on the training set: 3.207984476453696e-06\n",
      "Epoch: 4290\n",
      "sum of abs dWs for layer 0: 0.0001828337067754731 and sum of dbs: 0.1981807644397072\n",
      "sum of Ws for layer 0: 86.42321018590505 and sum of bs: 0.001599111784357006\n",
      "sum of abs dWs for layer 1: 0.004810525096877108 and sum of dbs: 0.13720676235141444\n",
      "sum of Ws for layer 1: 321.32497270691334 and sum of bs: 0.001688409453494934\n",
      "sum of abs dWs for layer 2: 0.002198194234340242 and sum of dbs: 0.250209125450387\n",
      "sum of Ws for layer 2: 340.86268580697123 and sum of bs: 0.007882700386235121\n",
      "sum of abs dWs for layer 3: 0.0010661308299988205 and sum of dbs: 0.0902216177342976\n",
      "sum of Ws for layer 3: 7.500588145475651 and sum of bs: 0.00238503835745301\n",
      "MSE on the training set: 3.2079813182525562e-06\n",
      "Epoch: 4300\n",
      "sum of abs dWs for layer 0: 0.0001828335356371078 and sum of dbs: 0.19818066690018674\n",
      "sum of Ws for layer 0: 86.42321018590626 and sum of bs: 0.0015991102746648854\n",
      "sum of abs dWs for layer 1: 0.0048105207349708485 and sum of dbs: 0.13720669480246178\n",
      "sum of Ws for layer 1: 321.32497270692767 and sum of bs: 0.0016884083256924658\n",
      "sum of abs dWs for layer 2: 0.0021981925075205146 and sum of dbs: 0.2502090022854338\n",
      "sum of Ws for layer 2: 340.8626858069781 and sum of bs: 0.007882698027979975\n",
      "sum of abs dWs for layer 3: 0.0010661301218561005 and sum of dbs: 0.09022157332343504\n",
      "sum of Ws for layer 3: 7.500588145479008 and sum of bs: 0.0023850374552370768\n",
      "MSE on the training set: 3.2079781600577753e-06\n",
      "Epoch: 4310\n",
      "sum of abs dWs for layer 0: 0.00018283348569070074 and sum of dbs: 0.19818056935849926\n",
      "sum of Ws for layer 0: 86.42321018590744 and sum of bs: 0.0015991087649735074\n",
      "sum of abs dWs for layer 1: 0.004810519612352112 and sum of dbs: 0.13720662725205732\n",
      "sum of Ws for layer 1: 321.32497270694205 and sum of bs: 0.0016884071978905527\n",
      "sum of abs dWs for layer 2: 0.0021981924715347113 and sum of dbs: 0.25020887911778666\n",
      "sum of Ws for layer 2: 340.8626858069848 and sum of bs: 0.007882695669725989\n",
      "sum of abs dWs for layer 3: 0.001066130319893996 and sum of dbs: 0.09022152891159943\n",
      "sum of Ws for layer 3: 7.500588145482364 and sum of bs: 0.002385036553021588\n",
      "MSE on the training set: 3.207975001858014e-06\n",
      "Epoch: 4320\n",
      "sum of abs dWs for layer 0: 0.00018283337862832032 and sum of dbs: 0.19818047181811407\n",
      "sum of Ws for layer 0: 86.42321018590864 and sum of bs: 0.001599107255282873\n",
      "sum of abs dWs for layer 1: 0.0048105169631013035 and sum of dbs: 0.13720655970252704\n",
      "sum of Ws for layer 1: 321.3249727069564 and sum of bs: 0.0016884060700891946\n",
      "sum of abs dWs for layer 2: 0.002198191638676224 and sum of dbs: 0.2502087559517601\n",
      "sum of Ws for layer 2: 340.86268580699164 and sum of bs: 0.007882693311473163\n",
      "sum of abs dWs for layer 3: 0.0010661300908579386 and sum of dbs: 0.09022148450034907\n",
      "sum of Ws for layer 3: 7.500588145485723 and sum of bs: 0.002385035650806543\n",
      "MSE on the training set: 3.207971843659154e-06\n",
      "Epoch: 4330\n",
      "sum of abs dWs for layer 0: 0.00018283310693366546 and sum of dbs: 0.19818037427964913\n",
      "sum of Ws for layer 0: 86.42321018590982 and sum of bs: 0.001599105745592981\n",
      "sum of abs dWs for layer 1: 0.004810509913461746 and sum of dbs: 0.13720649215428518\n",
      "sum of Ws for layer 1: 321.3249727069707 and sum of bs: 0.0016884049422883917\n",
      "sum of abs dWs for layer 2: 0.002198188508917287 and sum of dbs: 0.2502086327881223\n",
      "sum of Ws for layer 2: 340.86268580699846 and sum of bs: 0.007882690953221498\n",
      "sum of abs dWs for layer 3: 0.0010661286308272133 and sum of dbs: 0.09022144008996151\n",
      "sum of Ws for layer 3: 7.500588145489079 and sum of bs: 0.002385034748591942\n",
      "MSE on the training set: 3.20796868548893e-06\n",
      "Epoch: 4340\n",
      "sum of abs dWs for layer 0: 0.00018283294879308453 and sum of dbs: 0.19818027674005922\n",
      "sum of Ws for layer 0: 86.42321018591105 and sum of bs: 0.0015991042359038326\n",
      "sum of abs dWs for layer 1: 0.004810505898964061 and sum of dbs: 0.13720642460529026\n",
      "sum of Ws for layer 1: 321.324972706985 and sum of bs: 0.001688403814488144\n",
      "sum of abs dWs for layer 2: 0.0021981869634349715 and sum of dbs: 0.25020850962308644\n",
      "sum of Ws for layer 2: 340.8626858070052 and sum of bs: 0.007882688594970991\n",
      "sum of abs dWs for layer 3: 0.0010661280198688537 and sum of dbs: 0.09022139567906898\n",
      "sum of Ws for layer 3: 7.5005881454924355 and sum of bs: 0.0023850338463777855\n",
      "MSE on the training set: 3.207965527293726e-06\n",
      "Epoch: 4350\n",
      "sum of abs dWs for layer 0: 0.0001828328512225457 and sum of dbs: 0.1981801791996173\n",
      "sum of Ws for layer 0: 86.42321018591224 and sum of bs: 0.001599102726215427\n",
      "sum of abs dWs for layer 1: 0.00481050350341473 and sum of dbs: 0.1372063570557251\n",
      "sum of Ws for layer 1: 321.32497270699935 and sum of bs: 0.0016884026866884512\n",
      "sum of abs dWs for layer 2: 0.0021981862630023074 and sum of dbs: 0.250208386456992\n",
      "sum of Ws for layer 2: 340.86268580701204 and sum of bs: 0.007882686236721649\n",
      "sum of abs dWs for layer 3: 0.001066127861801362 and sum of dbs: 0.09022135126779401\n",
      "sum of Ws for layer 3: 7.500588145495793 and sum of bs: 0.002385032944164073\n",
      "MSE on the training set: 3.207962369094419e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4360\n",
      "sum of abs dWs for layer 0: 0.0001828328556399285 and sum of dbs: 0.19818008165768372\n",
      "sum of Ws for layer 0: 86.42321018591343 and sum of bs: 0.0015991012165277646\n",
      "sum of abs dWs for layer 1: 0.004810503833859088 and sum of dbs: 0.13720628950516092\n",
      "sum of Ws for layer 1: 321.32497270701367 and sum of bs: 0.0016884015588893135\n",
      "sum of abs dWs for layer 2: 0.0021981869854765567 and sum of dbs: 0.25020826328904344\n",
      "sum of Ws for layer 2: 340.86268580701886 and sum of bs: 0.007882683878473466\n",
      "sum of abs dWs for layer 3: 0.0010661284663232916 and sum of dbs: 0.0902213068558493\n",
      "sum of Ws for layer 3: 7.500588145499149 and sum of bs: 0.0023850320419508044\n",
      "MSE on the training set: 3.207959210917399e-06\n",
      "Epoch: 4370\n",
      "sum of abs dWs for layer 0: 0.00018283265731432126 and sum of dbs: 0.19817998411853047\n",
      "sum of Ws for layer 0: 86.42321018591461 and sum of bs: 0.001599099706840845\n",
      "sum of abs dWs for layer 1: 0.0048104987452711615 and sum of dbs: 0.13720622195646187\n",
      "sum of Ws for layer 1: 321.32497270702805 and sum of bs: 0.0016884004310907309\n",
      "sum of abs dWs for layer 2: 0.0021981848793409943 and sum of dbs: 0.2502081401245534\n",
      "sum of Ws for layer 2: 340.8626858070257 and sum of bs: 0.007882681520226443\n",
      "sum of abs dWs for layer 3: 0.0010661275548907389 and sum of dbs: 0.09022126244515376\n",
      "sum of Ws for layer 3: 7.500588145502507 and sum of bs: 0.0023850311397379804\n",
      "MSE on the training set: 3.20795605273693e-06\n",
      "Epoch: 4380\n",
      "sum of abs dWs for layer 0: 0.00018283247798503685 and sum of dbs: 0.19817988657883853\n",
      "sum of Ws for layer 0: 86.42321018591585 and sum of bs: 0.0015990981971546693\n",
      "sum of abs dWs for layer 1: 0.004810494164425378 and sum of dbs: 0.13720615440740275\n",
      "sum of Ws for layer 1: 321.32497270704243 and sum of bs: 0.0016883993032927036\n",
      "sum of abs dWs for layer 2: 0.0021981830382330004 and sum of dbs: 0.2502080169593944\n",
      "sum of Ws for layer 2: 340.86268580703245 and sum of bs: 0.007882679161980582\n",
      "sum of abs dWs for layer 3: 0.001066126785494983 and sum of dbs: 0.09022121803421657\n",
      "sum of Ws for layer 3: 7.500588145505864 and sum of bs: 0.0023850302375256\n",
      "MSE on the training set: 3.2079528945585857e-06\n",
      "Epoch: 4390\n",
      "sum of abs dWs for layer 0: 0.0001828324611002119 and sum of dbs: 0.19817978903752345\n",
      "sum of Ws for layer 0: 86.42321018591703 and sum of bs: 0.0015990966874692364\n",
      "sum of abs dWs for layer 1: 0.004810493925498241 and sum of dbs: 0.13720608685725647\n",
      "sum of Ws for layer 1: 321.32497270705676 and sum of bs: 0.0016883981754952312\n",
      "sum of abs dWs for layer 2: 0.0021981834635098705 and sum of dbs: 0.25020789379221775\n",
      "sum of Ws for layer 2: 340.86268580703927 and sum of bs: 0.007882676803735882\n",
      "sum of abs dWs for layer 3: 0.0010661272307381261 and sum of dbs: 0.09022117362255058\n",
      "sum of Ws for layer 3: 7.500588145509222 and sum of bs: 0.0023850293353136643\n",
      "MSE on the training set: 3.207949736390583e-06\n",
      "Epoch: 4400\n",
      "sum of abs dWs for layer 0: 0.00018283245599960154 and sum of dbs: 0.19817969149588036\n",
      "sum of Ws for layer 0: 86.42321018591822 and sum of bs: 0.0015990951777845464\n",
      "sum of abs dWs for layer 1: 0.004810494001533432 and sum of dbs: 0.13720601930689127\n",
      "sum of Ws for layer 1: 321.3249727070711 and sum of bs: 0.0016883970476983137\n",
      "sum of abs dWs for layer 2: 0.0021981840531848217 and sum of dbs: 0.2502077706246341\n",
      "sum of Ws for layer 2: 340.86268580704603 and sum of bs: 0.00788267444549234\n",
      "sum of abs dWs for layer 3: 0.0010661277640887997 and sum of dbs: 0.09022112921073752\n",
      "sum of Ws for layer 3: 7.500588145512578 and sum of bs: 0.002385028433102172\n",
      "MSE on the training set: 3.2079465782173707e-06\n",
      "Epoch: 4410\n",
      "sum of abs dWs for layer 0: 0.0001828324856886678 and sum of dbs: 0.19817959395393292\n",
      "sum of Ws for layer 0: 86.42321018591943 and sum of bs: 0.0015990936681005994\n",
      "sum of abs dWs for layer 1: 0.0048104950074598675 and sum of dbs: 0.13720595175632314\n",
      "sum of Ws for layer 1: 321.3249727070854 and sum of bs: 0.0016883959199019516\n",
      "sum of abs dWs for layer 2: 0.002198185128240934 and sum of dbs: 0.250207647456673\n",
      "sum of Ws for layer 2: 340.86268580705286 and sum of bs: 0.007882672087249961\n",
      "sum of abs dWs for layer 3: 0.001066128557568799 and sum of dbs: 0.09022108479878808\n",
      "sum of Ws for layer 3: 7.500588145515936 and sum of bs: 0.002385027530891124\n",
      "MSE on the training set: 3.207943420055718e-06\n",
      "Epoch: 4420\n",
      "sum of abs dWs for layer 0: 0.0001828322412239045 and sum of dbs: 0.19817949641534416\n",
      "sum of Ws for layer 0: 86.42321018592064 and sum of bs: 0.0015990921584173956\n",
      "sum of abs dWs for layer 1: 0.004810488685629457 and sum of dbs: 0.1372058842080079\n",
      "sum of Ws for layer 1: 321.32497270709973 and sum of bs: 0.0016883947921061447\n",
      "sum of abs dWs for layer 2: 0.002198182378375173 and sum of dbs: 0.25020752429288956\n",
      "sum of Ws for layer 2: 340.8626858070596 and sum of bs: 0.007882669729008742\n",
      "sum of abs dWs for layer 3: 0.0010661273011382559 and sum of dbs: 0.0902210403883476\n",
      "sum of Ws for layer 3: 7.500588145519292 and sum of bs: 0.0023850266286805206\n",
      "MSE on the training set: 3.207940261897102e-06\n",
      "Epoch: 4430\n",
      "sum of abs dWs for layer 0: 0.00018283225197609473 and sum of dbs: 0.19817939887397257\n",
      "sum of Ws for layer 0: 86.42321018592182 and sum of bs: 0.0015990906487349352\n",
      "sum of abs dWs for layer 1: 0.004810489185397711 and sum of dbs: 0.1372058166578279\n",
      "sum of Ws for layer 1: 321.3249727071141 and sum of bs: 0.0016883936643108926\n",
      "sum of abs dWs for layer 2: 0.0021981831892275497 and sum of dbs: 0.2502074011256464\n",
      "sum of Ws for layer 2: 340.86268580706644 and sum of bs: 0.007882667370768684\n",
      "sum of abs dWs for layer 3: 0.0010661279530219828 and sum of dbs: 0.09022099597665739\n",
      "sum of Ws for layer 3: 7.5005881455226495 and sum of bs: 0.0023850257264703607\n",
      "MSE on the training set: 3.207937103719247e-06\n",
      "Epoch: 4440\n",
      "sum of abs dWs for layer 0: 0.00018283223246750045 and sum of dbs: 0.19817930133302528\n",
      "sum of Ws for layer 0: 86.42321018592303 and sum of bs: 0.0015990891390532177\n",
      "sum of abs dWs for layer 1: 0.004810488876329934 and sum of dbs: 0.1372057491079335\n",
      "sum of Ws for layer 1: 321.3249727071284 and sum of bs: 0.0016883925365161956\n",
      "sum of abs dWs for layer 2: 0.002198183577892692 and sum of dbs: 0.2502072779589317\n",
      "sum of Ws for layer 2: 340.86268580707326 and sum of bs: 0.007882665012529787\n",
      "sum of abs dWs for layer 3: 0.0010661283786414396 and sum of dbs: 0.09022095156515808\n",
      "sum of Ws for layer 3: 7.500588145526006 and sum of bs: 0.0023850248242606453\n",
      "MSE on the training set: 3.2079339455684684e-06\n",
      "Epoch: 4450\n",
      "sum of abs dWs for layer 0: 0.0001828320266008504 and sum of dbs: 0.1981792037935908\n",
      "sum of Ws for layer 0: 86.4232101859242 and sum of bs: 0.0015990876293722432\n",
      "sum of abs dWs for layer 1: 0.004810483586180765 and sum of dbs: 0.13720568155905444\n",
      "sum of Ws for layer 1: 321.3249727071427 and sum of bs: 0.0016883914087220536\n",
      "sum of abs dWs for layer 2: 0.0021981813665388848 and sum of dbs: 0.25020715479409944\n",
      "sum of Ws for layer 2: 340.8626858070801 and sum of bs: 0.00788266265429205\n",
      "sum of abs dWs for layer 3: 0.0010661274108173533 and sum of dbs: 0.09022090715433861\n",
      "sum of Ws for layer 3: 7.500588145529364 and sum of bs: 0.002385023922051374\n",
      "MSE on the training set: 3.2079307874181634e-06\n",
      "Epoch: 4460\n",
      "sum of abs dWs for layer 0: 0.0001828320068127008 and sum of dbs: 0.1981791062528209\n",
      "sum of Ws for layer 0: 86.4232101859254 and sum of bs: 0.0015990861196920117\n",
      "sum of abs dWs for layer 1: 0.004810483269638918 and sum of dbs: 0.13720561400928105\n",
      "sum of Ws for layer 1: 321.32497270715714 and sum of bs: 0.0016883902809284668\n",
      "sum of abs dWs for layer 2: 0.0021981817512958615 and sum of dbs: 0.2502070316276072\n",
      "sum of Ws for layer 2: 340.86268580708685 and sum of bs: 0.007882660296055473\n",
      "sum of abs dWs for layer 3: 0.0010661278343396586 and sum of dbs: 0.09022086274291956\n",
      "sum of Ws for layer 3: 7.500588145532721 and sum of bs: 0.0023850230198425464\n",
      "MSE on the training set: 3.20792762926151e-06\n",
      "Epoch: 4470\n",
      "sum of abs dWs for layer 0: 0.00018283200884899506 and sum of dbs: 0.19817900871166164\n",
      "sum of Ws for layer 0: 86.42321018592662 and sum of bs: 0.001599084610012523\n",
      "sum of abs dWs for layer 1: 0.00481048353644173 and sum of dbs: 0.13720554645924773\n",
      "sum of Ws for layer 1: 321.32497270717147 and sum of bs: 0.0016883891531354353\n",
      "sum of abs dWs for layer 2: 0.002198182440540094 and sum of dbs: 0.2502069084606317\n",
      "sum of Ws for layer 2: 340.8626858070936 and sum of bs: 0.007882657937820059\n",
      "sum of abs dWs for layer 3: 0.001066128421049582 and sum of dbs: 0.09022081833132592\n",
      "sum of Ws for layer 3: 7.500588145536078 and sum of bs: 0.002385022117634164\n",
      "MSE on the training set: 3.207924471106472e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4480\n",
      "sum of abs dWs for layer 0: 0.00018283181039422243 and sum of dbs: 0.19817891117274192\n",
      "sum of Ws for layer 0: 86.4232101859278 and sum of bs: 0.0015990831003337785\n",
      "sum of abs dWs for layer 1: 0.00481047844439827 and sum of dbs: 0.13720547891071694\n",
      "sum of Ws for layer 1: 321.3249727071858 and sum of bs: 0.0016883880253429587\n",
      "sum of abs dWs for layer 2: 0.002198180332592034 and sum of dbs: 0.25020678529644225\n",
      "sum of Ws for layer 2: 340.86268580710043 and sum of bs: 0.007882655579585804\n",
      "sum of abs dWs for layer 3: 0.0010661275086407368 and sum of dbs: 0.09022077392073853\n",
      "sum of Ws for layer 3: 7.500588145539435 and sum of bs: 0.0023850212154262244\n",
      "MSE on the training set: 3.2079213129674535e-06\n",
      "Epoch: 4490\n",
      "sum of abs dWs for layer 0: 0.00018283178614626028 and sum of dbs: 0.19817881363196865\n",
      "sum of Ws for layer 0: 86.423210185929 and sum of bs: 0.0015990815906557763\n",
      "sum of abs dWs for layer 1: 0.004810478008652466 and sum of dbs: 0.1372054113609445\n",
      "sum of Ws for layer 1: 321.3249727072001 and sum of bs: 0.001688386897551037\n",
      "sum of abs dWs for layer 2: 0.002198180655123407 and sum of dbs: 0.25020666212994863\n",
      "sum of Ws for layer 2: 340.8626858071072 and sum of bs: 0.007882653221352709\n",
      "sum of abs dWs for layer 3: 0.0010661278988148735 and sum of dbs: 0.09022072950931886\n",
      "sum of Ws for layer 3: 7.500588145542793 and sum of bs: 0.0023850203132187294\n",
      "MSE on the training set: 3.207918154823425e-06\n",
      "Epoch: 4500\n",
      "sum of abs dWs for layer 0: 0.00018283169684384936 and sum of dbs: 0.19817871609210713\n",
      "sum of Ws for layer 0: 86.4232101859302 and sum of bs: 0.0015990800809785174\n",
      "sum of abs dWs for layer 1: 0.004810475834100843 and sum of dbs: 0.1372053438117844\n",
      "sum of Ws for layer 1: 321.32497270721444 and sum of bs: 0.0016883857697596706\n",
      "sum of abs dWs for layer 2: 0.002198180070038535 and sum of dbs: 0.2502065389645898\n",
      "sum of Ws for layer 2: 340.862685807114 and sum of bs: 0.007882650863120774\n",
      "sum of abs dWs for layer 3: 0.0010661278025663448 and sum of dbs: 0.09022068509830902\n",
      "sum of Ws for layer 3: 7.500588145546149 and sum of bs: 0.0023850194110116786\n",
      "MSE on the training set: 3.2079149966968477e-06\n",
      "Epoch: 4510\n",
      "sum of abs dWs for layer 0: 0.00018283150023356567 and sum of dbs: 0.19817861855371297\n",
      "sum of Ws for layer 0: 86.4232101859314 and sum of bs: 0.0015990785713020015\n",
      "sum of abs dWs for layer 1: 0.004810470791355607 and sum of dbs: 0.13720527626360915\n",
      "sum of Ws for layer 1: 321.32497270722877 and sum of bs: 0.0016883846419688594\n",
      "sum of abs dWs for layer 2: 0.0021981779878212228 and sum of dbs: 0.2502064158010567\n",
      "sum of Ws for layer 2: 340.86268580712084 and sum of bs: 0.007882648504890004\n",
      "sum of abs dWs for layer 3: 0.0010661269039473164 and sum of dbs: 0.09022064068795861\n",
      "sum of Ws for layer 3: 7.500588145549506 and sum of bs: 0.0023850185088050715\n",
      "MSE on the training set: 3.2079118385529175e-06\n",
      "Epoch: 4520\n",
      "sum of abs dWs for layer 0: 0.00018283150538925432 and sum of dbs: 0.19817852101249345\n",
      "sum of Ws for layer 0: 86.42321018593259 and sum of bs: 0.0015990770616262288\n",
      "sum of abs dWs for layer 1: 0.004810471141529493 and sum of dbs: 0.13720520871354075\n",
      "sum of Ws for layer 1: 321.32497270724315 and sum of bs: 0.0016883835141786032\n",
      "sum of abs dWs for layer 2: 0.0021981787205851604 and sum of dbs: 0.2502062926340109\n",
      "sum of Ws for layer 2: 340.8626858071276 and sum of bs: 0.00788264614666039\n",
      "sum of abs dWs for layer 3: 0.0010661275139785637 and sum of dbs: 0.09022059627633937\n",
      "sum of Ws for layer 3: 7.500588145552864 and sum of bs: 0.0023850176065989093\n",
      "MSE on the training set: 3.2079086804291636e-06\n",
      "Epoch: 4530\n",
      "sum of abs dWs for layer 0: 0.00018283128461658133 and sum of dbs: 0.1981784234743469\n",
      "sum of Ws for layer 0: 86.42321018593378 and sum of bs: 0.0015990755519511993\n",
      "sum of abs dWs for layer 1: 0.004810465452955917 and sum of dbs: 0.1372051411655336\n",
      "sum of Ws for layer 1: 321.3249727072574 and sum of bs: 0.0016883823863889018\n",
      "sum of abs dWs for layer 2: 0.0021981763012577536 and sum of dbs: 0.25020616947078744\n",
      "sum of Ws for layer 2: 340.8626858071344 and sum of bs: 0.007882643788431938\n",
      "sum of abs dWs for layer 3: 0.001066126434689882 and sum of dbs: 0.09022055186610074\n",
      "sum of Ws for layer 3: 7.500588145556222 and sum of bs: 0.0023850167043931907\n",
      "MSE on the training set: 3.207905522318262e-06\n",
      "Epoch: 4540\n",
      "sum of abs dWs for layer 0: 0.00018283128546558156 and sum of dbs: 0.19817832593357063\n",
      "sum of Ws for layer 0: 86.42321018593499 and sum of bs: 0.001599074042276913\n",
      "sum of abs dWs for layer 1: 0.004810465688019559 and sum of dbs: 0.13720507361576448\n",
      "sum of Ws for layer 1: 321.3249727072718 and sum of bs: 0.0016883812585997558\n",
      "sum of abs dWs for layer 2: 0.00219817697393014 and sum of dbs: 0.25020604630429477\n",
      "sum of Ws for layer 2: 340.86268580714125 and sum of bs: 0.007882641430204649\n",
      "sum of abs dWs for layer 3: 0.0010661270125181723 and sum of dbs: 0.09022050745468121\n",
      "sum of Ws for layer 3: 7.500588145559578 and sum of bs: 0.0023850158021879162\n",
      "MSE on the training set: 3.207902364175842e-06\n",
      "Epoch: 4550\n",
      "sum of abs dWs for layer 0: 0.00018283104440945216 and sum of dbs: 0.19817822839569335\n",
      "sum of Ws for layer 0: 86.42321018593618 and sum of bs: 0.0015990725326033692\n",
      "sum of abs dWs for layer 1: 0.004810459457299368 and sum of dbs: 0.13720500606794006\n",
      "sum of Ws for layer 1: 321.3249727072862 and sum of bs: 0.001688380130811165\n",
      "sum of abs dWs for layer 2: 0.002198174271617917 and sum of dbs: 0.2502059231414081\n",
      "sum of Ws for layer 2: 340.86268580714807 and sum of bs: 0.007882639071978517\n",
      "sum of abs dWs for layer 3: 0.0010661257815665576 and sum of dbs: 0.09022046304456416\n",
      "sum of Ws for layer 3: 7.500588145562936 and sum of bs: 0.0023850148999830863\n",
      "MSE on the training set: 3.20789920605596e-06\n",
      "Epoch: 4560\n",
      "sum of abs dWs for layer 0: 0.000182831112406844 and sum of dbs: 0.1981781308541652\n",
      "sum of Ws for layer 0: 86.42321018593738 and sum of bs: 0.001599071022930569\n",
      "sum of abs dWs for layer 1: 0.004810461487142327 and sum of dbs: 0.13720493851766888\n",
      "sum of Ws for layer 1: 321.3249727073005 and sum of bs: 0.001688379003023129\n",
      "sum of abs dWs for layer 2: 0.002198175881125113 and sum of dbs: 0.2502057999739822\n",
      "sum of Ws for layer 2: 340.8626858071549 and sum of bs: 0.007882636713753548\n",
      "sum of abs dWs for layer 3: 0.0010661268614748157 and sum of dbs: 0.09022041863280746\n",
      "sum of Ws for layer 3: 7.500588145566292 and sum of bs: 0.0023850139977786996\n",
      "MSE on the training set: 3.207896047935981e-06\n",
      "Epoch: 4570\n",
      "sum of abs dWs for layer 0: 0.0001828308968986942 and sum of dbs: 0.19817803331620426\n",
      "sum of Ws for layer 0: 86.42321018593859 and sum of bs: 0.0015990695132585117\n",
      "sum of abs dWs for layer 1: 0.004810455939281815 and sum of dbs: 0.1372048709697905\n",
      "sum of Ws for layer 1: 321.3249727073148 and sum of bs: 0.0016883778752356483\n",
      "sum of abs dWs for layer 2: 0.0021981735352441852 and sum of dbs: 0.2502056768109933\n",
      "sum of Ws for layer 2: 340.86268580716165 and sum of bs: 0.007882634355529739\n",
      "sum of abs dWs for layer 3: 0.0010661258215489819 and sum of dbs: 0.09022037422265342\n",
      "sum of Ws for layer 3: 7.50058814556965 and sum of bs: 0.002385013095574758\n",
      "MSE on the training set: 3.2078928898199247e-06\n",
      "Epoch: 4580\n",
      "sum of abs dWs for layer 0: 0.0001828308536517765 and sum of dbs: 0.19817793577604775\n",
      "sum of Ws for layer 0: 86.42321018593978 and sum of bs: 0.001599068003587197\n",
      "sum of abs dWs for layer 1: 0.004810454995725193 and sum of dbs: 0.1372048034204411\n",
      "sum of Ws for layer 1: 321.32497270732915 and sum of bs: 0.0016883767474487224\n",
      "sum of abs dWs for layer 2: 0.002198173592705105 and sum of dbs: 0.2502055536452751\n",
      "sum of Ws for layer 2: 340.8626858071685 and sum of bs: 0.007882631997307092\n",
      "sum of abs dWs for layer 3: 0.0010661260696591102 and sum of dbs: 0.09022032981151344\n",
      "sum of Ws for layer 3: 7.500588145573007 and sum of bs: 0.0023850121933712597\n",
      "MSE on the training set: 3.207889731701248e-06\n",
      "Epoch: 4590\n",
      "sum of abs dWs for layer 0: 0.00018283076262894012 and sum of dbs: 0.19817783823642235\n",
      "sum of Ws for layer 0: 86.42321018594099 and sum of bs: 0.0015990664939166262\n",
      "sum of abs dWs for layer 1: 0.00481045277517928 and sum of dbs: 0.13720473587144888\n",
      "sum of Ws for layer 1: 321.3249727073435 and sum of bs: 0.0016883756196623516\n",
      "sum of abs dWs for layer 2: 0.002198172983608998 and sum of dbs: 0.2502054304802181\n",
      "sum of Ws for layer 2: 340.86268580717524 and sum of bs: 0.007882629639085604\n",
      "sum of abs dWs for layer 3: 0.0010661259605356773 and sum of dbs: 0.09022028540061232\n",
      "sum of Ws for layer 3: 7.500588145576364 and sum of bs: 0.0023850112911682057\n",
      "MSE on the training set: 3.207886573579886e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4600\n",
      "sum of abs dWs for layer 0: 0.00018283071707359534 and sum of dbs: 0.19817774069660146\n",
      "sum of Ws for layer 0: 86.42321018594218 and sum of bs: 0.0015990649842467985\n",
      "sum of abs dWs for layer 1: 0.004810451769915906 and sum of dbs: 0.13720466832232658\n",
      "sum of Ws for layer 1: 321.3249727073578 and sum of bs: 0.001688374491876536\n",
      "sum of abs dWs for layer 2: 0.0021981730088531416 and sum of dbs: 0.2502053073149189\n",
      "sum of Ws for layer 2: 340.862685807182 and sum of bs: 0.007882627280865277\n",
      "sum of abs dWs for layer 3: 0.001066126191381934 and sum of dbs: 0.09022024098962367\n",
      "sum of Ws for layer 3: 7.500588145579721 and sum of bs: 0.002385010388965596\n",
      "MSE on the training set: 3.207883415478005e-06\n",
      "Epoch: 4610\n",
      "sum of abs dWs for layer 0: 0.0001828305410382917 and sum of dbs: 0.19817764315804026\n",
      "sum of Ws for layer 0: 86.42321018594336 and sum of bs: 0.0015990634745777135\n",
      "sum of abs dWs for layer 1: 0.004810447277110169 and sum of dbs: 0.13720460077404997\n",
      "sum of Ws for layer 1: 321.3249727073722 and sum of bs: 0.0016883733640912757\n",
      "sum of abs dWs for layer 2: 0.002198171213689506 and sum of dbs: 0.25020518415118725\n",
      "sum of Ws for layer 2: 340.8626858071889 and sum of bs: 0.007882624922646112\n",
      "sum of abs dWs for layer 3: 0.0010661254466032598 and sum of dbs: 0.09022019657920116\n",
      "sum of Ws for layer 3: 7.500588145583079 and sum of bs: 0.0023850094867634303\n",
      "MSE on the training set: 3.207880257362766e-06\n",
      "Epoch: 4620\n",
      "sum of abs dWs for layer 0: 0.00018283037311177743 and sum of dbs: 0.19817754561947432\n",
      "sum of Ws for layer 0: 86.42321018594455 and sum of bs: 0.0015990619649093715\n",
      "sum of abs dWs for layer 1: 0.004810443001044764 and sum of dbs: 0.1372045332257712\n",
      "sum of Ws for layer 1: 321.32497270738656 and sum of bs: 0.00168837223630657\n",
      "sum of abs dWs for layer 2: 0.002198169531654157 and sum of dbs: 0.25020506098745054\n",
      "sum of Ws for layer 2: 340.86268580719565 and sum of bs: 0.007882622564428106\n",
      "sum of abs dWs for layer 3: 0.001066124762453735 and sum of dbs: 0.09022015216877681\n",
      "sum of Ws for layer 3: 7.500588145586436 and sum of bs: 0.002385008584561709\n",
      "MSE on the training set: 3.2078770992821846e-06\n",
      "Epoch: 4630\n",
      "sum of abs dWs for layer 0: 0.00018283039504609125 and sum of dbs: 0.19817744807882248\n",
      "sum of Ws for layer 0: 86.42321018594576 and sum of bs: 0.0015990604552417731\n",
      "sum of abs dWs for layer 1: 0.004810443799679922 and sum of dbs: 0.13720446567609507\n",
      "sum of Ws for layer 1: 321.3249727074009 and sum of bs: 0.0016883711085224197\n",
      "sum of abs dWs for layer 2: 0.0021981704984947725 and sum of dbs: 0.25020493782112097\n",
      "sum of Ws for layer 2: 340.8626858072024 and sum of bs: 0.007882620206211262\n",
      "sum of abs dWs for layer 3: 0.0010661254979337678 and sum of dbs: 0.09022010775741583\n",
      "sum of Ws for layer 3: 7.500588145589793 and sum of bs: 0.002385007682360432\n",
      "MSE on the training set: 3.207873941179185e-06\n",
      "Epoch: 4640\n",
      "sum of abs dWs for layer 0: 0.0001828302242725765 and sum of dbs: 0.1981773505407196\n",
      "sum of Ws for layer 0: 86.42321018594697 and sum of bs: 0.0015990589455749174\n",
      "sum of abs dWs for layer 1: 0.004810439447510744 and sum of dbs: 0.1372043981281289\n",
      "sum of Ws for layer 1: 321.3249727074152 and sum of bs: 0.0016883699807388242\n",
      "sum of abs dWs for layer 2: 0.00219816877673379 and sum of dbs: 0.250204814657962\n",
      "sum of Ws for layer 2: 340.86268580720923 and sum of bs: 0.007882617847995576\n",
      "sum of abs dWs for layer 3: 0.0010661247924938274 and sum of dbs: 0.09022006334720009\n",
      "sum of Ws for layer 3: 7.50058814559315 and sum of bs: 0.0023850067801595982\n",
      "MSE on the training set: 3.207870783076104e-06\n",
      "Epoch: 4650\n",
      "sum of abs dWs for layer 0: 0.0001828301003523206 and sum of dbs: 0.19817725300174893\n",
      "sum of Ws for layer 0: 86.42321018594815 and sum of bs: 0.0015990574359088046\n",
      "sum of abs dWs for layer 1: 0.0048104363476645234 and sum of dbs: 0.13720433057958198\n",
      "sum of Ws for layer 1: 321.32497270742954 and sum of bs: 0.0016883688529557841\n",
      "sum of abs dWs for layer 2: 0.0021981677086557153 and sum of dbs: 0.25020469149372476\n",
      "sum of Ws for layer 2: 340.86268580721605 and sum of bs: 0.007882615489781053\n",
      "sum of abs dWs for layer 3: 0.0010661244373867918 and sum of dbs: 0.09022001893659483\n",
      "sum of Ws for layer 3: 7.500588145596507 and sum of bs: 0.002385005877959209\n",
      "MSE on the training set: 3.2078676249859947e-06\n",
      "Epoch: 4660\n",
      "sum of abs dWs for layer 0: 0.0001828301017192723 and sum of dbs: 0.19817715546162373\n",
      "sum of Ws for layer 0: 86.42321018594937 and sum of bs: 0.0015990559262434353\n",
      "sum of abs dWs for layer 1: 0.0048104365965674495 and sum of dbs: 0.13720426303026206\n",
      "sum of Ws for layer 1: 321.32497270744386 and sum of bs: 0.001688367725173299\n",
      "sum of abs dWs for layer 2: 0.002198168388548496 and sum of dbs: 0.2502045683280527\n",
      "sum of Ws for layer 2: 340.8626858072228 and sum of bs: 0.007882613131567691\n",
      "sum of abs dWs for layer 3: 0.0010661250190782334 and sum of dbs: 0.09021997452547126\n",
      "sum of Ws for layer 3: 7.500588145599864 and sum of bs: 0.0023850049757592638\n",
      "MSE on the training set: 3.2078644668998915e-06\n",
      "Epoch: 4670\n",
      "sum of abs dWs for layer 0: 0.00018283006571395664 and sum of dbs: 0.19817705792153034\n",
      "sum of Ws for layer 0: 86.42321018595055 and sum of bs: 0.0015990544165788087\n",
      "sum of abs dWs for layer 1: 0.004810435846563993 and sum of dbs: 0.13720419548096452\n",
      "sum of Ws for layer 1: 321.3249727074582 and sum of bs: 0.0016883665973913692\n",
      "sum of abs dWs for layer 2: 0.002198168547034838 and sum of dbs: 0.25020444516242113\n",
      "sum of Ws for layer 2: 340.8626858072297 and sum of bs: 0.007882610773355489\n",
      "sum of abs dWs for layer 3: 0.001066125321331306 and sum of dbs: 0.09021993011436229\n",
      "sum of Ws for layer 3: 7.5005881456032215 and sum of bs: 0.002385004073559763\n",
      "MSE on the training set: 3.207861308796241e-06\n",
      "Epoch: 4680\n",
      "sum of abs dWs for layer 0: 0.00018283004420863785 and sum of dbs: 0.19817696038124638\n",
      "sum of Ws for layer 0: 86.42321018595176 and sum of bs: 0.0015990529069149256\n",
      "sum of abs dWs for layer 1: 0.004810435484124355 and sum of dbs: 0.1372041279315403\n",
      "sum of Ws for layer 1: 321.3249727074725 and sum of bs: 0.0016883654696099942\n",
      "sum of abs dWs for layer 2: 0.0021981689078175255 and sum of dbs: 0.25020432199655346\n",
      "sum of Ws for layer 2: 340.8626858072364 and sum of bs: 0.007882608415144446\n",
      "sum of abs dWs for layer 3: 0.0010661257320003729 and sum of dbs: 0.09021988570316798\n",
      "sum of Ws for layer 3: 7.500588145606578 and sum of bs: 0.002385003171360706\n",
      "MSE on the training set: 3.2078581507323085e-06\n",
      "Epoch: 4690\n",
      "sum of abs dWs for layer 0: 0.00018283009520991836 and sum of dbs: 0.19817686284005895\n",
      "sum of Ws for layer 0: 86.42321018595295 and sum of bs: 0.0015990513972517855\n",
      "sum of abs dWs for layer 1: 0.004810437059686575 and sum of dbs: 0.1372040603815114\n",
      "sum of Ws for layer 1: 321.32497270748684 and sum of bs: 0.0016883643418291743\n",
      "sum of abs dWs for layer 2: 0.002198170280195839 and sum of dbs: 0.2502041988295633\n",
      "sum of Ws for layer 2: 340.8626858072432 and sum of bs: 0.007882606056934567\n",
      "sum of abs dWs for layer 3: 0.0010661266848217802 and sum of dbs: 0.09021984129156817\n",
      "sum of Ws for layer 3: 7.500588145609936 and sum of bs: 0.0023850022691620934\n",
      "MSE on the training set: 3.207854992656893e-06\n",
      "Epoch: 4700\n",
      "sum of abs dWs for layer 0: 0.0001828300365469597 and sum of dbs: 0.19817676530058012\n",
      "sum of Ws for layer 0: 86.42321018595415 and sum of bs: 0.0015990498875893882\n",
      "sum of abs dWs for layer 1: 0.004810435704076413 and sum of dbs: 0.13720399283262913\n",
      "sum of Ws for layer 1: 321.3249727075013 and sum of bs: 0.0016883632140489097\n",
      "sum of abs dWs for layer 2: 0.002198170122567269 and sum of dbs: 0.25020407566469877\n",
      "sum of Ws for layer 2: 340.86268580725005 and sum of bs: 0.007882603698725846\n",
      "sum of abs dWs for layer 3: 0.0010661268176554593 and sum of dbs: 0.09021979688073613\n",
      "sum of Ws for layer 3: 7.5005881456132935 and sum of bs: 0.002385001366963925\n",
      "MSE on the training set: 3.2078518345716136e-06\n",
      "Epoch: 4710\n",
      "sum of abs dWs for layer 0: 0.0001828299477348818 and sum of dbs: 0.19817666776126563\n",
      "sum of Ws for layer 0: 86.42321018595533 and sum of bs: 0.0015990483779277347\n",
      "sum of abs dWs for layer 1: 0.004810433542615796 and sum of dbs: 0.13720392528385805\n",
      "sum of Ws for layer 1: 321.3249727075156 and sum of bs: 0.0016883620862691998\n",
      "sum of abs dWs for layer 2: 0.0021981695443050665 and sum of dbs: 0.25020395250003946\n",
      "sum of Ws for layer 2: 340.8626858072568 and sum of bs: 0.007882601340518287\n",
      "sum of abs dWs for layer 3: 0.0010661267250554845 and sum of dbs: 0.09021975246997818\n",
      "sum of Ws for layer 3: 7.50058814561665 and sum of bs: 0.0023850004647662003\n",
      "MSE on the training set: 3.207848676504986e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4720\n",
      "sum of abs dWs for layer 0: 0.00018283003625375015 and sum of dbs: 0.19817657022040014\n",
      "sum of Ws for layer 0: 86.42321018595653 and sum of bs: 0.0015990468682668237\n",
      "sum of abs dWs for layer 1: 0.004810436120965986 and sum of dbs: 0.1372038577340482\n",
      "sum of Ws for layer 1: 321.3249727075299 and sum of bs: 0.001688360958490045\n",
      "sum of abs dWs for layer 2: 0.0021981714401108814 and sum of dbs: 0.25020382933345237\n",
      "sum of Ws for layer 2: 340.86268580726363 and sum of bs: 0.007882598982311887\n",
      "sum of abs dWs for layer 3: 0.001066127958399946 and sum of dbs: 0.09021970805852386\n",
      "sum of Ws for layer 3: 7.500588145620008 and sum of bs: 0.00238499956256892\n",
      "MSE on the training set: 3.207845518431698e-06\n",
      "Epoch: 4730\n",
      "sum of abs dWs for layer 0: 0.00018282997070165737 and sum of dbs: 0.19817647268070343\n",
      "sum of Ws for layer 0: 86.42321018595773 and sum of bs: 0.001599045358606656\n",
      "sum of abs dWs for layer 1: 0.004810434581219921 and sum of dbs: 0.13720379018502302\n",
      "sum of Ws for layer 1: 321.32497270754425 and sum of bs: 0.0016883598307114458\n",
      "sum of abs dWs for layer 2: 0.0021981711863647683 and sum of dbs: 0.25020370616831966\n",
      "sum of Ws for layer 2: 340.86268580727045 and sum of bs: 0.00788259662410665\n",
      "sum of abs dWs for layer 3: 0.0010661280397232845 and sum of dbs: 0.09021966364759483\n",
      "sum of Ws for layer 3: 7.5005881456233645 and sum of bs: 0.0023849986603720836\n",
      "MSE on the training set: 3.207842360373473e-06\n",
      "Epoch: 4740\n",
      "sum of abs dWs for layer 0: 0.00018282991919730792 and sum of dbs: 0.19817637514143008\n",
      "sum of Ws for layer 0: 86.42321018595894 and sum of bs: 0.0015990438489472312\n",
      "sum of abs dWs for layer 1: 0.0048104334169458 and sum of dbs: 0.13720372263628272\n",
      "sum of Ws for layer 1: 321.3249727075586 and sum of bs: 0.0016883587029334014\n",
      "sum of abs dWs for layer 2: 0.002198171128608022 and sum of dbs: 0.2502035830037143\n",
      "sum of Ws for layer 2: 340.8626858072772 and sum of bs: 0.007882594265902572\n",
      "sum of abs dWs for layer 3: 0.0010661282260788516 and sum of dbs: 0.09021961923685626\n",
      "sum of Ws for layer 3: 7.500588145626722 and sum of bs: 0.0023849977581756913\n",
      "MSE on the training set: 3.2078392022911165e-06\n",
      "Epoch: 4750\n",
      "sum of abs dWs for layer 0: 0.00018282984602690678 and sum of dbs: 0.19817627760244166\n",
      "sum of Ws for layer 0: 86.42321018596013 and sum of bs: 0.0015990423392885497\n",
      "sum of abs dWs for layer 1: 0.004810431673567107 and sum of dbs: 0.1372036550877345\n",
      "sum of Ws for layer 1: 321.3249727075729 and sum of bs: 0.001688357575155912\n",
      "sum of abs dWs for layer 2: 0.0021981707685644098 and sum of dbs: 0.2502034598394642\n",
      "sum of Ws for layer 2: 340.86268580728404 and sum of bs: 0.007882591907699655\n",
      "sum of abs dWs for layer 3: 0.0010661282504288126 and sum of dbs: 0.09021957482624597\n",
      "sum of Ws for layer 3: 7.500588145630079 and sum of bs: 0.002384996855979743\n",
      "MSE on the training set: 3.2078360442531213e-06\n",
      "Epoch: 4760\n",
      "sum of abs dWs for layer 0: 0.00018282967725243167 and sum of dbs: 0.1981761800643867\n",
      "sum of Ws for layer 0: 86.42321018596132 and sum of bs: 0.001599040829630611\n",
      "sum of abs dWs for layer 1: 0.004810427374827002 and sum of dbs: 0.1372035875398132\n",
      "sum of Ws for layer 1: 321.3249727075873 and sum of bs: 0.0016883564473789774\n",
      "sum of abs dWs for layer 2: 0.0021981690746867774 and sum of dbs: 0.2502033366763759\n",
      "sum of Ws for layer 2: 340.86268580729086 and sum of bs: 0.0078825895494979\n",
      "sum of abs dWs for layer 3: 0.0010661275599309766 and sum of dbs: 0.0902195304160553\n",
      "sum of Ws for layer 3: 7.500588145633437 and sum of bs: 0.002384995953784239\n",
      "MSE on the training set: 3.207832886187538e-06\n",
      "Epoch: 4770\n",
      "sum of abs dWs for layer 0: 0.00018282982331406273 and sum of dbs: 0.1981760825230615\n",
      "sum of Ws for layer 0: 86.4232101859625 and sum of bs: 0.0015990393199734156\n",
      "sum of abs dWs for layer 1: 0.004810431491211917 and sum of dbs: 0.13720351998970043\n",
      "sum of Ws for layer 1: 321.32497270760155 and sum of bs: 0.0016883553196025984\n",
      "sum of abs dWs for layer 2: 0.0021981717733051934 and sum of dbs: 0.2502032135092217\n",
      "sum of Ws for layer 2: 340.8626858072977 and sum of bs: 0.007882587191297304\n",
      "sum of abs dWs for layer 3: 0.00106612922352918 and sum of dbs: 0.09021948600439594\n",
      "sum of Ws for layer 3: 7.500588145636795 and sum of bs: 0.002384995051589179\n",
      "MSE on the training set: 3.2078297281366216e-06\n",
      "Epoch: 4780\n",
      "sum of abs dWs for layer 0: 0.00018282965276076972 and sum of dbs: 0.19817598498537578\n",
      "sum of Ws for layer 0: 86.4232101859637 and sum of bs: 0.0015990378103169633\n",
      "sum of abs dWs for layer 1: 0.004810427144928972 and sum of dbs: 0.13720345244202875\n",
      "sum of Ws for layer 1: 321.3249727076159 and sum of bs: 0.0016883541918267743\n",
      "sum of abs dWs for layer 2: 0.0021981700546127685 and sum of dbs: 0.2502030903465944\n",
      "sum of Ws for layer 2: 340.8626858073044 and sum of bs: 0.007882584833097868\n",
      "sum of abs dWs for layer 3: 0.0010661285197296618 and sum of dbs: 0.09021944159437167\n",
      "sum of Ws for layer 3: 7.500588145640153 and sum of bs: 0.002384994149394563\n",
      "MSE on the training set: 3.207826570071665e-06\n",
      "Epoch: 4790\n",
      "sum of abs dWs for layer 0: 0.00018282957153251914 and sum of dbs: 0.19817588744665113\n",
      "sum of Ws for layer 0: 86.42321018596489 and sum of bs: 0.0015990363006612542\n",
      "sum of abs dWs for layer 1: 0.004810425186177886 and sum of dbs: 0.13720338489366168\n",
      "sum of Ws for layer 1: 321.32497270763025 and sum of bs: 0.001688353064051505\n",
      "sum of abs dWs for layer 2: 0.0021981695821485835 and sum of dbs: 0.25020296718267604\n",
      "sum of Ws for layer 2: 340.8626858073112 and sum of bs: 0.007882582474899593\n",
      "sum of abs dWs for layer 3: 0.0010661284838285818 and sum of dbs: 0.09021939718388106\n",
      "sum of Ws for layer 3: 7.500588145643509 and sum of bs: 0.002384993247200392\n",
      "MSE on the training set: 3.2078234120238946e-06\n",
      "Epoch: 4800\n",
      "sum of abs dWs for layer 0: 0.000182829485531549 and sum of dbs: 0.1981757899081313\n",
      "sum of Ws for layer 0: 86.42321018596607 and sum of bs: 0.001599034791006288\n",
      "sum of abs dWs for layer 1: 0.004810423099849694 and sum of dbs: 0.13720331734543292\n",
      "sum of Ws for layer 1: 321.3249727076446 and sum of bs: 0.0016883519362767912\n",
      "sum of abs dWs for layer 2: 0.0021981690430908953 and sum of dbs: 0.2502028440190133\n",
      "sum of Ws for layer 2: 340.86268580731803 and sum of bs: 0.007882580116702481\n",
      "sum of abs dWs for layer 3: 0.001066128412237338 and sum of dbs: 0.09021935277348274\n",
      "sum of Ws for layer 3: 7.500588145646866 and sum of bs: 0.002384992345006664\n",
      "MSE on the training set: 3.207820253982288e-06\n",
      "Epoch: 4810\n",
      "sum of abs dWs for layer 0: 0.00018282928545909264 and sum of dbs: 0.198175692370983\n",
      "sum of Ws for layer 0: 86.4232101859673 and sum of bs: 0.0015990332813520653\n",
      "sum of abs dWs for layer 1: 0.004810417964562018 and sum of dbs: 0.1372032497981248\n",
      "sum of Ws for layer 1: 321.32497270765896 and sum of bs: 0.0016883508085026324\n",
      "sum of abs dWs for layer 2: 0.0021981669125488413 and sum of dbs: 0.2502027208570571\n",
      "sum of Ws for layer 2: 340.8626858073248 and sum of bs: 0.007882577758506528\n",
      "sum of abs dWs for layer 3: 0.0010661274877131416 and sum of dbs: 0.09021930836370076\n",
      "sum of Ws for layer 3: 7.500588145650224 and sum of bs: 0.002384991442813381\n",
      "MSE on the training set: 3.2078170959369636e-06\n",
      "Epoch: 4820\n",
      "sum of abs dWs for layer 0: 0.00018282920074772554 and sum of dbs: 0.19817559483264655\n",
      "sum of Ws for layer 0: 86.42321018596849 and sum of bs: 0.0015990317716985856\n",
      "sum of abs dWs for layer 1: 0.0048104159127079285 and sum of dbs: 0.1372031822500211\n",
      "sum of Ws for layer 1: 321.3249727076733 and sum of bs: 0.0016883496807290287\n",
      "sum of abs dWs for layer 2: 0.002198166391490305 and sum of dbs: 0.25020259769362424\n",
      "sum of Ws for layer 2: 340.8626858073316 and sum of bs: 0.007882575400311736\n",
      "sum of abs dWs for layer 3: 0.001066127425768119 and sum of dbs: 0.09021926395338538\n",
      "sum of Ws for layer 3: 7.500588145653581 and sum of bs: 0.0023849905406205414\n",
      "MSE on the training set: 3.207813937893688e-06\n",
      "Epoch: 4830\n",
      "sum of abs dWs for layer 0: 0.00018282905823807584 and sum of dbs: 0.19817549729512987\n",
      "sum of Ws for layer 0: 86.42321018596968 and sum of bs: 0.0015990302620458483\n",
      "sum of abs dWs for layer 1: 0.004810412315990901 and sum of dbs: 0.13720311470246813\n",
      "sum of Ws for layer 1: 321.3249727076876 and sum of bs: 0.0016883485529559797\n",
      "sum of abs dWs for layer 2: 0.0021981650640394983 and sum of dbs: 0.2502024745312117\n",
      "sum of Ws for layer 2: 340.86268580733844 and sum of bs: 0.007882573042118105\n",
      "sum of abs dWs for layer 3: 0.001066126931646241 and sum of dbs: 0.09021921954343853\n",
      "sum of Ws for layer 3: 7.500588145656937 and sum of bs: 0.002384989638428146\n",
      "MSE on the training set: 3.2078107798741755e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4840\n",
      "sum of abs dWs for layer 0: 0.00018282890104648065 and sum of dbs: 0.19817539975733361\n",
      "sum of Ws for layer 0: 86.42321018597087 and sum of bs: 0.0015990287523938546\n",
      "sum of abs dWs for layer 1: 0.00481040832684118 and sum of dbs: 0.13720304715472872\n",
      "sum of Ws for layer 1: 321.32497270770193 and sum of bs: 0.0016883474251834863\n",
      "sum of abs dWs for layer 2: 0.0021981635317615674 and sum of dbs: 0.2502023513684525\n",
      "sum of Ws for layer 2: 340.8626858073452 and sum of bs: 0.007882570683925633\n",
      "sum of abs dWs for layer 3: 0.0010661263277540458 and sum of dbs: 0.0902191751333664\n",
      "sum of Ws for layer 3: 7.500588145660295 and sum of bs: 0.002384988736236195\n",
      "MSE on the training set: 3.207807621844486e-06\n",
      "Epoch: 4850\n",
      "sum of abs dWs for layer 0: 0.00018282879834395256 and sum of dbs: 0.19817530221932497\n",
      "sum of Ws for layer 0: 86.42321018597207 and sum of bs: 0.0015990272427426036\n",
      "sum of abs dWs for layer 1: 0.0048104057941058456 and sum of dbs: 0.13720297960684802\n",
      "sum of Ws for layer 1: 321.32497270771626 and sum of bs: 0.0016883462974115475\n",
      "sum of abs dWs for layer 2: 0.002198162759686763 and sum of dbs: 0.25020222820543003\n",
      "sum of Ws for layer 2: 340.862685807352 and sum of bs: 0.007882568325734323\n",
      "sum of abs dWs for layer 3: 0.0010661261312786717 and sum of dbs: 0.09021913072319916\n",
      "sum of Ws for layer 3: 7.500588145663654 and sum of bs: 0.002384987834044688\n",
      "MSE on the training set: 3.207804463814072e-06\n",
      "Epoch: 4860\n",
      "sum of abs dWs for layer 0: 0.00018282868922181773 and sum of dbs: 0.19817520468151623\n",
      "sum of Ws for layer 0: 86.42321018597328 and sum of bs: 0.0015990257330920964\n",
      "sum of abs dWs for layer 1: 0.00481040308978702 and sum of dbs: 0.1372029120591024\n",
      "sum of Ws for layer 1: 321.32497270773064 and sum of bs: 0.001688345169640164\n",
      "sum of abs dWs for layer 2: 0.0021981618980494483 and sum of dbs: 0.25020210504265716\n",
      "sum of Ws for layer 2: 340.86268580735884 and sum of bs: 0.007882565967544174\n",
      "sum of abs dWs for layer 3: 0.0010661258868018527 and sum of dbs: 0.09021908631312203\n",
      "sum of Ws for layer 3: 7.5005881456670105 and sum of bs: 0.002384986931853625\n",
      "MSE on the training set: 3.207801305782768e-06\n",
      "Epoch: 4870\n",
      "sum of abs dWs for layer 0: 0.00018282849658106328 and sum of dbs: 0.19817510714470232\n",
      "sum of Ws for layer 0: 86.42321018597447 and sum of bs: 0.0015990242234423316\n",
      "sum of abs dWs for layer 1: 0.004810398153133494 and sum of dbs: 0.13720284451202486\n",
      "sum of Ws for layer 1: 321.324972707745 and sum of bs: 0.0016883440418693355\n",
      "sum of abs dWs for layer 2: 0.0021981598711864453 and sum of dbs: 0.25020198188112225\n",
      "sum of Ws for layer 2: 340.86268580736566 and sum of bs: 0.007882563609355185\n",
      "sum of abs dWs for layer 3: 0.0010661250178405752 and sum of dbs: 0.09021904190349202\n",
      "sum of Ws for layer 3: 7.500588145670368 and sum of bs: 0.002384986029663006\n",
      "MSE on the training set: 3.2077981477660754e-06\n",
      "Epoch: 4880\n",
      "sum of abs dWs for layer 0: 0.00018282840345247 and sum of dbs: 0.1981750096068436\n",
      "sum of Ws for layer 0: 86.42321018597566 and sum of bs: 0.00159902271379331\n",
      "sum of abs dWs for layer 1: 0.004810395876297268 and sum of dbs: 0.13720277696424776\n",
      "sum of Ws for layer 1: 321.32497270775934 and sum of bs: 0.001688342914099062\n",
      "sum of abs dWs for layer 2: 0.002198159232684617 and sum of dbs: 0.250201858718289\n",
      "sum of Ws for layer 2: 340.86268580737243 and sum of bs: 0.007882561251167357\n",
      "sum of abs dWs for layer 3: 0.0010661248929529923 and sum of dbs: 0.090218997493393\n",
      "sum of Ws for layer 3: 7.500588145673724 and sum of bs: 0.002384985127472831\n",
      "MSE on the training set: 3.207794989746484e-06\n",
      "Epoch: 4890\n",
      "sum of abs dWs for layer 0: 0.00018282830469673172 and sum of dbs: 0.19817491206885357\n",
      "sum of Ws for layer 0: 86.42321018597686 and sum of bs: 0.0015990212041450318\n",
      "sum of abs dWs for layer 1: 0.004810393449058023 and sum of dbs: 0.1372027094163838\n",
      "sum of Ws for layer 1: 321.32497270777367 and sum of bs: 0.001688341786329344\n",
      "sum of abs dWs for layer 2: 0.0021981585156758627 and sum of dbs: 0.2502017355552934\n",
      "sum of Ws for layer 2: 340.8626858073792 and sum of bs: 0.007882558892980689\n",
      "sum of abs dWs for layer 3: 0.0010661247259865845 and sum of dbs: 0.0902189530832353\n",
      "sum of Ws for layer 3: 7.5005881456770815 and sum of bs: 0.0023849842252831005\n",
      "MSE on the training set: 3.2077918317121075e-06\n",
      "Epoch: 4900\n",
      "sum of abs dWs for layer 0: 0.00018282808725273876 and sum of dbs: 0.1981748145322983\n",
      "sum of Ws for layer 0: 86.42321018597805 and sum of bs: 0.0015990196944974965\n",
      "sum of abs dWs for layer 1: 0.004810387849447168 and sum of dbs: 0.13720264186948278\n",
      "sum of Ws for layer 1: 321.324972707788 and sum of bs: 0.0016883406585601803\n",
      "sum of abs dWs for layer 2: 0.0021981561427617245 and sum of dbs: 0.2502016123940829\n",
      "sum of Ws for layer 2: 340.862685807386 and sum of bs: 0.007882556534795181\n",
      "sum of abs dWs for layer 3: 0.0010661236715643028 and sum of dbs: 0.09021890867372237\n",
      "sum of Ws for layer 3: 7.50058814568044 and sum of bs: 0.0023849833230938137\n",
      "MSE on the training set: 3.207788673711884e-06\n",
      "Epoch: 4910\n",
      "sum of abs dWs for layer 0: 0.00018282800497557806 and sum of dbs: 0.19817471699459935\n",
      "sum of Ws for layer 0: 86.42321018597924 and sum of bs: 0.0015990181848507042\n",
      "sum of abs dWs for layer 1: 0.004810385862651479 and sum of dbs: 0.13720257432181604\n",
      "sum of Ws for layer 1: 321.3249727078023 and sum of bs: 0.0016883395307915722\n",
      "sum of abs dWs for layer 2: 0.0021981556556529675 and sum of dbs: 0.25020148923145114\n",
      "sum of Ws for layer 2: 340.86268580739284 and sum of bs: 0.007882554176610835\n",
      "sum of abs dWs for layer 3: 0.001066123627808623 and sum of dbs: 0.09021886426369599\n",
      "sum of Ws for layer 3: 7.500588145683797 and sum of bs: 0.0023849824209049714\n",
      "MSE on the training set: 3.2077855157011363e-06\n",
      "Epoch: 4920\n",
      "sum of abs dWs for layer 0: 0.00018282789633890684 and sum of dbs: 0.1981746194572911\n",
      "sum of Ws for layer 0: 86.42321018598044 and sum of bs: 0.0015990166752046552\n",
      "sum of abs dWs for layer 1: 0.004810383171310065 and sum of dbs: 0.13720250677441234\n",
      "sum of Ws for layer 1: 321.32497270781664 and sum of bs: 0.0016883384030235192\n",
      "sum of abs dWs for layer 2: 0.002198154800784822 and sum of dbs: 0.25020136606930615\n",
      "sum of Ws for layer 2: 340.86268580739966 and sum of bs: 0.00788255181842765\n",
      "sum of abs dWs for layer 3: 0.0010661233869622452 and sum of dbs: 0.09021881985384543\n",
      "sum of Ws for layer 3: 7.5005881456871535 and sum of bs: 0.0023849815187165727\n",
      "MSE on the training set: 3.207782357678979e-06\n",
      "Epoch: 4930\n",
      "sum of abs dWs for layer 0: 0.0001828277340184239 and sum of dbs: 0.19817452192023913\n",
      "sum of Ws for layer 0: 86.42321018598163 and sum of bs: 0.001599015165559349\n",
      "sum of abs dWs for layer 1: 0.004810379045068294 and sum of dbs: 0.1372024392271815\n",
      "sum of Ws for layer 1: 321.32497270783097 and sum of bs: 0.0016883372752560214\n",
      "sum of abs dWs for layer 2: 0.002198153196935672 and sum of dbs: 0.2502012429074808\n",
      "sum of Ws for layer 2: 340.8626858074064 and sum of bs: 0.007882549460245625\n",
      "sum of abs dWs for layer 3: 0.0010661227447086737 and sum of dbs: 0.09021877544411028\n",
      "sum of Ws for layer 3: 7.500588145690513 and sum of bs: 0.002384980616528618\n",
      "MSE on the training set: 3.207779199674003e-06\n",
      "Epoch: 4940\n",
      "sum of abs dWs for layer 0: 0.0001828276970296272 and sum of dbs: 0.19817442438168162\n",
      "sum of Ws for layer 0: 86.42321018598282 and sum of bs: 0.0015990136559147863\n",
      "sum of abs dWs for layer 1: 0.004810378268771157 and sum of dbs: 0.1372023716789424\n",
      "sum of Ws for layer 1: 321.32497270784535 and sum of bs: 0.0016883361474890784\n",
      "sum of abs dWs for layer 2: 0.0021981533416779433 and sum of dbs: 0.25020111974378406\n",
      "sum of Ws for layer 2: 340.8626858074132 and sum of bs: 0.00788254710206476\n",
      "sum of abs dWs for layer 3: 0.0010661230395856416 and sum of dbs: 0.09021873103369915\n",
      "sum of Ws for layer 3: 7.500588145693869 and sum of bs: 0.0023849797143411086\n",
      "MSE on the training set: 3.2077760416690635e-06\n",
      "Epoch: 4950\n",
      "sum of abs dWs for layer 0: 0.00018282749608032205 and sum of dbs: 0.19817432684506706\n",
      "sum of Ws for layer 0: 86.42321018598402 and sum of bs: 0.0015990121462709662\n",
      "sum of abs dWs for layer 1: 0.004810373110041277 and sum of dbs: 0.1372023041320069\n",
      "sum of Ws for layer 1: 321.3249727078597 and sum of bs: 0.0016883350197226905\n",
      "sum of abs dWs for layer 2: 0.0021981511988929353 and sum of dbs: 0.2502009965825045\n",
      "sum of Ws for layer 2: 340.86268580742 and sum of bs: 0.007882544743885058\n",
      "sum of abs dWs for layer 3: 0.0010661221084959157 and sum of dbs: 0.09021868662416105\n",
      "sum of Ws for layer 3: 7.500588145697225 and sum of bs: 0.002384978812154042\n",
      "MSE on the training set: 3.207772883683844e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4960\n",
      "sum of abs dWs for layer 0: 0.00018282747861473082 and sum of dbs: 0.19817422930656825\n",
      "sum of Ws for layer 0: 86.42321018598521 and sum of bs: 0.0015990106366278894\n",
      "sum of abs dWs for layer 1: 0.004810372855564677 and sum of dbs: 0.13720223658380926\n",
      "sum of Ws for layer 1: 321.32497270787394 and sum of bs: 0.0016883338919568575\n",
      "sum of abs dWs for layer 2: 0.0021981516160074896 and sum of dbs: 0.25020087341888264\n",
      "sum of Ws for layer 2: 340.86268580742683 and sum of bs: 0.007882542385706513\n",
      "sum of abs dWs for layer 3: 0.0010661225493486508 and sum of dbs: 0.0902186422137769\n",
      "sum of Ws for layer 3: 7.500588145700583 and sum of bs: 0.0023849779099674204\n",
      "MSE on the training set: 3.207769725691332e-06\n",
      "Epoch: 4970\n",
      "sum of abs dWs for layer 0: 0.0001828273442164494 and sum of dbs: 0.1981741317696921\n",
      "sum of Ws for layer 0: 86.4232101859864 and sum of bs: 0.0015990091269855555\n",
      "sum of abs dWs for layer 1: 0.004810369475647765 and sum of dbs: 0.13720216903670052\n",
      "sum of Ws for layer 1: 321.3249727078884 and sum of bs: 0.0016883327641915797\n",
      "sum of abs dWs for layer 2: 0.0021981504017191417 and sum of dbs: 0.25020075025727956\n",
      "sum of Ws for layer 2: 340.86268580743365 and sum of bs: 0.00788254002752913\n",
      "sum of abs dWs for layer 3: 0.0010661221158716505 and sum of dbs: 0.09021859780412189\n",
      "sum of Ws for layer 3: 7.500588145703941 and sum of bs: 0.0023849770077812426\n",
      "MSE on the training set: 3.2077665676916227e-06\n",
      "Epoch: 4980\n",
      "sum of abs dWs for layer 0: 0.00018282712350014977 and sum of dbs: 0.198174034233611\n",
      "sum of Ws for layer 0: 86.42321018598763 and sum of bs: 0.0015990076173439647\n",
      "sum of abs dWs for layer 1: 0.004810363788568 and sum of dbs: 0.1372021014901259\n",
      "sum of Ws for layer 1: 321.3249727079027 and sum of bs: 0.001688331636426857\n",
      "sum of abs dWs for layer 2: 0.0021981479831444583 and sum of dbs: 0.2502006270966662\n",
      "sum of Ws for layer 2: 340.86268580744047 and sum of bs: 0.007882537669352908\n",
      "sum of abs dWs for layer 3: 0.001066121036974281 and sum of dbs: 0.0902185533948243\n",
      "sum of Ws for layer 3: 7.500588145707297 and sum of bs: 0.0023849761055955085\n",
      "MSE on the training set: 3.207763409701406e-06\n",
      "Epoch: 4990\n",
      "sum of abs dWs for layer 0: 0.00018282690332050938 and sum of dbs: 0.19817393669787023\n",
      "sum of Ws for layer 0: 86.4232101859888 and sum of bs: 0.001599006107703117\n",
      "sum of abs dWs for layer 1: 0.004810358115833892 and sum of dbs: 0.13720203394378053\n",
      "sum of Ws for layer 1: 321.324972707917 and sum of bs: 0.0016883305086626893\n",
      "sum of abs dWs for layer 2: 0.0021981455720542203 and sum of dbs: 0.25020050393647686\n",
      "sum of Ws for layer 2: 340.8626858074473 and sum of bs: 0.007882535311177847\n",
      "sum of abs dWs for layer 3: 0.0010661199620874097 and sum of dbs: 0.09021850898567987\n",
      "sum of Ws for layer 3: 7.500588145710655 and sum of bs: 0.0023849752034102193\n",
      "MSE on the training set: 3.2077602517254e-06\n"
     ]
    }
   ],
   "source": [
    "our_nn = NeuralNetwork(number_of_layers= 3, number_of_neurons_per_layer= [100, 40, 68], input_size= 1 )\n",
    "\n",
    "our_nn.train(X_train, y_train, lr = 1e-7, epochs = 5000, batch_size= 100, shuffle_indexes= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "572c4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, intermediate = our_nn._forward_batch(x0s = X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07dd3d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00533396]),\n",
       " array([2.00542893]),\n",
       " array([4.00534492]),\n",
       " array([6.0052609]),\n",
       " array([8.00517688]),\n",
       " array([10.00509287]),\n",
       " array([12.00500885]),\n",
       " array([14.00492484]),\n",
       " array([16.00484082]),\n",
       " array([18.00475681]),\n",
       " array([20.00467279]),\n",
       " array([22.00458877]),\n",
       " array([24.00450476]),\n",
       " array([26.00442074]),\n",
       " array([28.00433673]),\n",
       " array([30.00425271]),\n",
       " array([32.0041687]),\n",
       " array([34.00408468]),\n",
       " array([36.00400066]),\n",
       " array([38.00391665]),\n",
       " array([40.00383263]),\n",
       " array([42.00374862]),\n",
       " array([44.0036646]),\n",
       " array([46.00358059]),\n",
       " array([48.00349657]),\n",
       " array([50.00341255]),\n",
       " array([52.00332854]),\n",
       " array([54.00324452]),\n",
       " array([56.00316051]),\n",
       " array([58.00307649]),\n",
       " array([60.00299248]),\n",
       " array([62.00290846]),\n",
       " array([64.00282444]),\n",
       " array([66.00274043]),\n",
       " array([68.00265641]),\n",
       " array([70.0025724]),\n",
       " array([72.00248838]),\n",
       " array([74.00240437]),\n",
       " array([76.00232035]),\n",
       " array([78.00223633]),\n",
       " array([80.00215232]),\n",
       " array([82.0020683]),\n",
       " array([84.00198429]),\n",
       " array([86.00190027]),\n",
       " array([88.00181626]),\n",
       " array([90.00173224]),\n",
       " array([92.00164822]),\n",
       " array([94.00156421]),\n",
       " array([96.00148019]),\n",
       " array([98.00139618]),\n",
       " array([100.00131216]),\n",
       " array([102.00122814]),\n",
       " array([104.00114413]),\n",
       " array([106.00106011]),\n",
       " array([108.0009761]),\n",
       " array([110.00089208]),\n",
       " array([112.00080807]),\n",
       " array([114.00072405]),\n",
       " array([116.00064003]),\n",
       " array([118.00055602]),\n",
       " array([120.000472]),\n",
       " array([122.00038799]),\n",
       " array([124.00030397]),\n",
       " array([126.00021996]),\n",
       " array([128.00013594]),\n",
       " array([130.00005192]),\n",
       " array([131.99996791]),\n",
       " array([133.99988389]),\n",
       " array([135.99979988]),\n",
       " array([137.99971586]),\n",
       " array([139.99963185]),\n",
       " array([141.99954783]),\n",
       " array([143.99946381]),\n",
       " array([145.9993798]),\n",
       " array([147.99929578]),\n",
       " array([149.99921177]),\n",
       " array([151.99912775]),\n",
       " array([153.99904374]),\n",
       " array([155.99895972]),\n",
       " array([157.9988757]),\n",
       " array([159.99879169]),\n",
       " array([161.99870767]),\n",
       " array([163.99862366]),\n",
       " array([165.99853964]),\n",
       " array([167.99845563]),\n",
       " array([169.99837161]),\n",
       " array([171.99828759]),\n",
       " array([173.99820358]),\n",
       " array([175.99811956]),\n",
       " array([177.99803555]),\n",
       " array([179.99795153]),\n",
       " array([181.99786752]),\n",
       " array([183.9977835]),\n",
       " array([185.99769948]),\n",
       " array([187.99761547]),\n",
       " array([189.99753145]),\n",
       " array([191.99744744]),\n",
       " array([193.99736342]),\n",
       " array([195.99727941]),\n",
       " array([197.99719539])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb94974",
   "metadata": {},
   "outputs": [],
   "source": [
    "-7.355385292240312, -7.43845899976959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, intermediate = our_nn._forward_batch(x0s = X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d92af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10856309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eaf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_unn = NeuralNetwork(number_of_layers= 3, number_of_neurons_per_layer= [200, 33, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf76b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0s =  [[0 for i in range(58)], [1 for i in range(58)]]\n",
    "\n",
    "desired_outputs = [0,1]\n",
    "\n",
    "outputs, intermediate_outputs_list =  our_unn._forward_batch(x0s= x0s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(our_unn.bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3edcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dWs, dbiases = our_unn.backward(x0s, outputs, desired_outputs,intermediate_outputs_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dWs), len(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece20429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALISE THE PARAMETERS OF YOUR NEURAL NETWORK AND...\n",
    "# PLEASE CONSIDER TO USE A MASK OF ONE FOR THE ACTION MADE AND ZERO OTHERWISE IF YOU ARE NOT USING VANILLA GRADIENT DESCENT...\n",
    "# WE SUGGEST A NETWORK WITH ONE HIDDEN LAYER WITH SIZE 200. \n",
    "\n",
    "\n",
    "S,X,allowed_a=env.Initialise_game()\n",
    "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "\n",
    "N_in=np.shape(X)[0]    ## INPUT SIZE\n",
    "N_h=200                ## NUMBER OF HIDDEN NODES\n",
    "\n",
    "\n",
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS SUGGESTED (FOR A GRID SIZE OF 4)\n",
    "\n",
    "epsilon_0 = 0.2     # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY\n",
    "beta = 0.00005      # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING (SEE epsilon_f BELOW)\n",
    "gamma = 0.85        # THE DISCOUNT FACTOR\n",
    "eta = 0.0035        # THE LEARNING RATE\n",
    "\n",
    "N_episodes = 100000 # THE NUMBER OF GAMES TO BE PLAYED \n",
    "\n",
    "# SAVING VARIABLES\n",
    "R_save = np.zeros([N_episodes, 1])\n",
    "N_moves_save = np.zeros([N_episodes, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP BONE STRUCTURE...\n",
    "# I WROTE FOR YOU A RANDOM AGENT (THE RANDOM AGENT WILL BE SLOWER TO GIVE CHECKMATE THAN AN OPTIMISED ONE, \n",
    "# SO DON'T GET CONCERNED BY THE TIME IT TAKES), CHANGE WITH YOURS ...\n",
    "\n",
    "for n in range(N_episodes):\n",
    "\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
    "    Done=0                                   ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "    i = 1                                    ## COUNTER FOR NUMBER OF ACTIONS\n",
    "    \n",
    "    S,X,allowed_a=env.Initialise_game()      ## INITIALISE GAME\n",
    "    print(f'Episode number: {n}')            ## REMOVE THIS OF COURSE, WE USED THIS TO CHECK THAT IT WAS RUNNING.  - Glad to hear.\n",
    "    \n",
    "    while Done==0:                           ## START THE EPISODE\n",
    "        \n",
    "        \n",
    "        ## THIS IS A RANDOM AGENT, CHANGE IT...\n",
    "        \n",
    "        a,_=np.where(allowed_a==1)\n",
    "        a_agent=np.random.permutation(a)[0]\n",
    "\n",
    "                \n",
    "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a_agent)\n",
    "        \n",
    "        ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE\n",
    "        if Done==1:\n",
    "            \n",
    "            \n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # IF THE EPISODE IS NOT OVER...\n",
    "        else:\n",
    "            \n",
    "            ## ONLY TO PUT SUMETHING\n",
    "            PIPPO=1\n",
    "            \n",
    "            \n",
    "        # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
    "        S=np.copy(S_next)\n",
    "        X=np.copy(X_next)\n",
    "        allowed_a=np.copy(allowed_a_next)\n",
    "        \n",
    "        i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288ad57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
